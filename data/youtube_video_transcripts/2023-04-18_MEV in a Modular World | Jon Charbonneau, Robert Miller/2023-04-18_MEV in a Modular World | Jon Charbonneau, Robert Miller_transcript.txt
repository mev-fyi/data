Speaker A (00:00:00.090 to 00:01:08.418): Alright, everyone, welcome back to another episode of Bell Curve. Before we jump in, quick disclaimer. The views expressed by my co host today are their personal views and they do not represent the views of any organization with which the co hosts are associated with. Nothing in the episode is construed or relied upon as financial, technical, tax, legal, or other advice. You know the deal. Now let's jump into the episode. Hey, everyone. Before we get into it today, just want to give a quick shout out to this season sponsor, Rook. Close to a billion dollars worth of mev has been taken out of users'pockets and that's just on Ethereum and that number is only getting larger. Unfortunately, Rook thinks that it's time for a change and they've built a solution which is going to automatically redirect that mev back to where it belongs into the user's pocket. So you're going to be hearing all about them later in the show. I'm a huge fan of this team and what they're building, so stay tuned to find out more. All right, partner. Today we're going to be talking to John Tribunau and Robert Miller. I am psyched for this episode. It's going to be a very good one. You want to talk a little bit about why we've got John and Robert on today?
Speaker B (00:01:08.584 to 00:02:36.110): Yeah. So I think our topic is mev in the modular. Mean, I couldn't I couldn't imagine two better guests, basically, to interview about this. I think John has written some of the seminal posts about Ethereum's roll up Centric Roadmap, and recently he also hit it out of the park with his post on Roll Ups Aren't Real. Roll ups aren't real is what it was called about. Like, I mean, it's even hard to say what the post was about because it covered everything, right? Like different sequencer options for roll ups to decentralized blockbusting. And yeah, it can really recommend this post to everyone. And I think we will explore in this episode, basically, what this roll up Centric Roadmap will look like for crypto, the different options for decentralizing sequencers, like the challenges also that rollups face. In doing that, we will talk about chat sequencing and cross domain mev and then ultimately we will make the bridge also to Swath. And I'm very glad that we have today Robert Miller with us, head of Product at Flashbots. So he's one of the main folks in charge of actually designing and implementing Suave. And yeah, he's also someone who has just this incredible wealth of knowledge about the MEB supply chain. And so, yeah, I think this would be just a blast of a conversation.
Speaker C (00:02:36.690 to 00:02:37.146): Agreed.
Speaker A (00:02:37.178 to 00:02:49.570): Hasu all right, let's jump right into it. All right, everyone, welcome back to another episode of Bell Curve. Today, Hasu and I are joined by Robert Miller of Flashbots and John Tribunau of DBA. Guys, welcome to the show.
Speaker C (00:02:49.720 to 00:02:50.930): Thanks for having us on.
Speaker D (00:02:51.000 to 00:02:54.226): Good morning. Good afternoon. Thanks for having us thought you were.
Speaker A (00:02:54.248 to 00:03:39.118): Going to give us a good morning, good afternoon. Good night, guys. We are really stoked for this episode. I've been looking forward to this one. The title of the episode is Mev in a Modular World. So thus far in the first two episodes of this season hasu and I have really explored kind of the infrastructure kind of from the perspective of main chain Ethereum. Obviously, as everyone in crypto knows, the roadmap for Ethereum is a modular one and a lot of mev based activity is going to move up the stack onto some of these roll ups and layer twos. So maybe if we could just start from sort of a 10,000 foot vantage point and John or Robert, whoever kind of wants to take this first. How do you see mev changing as Ethereum adopts this modular roadmap?
Speaker D (00:03:39.214 to 00:07:01.842): Yeah, so obviously things will change a lot. There's kind of two parts to what I would say would change as you move to roll ups. Some of it is kind of what I'll call, like, fundamental to roll ups in that there are different kind of interactions at the boundaries between a roll up and an L one versus potentially other chains where, for example, if you have like layer one sequencing for a layer two where that is not normally the case for any kind of other chain the reality is that most of it that will change is just the kind of simple stuff and a factor of what roll ups look like today. So simple example being generally all of the MAV is going to be done in real time by the person who is the leader of whoever is choosing the ordering of those blocks. So if I'm doing swaps on uniswap on L One then generally it's the Ethereum Validators who are doing that. To the extent that I move in L Two and L two, it's obviously going to be the sequencers who are doing that there as opposed to it Ethereum L One validators who are not going to be that kind of real time person who's deciding the ordering for most transactions. And the big difference between most roll ups today and the way that any other chain work is the fact that all of them run centralized sequencers today, which is obviously just not a thing that we see on any other L One chains. Because if you had a centralized sequencer for an L One, I don't really know what the point of that would be. And that has had a lot of nice things for users, but also obviously a lot of negative externalities in that very just kind of simple setup that all of them have taken. Because what effectively all of them do is we run a centralized sequencer and we keep all the order filled private and we just do a simple first come, first serve because that sounds really nice for users of yeah, you could trust us. We'll give you fast confirmations. We're going to make sure that you don't get front run. We're going to give you, quote unquote, fair ordering of as we see everything, we're going to put it in order and that sounds really nice. The problems that you obviously start to see with that are a lot of the problems that we've seen with, for example, Arbitram over the past months as they're thinking through their strategy long term, of when you have this simple first come, first serve. And the only way that I can express my preference of what order do I want to be in that block is, well, I'm going to race to be the first person in that block. So that means people are investing in latency infrastructure, trying to co locate with the sequencer, they're trying to spam in many cases. And you start to see the result of that is you end up with an incredibly heavy load on the sequencer to the point where Arbitram sequencer had hundreds of thousands of connections to it and they were thinking about implementing proof of work to kind of meter those connections. Don't do proof of work as a solution to mev, will be my only response there. So all of these roll ups are trying to think through their longer term strategy of okay, we're going to have a lot of the same problems that l One set to deal with. So you see people like Arbitram starting to think about their longer term plans of okay, how do we implement some kind of way for people to express their preferences and an ability to express those preferences by paying for those preferences directly as opposed to just paying by investing in latency infrastructure and spam in the network. So those are things like their time boost proposal or what Shinnon proposed with the frequent batch auction. First come, first serve variation, where you allow that flexibility to layer on some kind of auction mechanism for people to pay. So that you can get much better, much more robust infrastructure in the same way that we see on the layer one to deal with kind of mev today.
Speaker C (00:07:01.976 to 00:08:10.710): I think the only thing that I would add to that is that I think L two S give you a roll up, specifically, give you a place where you can experiment more with these types of different types of ordering policies in a much faster way rather than L one. It's kind of part of the thesis of the roll up centric roadmap for ethereum that you have more space for experimentation before you can break things down to the protocol level if something works. So, very interested to see how these experiments around fair ordering fare in the wild. We've already seen some of it play out with the Arbitrum airdrop a few weeks ago that John alluded to interested to see maybe some cryptographic solutions solutions in quotes to mev that I've heard might launch. The last thing that I would add is that I think as you have more roll ups proliferating you have more mev that exists not just on these roll ups but between the roll ups as well. So I think we're going to see the rise of cross domain mev and more demand for solutions for some kind of synchronicity between different asynchronous execution environments, as liquidity fragments to many different more places, and you see more activity across different roll ups.
Speaker B (00:08:11.450 to 00:08:41.794): I have a follow up question to that, but I think we'll put a pin in it and move it to the section on shared sequencing, which we'll get to later. I have a follow up question for you, John. So you've written a lot about the rollup centric future for Ethereum. You had some iconic articles here, both about that and sequencing. So can you give us your best guess how you think, not in terms of mev, but just overall how you think the ecosystem will look like in a couple of years?
Speaker D (00:08:41.992 to 00:09:44.262): So the main thing that I probably expect to see out of most roll ups is they start to look like l ones in the way that they run in very simple, traditional ways. My guess is that it's not going to be all of these radically different everyone's using a shared sequencer or everyone's using a centralized sequencer type thing. My guess is that there's going to be meaningful pressure to decentralize, whether that is on a technical level, social level, if we want to, or very possibly, quite frankly, on a regulatory level of guessing, but we don't know what that is going to look like. There are probably some questions when you're the only person operating this thing, so there's probably going to be pressure to do that. And the simplest thing that we know works is you strap a consensus set on there, you have some form of PBS, you have some form of auction between these validators who are on the roll ups and we know that's a system that works reasonably well. So my guess is we start to see that across a lot of roll ups over the next year.
Speaker C (00:09:44.316 to 00:10:08.640): Plus, I've got a question if I can steal the mic from Michael and Hasu. So, John, how do you square things like existing consensus sets, auctions like PBS, which generally take a little bit more latency with users kind of existing desire for super low level block times on roll ups as well as private mem pools? So how do you think these two things, which seem intention to me will play out in the future?
Speaker D (00:10:09.090 to 00:12:19.382): Sure. So as far as the latency, I understand that some roll ups care a lot more about that than I care about that, and that, I think, low latency certainly well below Ethereum block times. Yeah, users want that. Does it have to be 300 milliseconds, 400 milliseconds? No, I don't really think it does. I think it's fine if your blocks are a second or whatever, I really don't notice the difference. Quite frankly, as a user, I don't get the obsession with let's do this literally at the speed of light. So that would be my general intuition on latency. As far as that, as far as some of the privacy aspects and layering on these kind of additional features that users like, we don't want everyone to get front run and stuff. My guess is it's going to be a little bit easier to implement a lot of that stuff on these roll ups compared to something like, say, Ethereum. Because while I expect some form of PBS auction mechanism, et cetera to arise in these roll ups, they're also going to be realistically much smaller validator sets because we have a much weaker trust assumption of what can an L two validator set do compared to an L one validator set. So we have a much smaller semi trusted on weak assumptions type of validator set that's probably running these maybe you have ten sequencers, 20 sequencers, whatever it is. So you start to end up in a situation that looks a little more like the Mev Guest days or even a step further of cosmos where you start doing protocol on builders, stuff like that. We're not going to have these super strict requirements of what you guys have to do for Ethereum today, where we have to do this commit reveal process. And you want to support this long tail of hundreds of thousands of validators, it's probably okay to have some level of trust on, hey, we'll show you the bundles in this kind of builder interface with the proposer. And yeah, if you start to steal them, we're going to kick you off because you're one of ten or 20 validators and we know who you are and you're gone. So being able to start to offer those features when you have a known set of smaller validators becomes a lot easier. And similarly enforcing those things, like if you start front running, et cetera, and have some kind of privacy trust around them.
Speaker C (00:12:19.516 to 00:14:58.620): I think there's two interesting things, and I'm going to go back to one thing that you said earlier about a single sequencer that I want to follow up on. There one is it's just latency in general in these systems, and I think it's not very well understood generally that if you have an ordering protocol on your roll up that incentivizes latency gains, sort of the structure of the market, you're incentivizing is all actors to eventually colocate in one place. You may not get a roll up that has a single sequencer controlled by a single party, but you may get a roll up that is controlled by a single data center in a single jurisdiction somewhere subject to a single set of laws, which is a huge surface area for regulation risk. As you were saying earlier, John, I think it's only like one step worse than having a single sequencer, having your entire chain co locating in a single Amazon or similar data center somewhere else within the world. And what we really want is to have these systems geographically distributed across the world in many different places such that no single sequencer is able to impose its utility function on the network and arbitrarily censor a set of transactions. And the other thing that I wanted to follow up on what you were saying is sort of pushback against the notion that the PBS style is only useful. I think what you were trying to say was if you don't have kind of trust requirements between Validators and if you have a smaller set that is more trusted you can use something like mevga for your sending bundles in the clear. And I think what is interesting about PBS is not only this commit and reveal scheme and the privacy in it, but that it gives you the ability to iterate more quickly on features that are user facing than what Validators would. So if you've ever worked with these companies that run Validators right now they're not going to roll out features as quickly as your builder 69 or your beaver builder does on ETH mainnet. And to bring this back as an example of a feature that you could see being offered by a builder on a roll up but not on probably won't be rolled out as quickly as a Validator would be. Something like pre commitments where a user is paying a small fee to a builder and in return the builder commits to including the transaction in a certain place with certain state. Before that the builder's block has actually been included on chain. So I think this PBS style is still useful even in the absence of super hard trustlessness requirements that you're gesturing towards.
Speaker B (00:14:59.070 to 00:15:29.026): That is a very interesting answer to me and I think maybe we should jump ahead to kind of this idea of roll up sequencing and shared sequencing because you were saying that you would expect block builders to offer precommits or soft commits as a service for users. Who do you think is kind of in the better position to offer this? Do you think it will be Blockbuilders or do you think it will be the sequencer who ends up selecting the block builder or kind of the proposer in that kind of system at the.
Speaker D (00:15:29.048 to 00:16:16.846): Roll up level I would fully expect it to be the sequencer. I think that there is a use for that type of service on Ethereum L one particularly because the block times are so slow. I don't think you need that kind of thing at the roll up level because that's the whole point of putting on a roll up consensus is that they're going to be giving you these half second, 1 second type pre confirmations anyway, which is much faster than the L one itself. And while there are potentially other features that builders could certainly give you. I don't think the pre commit is necessarily as valuable, at least at the L two level compared to the L one level on that kind of typical UX because you don't have these super long block times anymore at the L two level. And there certainly could be other features that builders give you and are very helpful with, but that one in particular I would guess is less applicable.
Speaker C (00:16:16.958 to 00:17:09.480): So I think the unsaid reason why I brought that up as a feature is I think if you start from the place that these consensus sets need to be decentralized and they have centralizing tendencies and you lose some of the properties that you want. If they're all colocating in one data center, then how do we provide users this really fast confirmation of the transactions being on chain in the absence of having low block times? And I think pre commitments is one way to do that, even if you have block times that are a little bit slower. That's why I think they are still interesting in the roll up context because I think we may have to lengthen these block times anyway and like the 300 500 milliseconds instant confirmation thing may just be an aberration of the past. I'm not sure that that is true, but I think that is an interesting argument and like an alternative way we can service those needs and why I brought it up.
Speaker D (00:17:09.850 to 00:17:33.950): Do you think that any roll ups will take that trade off of trying to look more like a decentralization goal of an L one, where even the L two S consensus itself has longer block times and say a very decentralized committee? Because at that point I would just say why not just use the L One as your sequencer? You do some variation of a base roll up if you want those super strong guarantees.
Speaker C (00:17:37.250 to 00:18:35.550): Well, so I don't know whether a roll up will try that. I would like someone to try. And I think the design space here is pretty broad for doing a little bit faster, little bit faster block times than L one, but not a based roll up. Maybe based roll up, but with precommits. These are all different trade off spaces that I'd like to see tried out in practice. I don't know what is the best, but I do worry that this desire that users have for super fast block times and the implementation of first come, first serve would lead to geographically centralized chains which would lead to censorship over time. And that's sort of what I'm seeing in the future and I think this is an interesting design space to play in. But frankly, I don't know what to expect people to adopt in the future. The only thing that I would touch on is I think there are reasons why you may not want to be a base roll up independent of this too. So that's sort of a different dimension of choice than lock times and precommits.
Speaker A (00:18:35.890 to 00:19:20.350): Hey, guys. I almost hate to jump in on such an interesting conversation here, but on the off chance that there are a couple listeners out there who might not be fully following everything that we're saying here, I'd love to. Almost. Just back up for a second and just level set on sort of a definition of, you know, at the risk of almost starting at a pretty basic level, maybe, John, I could call on you because, again, you've done so much work on this. Can you just kind of give us a high level definition of what a sequencer sort of looks like today? And for an audience that's very familiar with what a validator's sort of does from a functional standpoint, could you kind of compare and contrast how they're different? And then maybe at the end of that definition, if you could kind of segue into when we talk about a decentralized network of sequencers or shared sequencing, if you could kind of feed into that definition, I think that'd be really helpful.
Speaker D (00:19:21.330 to 00:23:43.250): Sure. So the way that roll up sequencers work today is quite simple because it is one person running it for each of them. As a user, I send my transaction to this centralized sequencer. This centralized sequencer then determines based on what they've received, the ordering of those transactions, and they give immediate soft confirmations to users, which is really easy for them to do because in particular, they're not doing any kind of complex ordering around them, waiting for long block times or anything. They're basically just giving you out first come, first serve as they receive them. We give you a soft commitment of hey, I will eventually put this on the L one in exactly this order. So you are trusting the sequencer for that ordering. They could lie if they wanted to, of they could reorder them, put them in a different order based on that feed that they've put out there because there is no explicit penalty against them. So they're giving out that real time feed. And then after the fact, as they're doing this feed, as they have published that and the users have the soft commitments they're applying the deterministic, whatever it is, for that roll up state transition function of, okay, I have all these transactions. I execute all of them and I compute. What is the updated state of all of those things? So generating a new state route, et cetera, and pointing out, because this will probably come up later, that sequencing and that determining the state and executing while they are the same party today, those are two logically distinct roles which can be separated. So just an important point because I'm sure we'll touch on that later, but what they are doing today is they're also executing and generating that state. And then eventually they take that updated state, they post it on the L One along with all the transaction data, et cetera. And that's the point at which you actually have a confirmation now of, okay, any full node can look at the data that's been posted to the L One, the state route that's been posted to the L One and say, okay, this is definitely going to be the state of this roll up. It in the case of optimistic roll ups, the L One isn't aware of what is finalized because you have this fraud challenge timeout period. But anyone, once it is posted to the L One presuming the full transaction data is there. Anyone can compute this will be the state of this roll up. So the difference is going to be obviously decentralizing. What does that actual sequencer role look like? So a lot of that is today you are entirely trusting that sequencer for real time censorship resistance liveness and the ordering of your transactions. They could censor you in real time, they could go down, they could reorder them as they want, they could do all of that stuff and there's really no explicit penalty against them. So decentralizing that role in some way, whether that be just a simple leader selection algorithm or you just implement a whole new consensus mechanism on this layer two that looks very much like an L One at that point, such that there is an actual penalty. So in the simple example there, if, say, we throw tendermint on a roll up to decentralize that sequencer, as opposed to the centralized sequencer just giving you out the soft commitment, you now have a whole validator set on that. L two that even prior to the layer one confirming it, says, yes, we also agree that this is the ordering and this is what will eventually go on. The l one. So even before it gets posted on the L One, if you tried to change that ordering, they would be slashed for reordering the chain at that point, because they have made now a much stronger, soft commitment to you with the economic weight of whatever their stake is, of saying, yes, this is actually what will end up on the L One. And that also improves the censorship and liveness of the system. Just because you are no longer waiting for just one operator to potentially censor you to include your transaction, you now have maybe you're cycling through 100 validators. So it improves your real time censorship resistance and ordering guarantees, with the important point that for any mature roll up you should get eventual censorship resistance and eventual inclusion at exactly the same rate guarantee of whatever the L One is itself. Because you should always be able to, if you have a mature system, be able to force inclusion directly into the L One itself and do stuff like that, such that even if the entire L Two is censoring me or they've gone down that I can always get in through the L One directly. But those real time guarantees get much, much better as you decentralize that operator.
Speaker A (00:23:44.390 to 00:24:38.070): Yeah, and I really like the example, actually, of just imagine the issue with being a single Sequencer roll up is obviously okay for now, but you would never have that at the layer one, for instance. Right. Imagine having one validator on Ethereum. That defeats the entire point. And I think users can kind of intuitively grasp there. That's probably not a permanent state of affairs, but in the meantime, it's definitely sort of a honeypot for mev type activity, maybe either Robert or John, if you want to. Just before we get to that very important point about Sequencers kind of combining the role of proposer and builder that we've separated on the layer one, could you actually just talk a little bit about the different models that we're seeing in terms of Mev on roll ups? Like, if you could even sort of compare and contrast the Meva the auction sort of style that we're seeing on Optimism versus Arbitrum, it's kind of wanting to be a fair ordering type roll up.
Speaker C (00:24:38.140 to 00:29:40.238): Whoever wants to take that, I can start riffing. And John, I'm sure you'll follow up when I'm done. So, Meva was this really early idea for how you handle mev. It predates the official launch of Flashbots. It's in the E Three research post, I think, by Carl from Optimism in 2018 or 2019. And at the time, roll ups were just bubbling up as an idea. It wasn't as mature as thinking now. And I think the team was thinking about ways to perpetually fund roll ups and perpetually use that funding that roll ups get for public goods. And Meva, which stands for Mev Auctions, was this idea that the roll up sequencer would auction off the right to propose a block in an open market. Every single block you could bid, hey, I'm willing to pay one ETH or two ETH for the right to sequence this entire block, which is the idea behind Mev auctions. It's kind of interesting because there are some reasons why this structure isn't a good one and why the Optimism team has moved away from Meva. And it differs a little bit from sort of the PBS Memphis style market that we see on ETH layer one. And the way that it differs is that as I remember the Meva structure, it is you're auctioning off a block in advance. So you are buying a block in the future as opposed to bidding in real time for the existing block, which is slightly economically inefficient compared to a bunch of people bidding in real time for inclusion of a full block like you have in Boost today. Both of these auctions are in contrast to first come, first serve, which is being pursued by the Arbitrum team right now, and the way that most roll ups actually work in practice today, where the sequencer is attempting to. Include transactions in the order that they receive them with sort of no way of expressing your preference of priority over other transactions. And this can be problematic because it incentivizes one of two things depending on the economics of the domain. The first is it incentivizes spam. So if your transaction is just included on a first come, first serve basis and there's really low fee block space, then a way that you could optimize for mev is just to hit the sequencer as hard as you can and do your mev search on chain. So you actually implement like a background Arbitrage model in a smart contract and just repeatedly hit your smart contract trying to backrun transactions, probabilistically and reverting. When you're not able to successfully do that and as a searcher, you have no idea when this is going to be successful. But you're outsourcing this work of doing it to the sequencer you're using up gas on chain because it's so cheap and a single time when you probabilistically are able to find some Arbitrage on chain is going to pay for all of your reverts. So this is kind of the way that we saw first come, first serve play out on Solana as an example where searchers would send millions, tens of millions of transactions to try to capture a single liquidation probabilistically hitting the chain as much as possible. The second way that you see this play out on first come, first serve domains which is like Arbitrum today optimism as well since they're also first come, first serve today is optimizing for latency. So you try to find where the single sequencer is colocate with them and optimize to the absolute smallest millisecond possible of latency in your code to be able to have the first transaction hit the sequencer possible after some enemy extraction opportunity is created and this creates all sorts of games that are being played today. Arbitrum had a problem where there was an endpoint that searchers would listen to on their sequencer and searchers would try to open up many many different connections to this endpoint, I think tens of thousands of them and it was really hurting their sequencer's ability to operate an efficient process. And searchers were doing this because they're able to get some level of edge in the latency game here. And like I said, the other externality of latency games is that incentivize colocation in a single geographic location and over time. This is a vector for censorship. These are kind of the three broad different ordering protocols that I would highlight to you right now. There's kind of PBS style on e layer one with Map boost where you're biding in real time, mev auctions where you're bidding in advance in the future and first come, first serve which is trying to include transactions as they come. Is that what you're asking Michael?
Speaker A (00:29:40.414 to 00:30:13.730): That's exactly it, yes. Maybe if we could actually dig into know actually hasi maybe could I turn this over to you here because I know you gave a great talk about this in mev economics, which was definitely worth watching and I would highly recommend that. But basically on the importance of PBS on roll ups or layer twos. So maybe know Robert's explanation there serving as a jumping off point. Could you describe why it's important to kind of transport this concept of PBS from ETH main chain up to the roll up layer?
Speaker B (00:30:14.550 to 00:32:21.370): Yeah, I think it kind of connects to something that both Robert and John were already talking about, which is right now, the sequencer in these layer two domains basically plays two roles. It proposes new blocks, but it also decides on the ordering of that block. And in order to decentralize the sequencer, you need to come up with a mechanism that doesn't introduce decentralizing tendencies that especially Robert was mentioning, to either spam the chain or optimize your latency to the degree that the entire chain will kind of centralize or that the chain sequencing will centralize into a single geographical domain or jurisdiction. We actually know one mechanism from ethereum layer one. It satisfies all of these requirements and that is kind of to have some leader rotation mechanism with a decent amount of block time so it's not so low that kind of latency plays an overwhelming role. And then you outsource the construction of these blocks to a professional market of block builders and you kind of manage to isolate the centralizing pressure on the block building role that way, and in a role where it's kind of more easily contained and you try to create as much competition as possible in the block builder market. And so I think the point of my talk was that there are different mechanisms to decentralize layer two sequencers and the PBS that we know from layer one is still the best thing that we have. And so I think it would be very worthwhile for more layer twos and roll up ecosystems to think about how they can best port this model to their respective domains.
Speaker A (00:32:22.670 to 00:33:31.440): Yeah, absolutely. And I think that's kind of a good way to frame the rest of this discussion. So maybe this is a little bit tough because this hasn't actually happened necessarily yet, but let's try to focus the rest of this discussion on kind of the sequencing role on the rollup layer and talk about kind of that roadmap to decentralizing sequencer set or even shared sequencer sets. And then I want to talk about decentralizing the role of block building. And then Robert, this is really where we're probably going to call on you a lot to talk about suave and what that's going to look like, but maybe thank you for that. So before we get there, John, can we talk a little bit? Because I know you've written about this pretty extensively about what the roadmap looks like to decentralizing sequencer sets. So maybe for listeners, I almost kind of picture this as if you talk about decentralizing sequencer sets that's one roll up with many dedicated sequencers, that sequence transactions in that particular roll up. And there's kind of this other world of shared sequencer sets where say, hey, there are a bunch of different roll ups that settle back to Ethereum. Wouldn't it be great if there was this one network of sequencers that sequence transactions for all of these different roll ups? So could you kind of talk about the roadmaps for both of those?
Speaker D (00:33:31.890 to 00:34:33.666): So the basic consideration that a lot of these roll ups are going to be taking when they're thinking about how do we decentralize this is roll ups want to effectively inherit the guarantees of what their Layer One is eventually. So that includes the censorship, resistance and liveness of that system. So the simplest way to do that is a form of shared sequencer is just let the Layer One sequencer blocks for you. Effectively, this is some variation of what's been called total anarchy based roll ups. Pure fork choice, there's a number of them. But the basic idea is you're letting the L One choose your ordering for you. You have this kind of PBS interaction at the Layer One where that's how your block gets selected. It works, but you lose value to the L One. In the simple implementation of it, all of the value would go to whoever is the Layer One block producer something that's a good thing. That is an argument that, for example, Justin has made in Base Role. So that is a desirable thing. Whereas I would point out that you.
Speaker B (00:34:33.688 to 00:34:48.782): Don'T actually lose all of the value to the Layer One because you can create kind of systems where the Layer One sequencer has to burn some amount of tokens. So very similar to kind of an ERP 1559 mechanism.
Speaker D (00:34:48.946 to 00:38:41.730): Yes, you can. There are still ways to get value back to your roll up. It is trickier to do, but you can do it. But yeah, in the simple implementation, at least that is what it would look like. And for certain layers that is not necessarily something. Yeah, it also looks very different depending on what your Layer One is. Whether it's something like Celestia, whether it's something like Ethereum. Do you have a smart contract that's arbitrating this kind of thing? It does start to look different across different data layers. And in particular when people were starting to talk about sovereign roll ups on bitcoin where they were very against it because you could try to send that value down and potentially roll ups don't want to for whatever reason, whether that be regulatory or otherwise, that they purposely don't want a token, et cetera. In which case it is forcibly going to go to the L One if you don't implement that mechanism. But yes, it is not a strict option that it has to lose all of the value to the Layer One that is a good point. To be clear. The other main thing with this that I think is the bigger problem, quite frankly, is just the UX of it, of, okay, you're back to the layer one now of you get layer one block times. There's no really practical, effective way to implement faster block times that's good if you're doing any forms of these layer one sequencing. So that's where it comes in, okay, for these layer twos, we implement something else in addition to what the Layer one gives us, where we're always going to eventually get those layer 100% guarantees at whatever the layer one block time is. The point of all these other mechanisms is can we get like 99% guarantees or whatever number in between those layer one block times because most users do not want to wait that long, is the reality of it. So the simple option that you can do is some form of you don't even necessarily need to do consensus strictly because the layer one eventually provides you consensus. You just do some form of leader selection, which could be you have basically tendermint, like minus the consensus, where it's just this kind of round robin style thing, and then you just inherit the layer one consensus. There are problems with this in my mind. When you don't have a consensus, particularly stuff like Liveness, you're not going to be able to increment the leader quicker than the layer one blocks. You're not going to be able to give any soft, good pre confirmations in this if there isn't any kind of consensus voting off on it. So the reality is, I think that most of them will implement some form of consensus. Whether that starts off as a proof of authority type of thing. I think most of it kind of gravitates towards you have a consensus that looks pretty similar to a layer one, whether that's tendermint hot stuff, whatever, some variation of that, that probably gets implemented at most of these layer Two seems like the simplest thing. If I was pressured as a roll up, whether that's regulatory pressure or whatever else to decentralize my sequencer right now, that is what I would be doing, quite frankly. The tried and true tested things of implement a simple consensus, set some form of PBS on top of that. The other idea that a lot of people are starting to get excited about right now is, okay, it's kind of hard, it's kind of annoying for all the roll ups to figure that out for themselves. What if we just have one layer that just figures it out for all of us basically? And that's that idea of shared sequencer where the layer one is a form of shared sequencer, but for the reasons that I described earlier, it's not very feature rich, you're going to get slow confirmations, et cetera. So what if we make another shared sequencing layer that is actually optimized to be a shared sequencing layer. And then all of these roll ups can just opt into this shared sequencer and say, hey, for all of these roll ups on top of it, we will just use you as our sequencer. And then they can still give those fast pre confirmations. It's nice and easy for the roll ups, we don't have to worry about figuring this out for ourselves. But it does come with a host of other complications, which I imagine we'll get into as well.
Speaker A (00:38:41.800 to 00:39:33.886): Yeah, could we get into some of those complications? Because it's a pretty complicated area and one that's definitely developing sort of in real time. And from my understanding, when I hear people talk about shared sequencer, like a shared sequencer network, it's something that's far off in the future. So can you describe why that is? What are some of the complications there? And then one question that I always have listening to people talk about this is who do you envision these sequencers being? From one standpoint, I could imagine validators sort of on layer ones, kind of like the block daemons or figments of the world saying, hey, what makes sense for me and my business is to go and sort of be a sequencer on some of these layer twos. Do you see them being a totally independent network of sort of single sequencers or can you give us kind of an idea of who these sequencers might end up being in practice? And then, yeah, some of the complications.
Speaker D (00:39:33.998 to 00:41:50.558): As far as just who the entities are, my strong guess is that they are very similar, the same entities. It's a very similar role potentially at higher resource requirements, since some of them will be running at a bit higher speed than, for example, layer one, ethereum, but probably not that different from a lot of other chains. I think it'll be quite similar from that perspective. And then as far as what the shared sequencers actually look like and what the difficulties are. So the basic idea of them, at least for most of them well, I'll start with the simple case of you could have a shared sequencer that is just it is a fully stateful, normal sequencer that works the way that any proposer normally works, where I'm fully executing all the transactions for all of these chains that can obviously only scale so far because I, as one person, am probably not going going to be fully executing a thousand chains. And if the goal is actually to stick a ton of roll ups on top of this thing, you probably need to get a little bit more creative of, okay, how do we scale this thing and open it up to everyone and make it easy to deploy to? So the main idea that the two biggest shared sequencers have announced so far, they have a very similar concept, it's espresso. And Astria are the two that have announced recently, there are others who are building as well. But the basic idea of what they're doing to try to scale this out to many roll ups. Hey, we want this future of a million roll ups and we want to be able to provide the CC service. Is back to that point that I was describing earlier of while a sequencer today does both of those roles of we tell you this is the order of transactions and then they execute and they compute the state after that. Those are logically separate roles. And so these shared sequencers completely strip those two parts out. So these shared sequencers like Espresso and Astria say, we will order your transactions for all these roll ups and say, this is the order that we're going to include them in, but they do not execute any of them. So they have no notion and understanding of what these transactions actually are at all. So that removes the really difficult part of this because okay, now I don't need to hold the state for these anymore, I don't need to run the computation of executing these anymore. It's very simple. I'm just a dumb pipe that is saying they go through in this order.
Speaker B (00:41:50.724 to 00:43:19.690): If I can jump in here. I thought about this a fair bit when these came out and to me it really seems like such a huge cop out in a sense because you can claim all day long, oh yeah, my proposers don't have to execute any transactions and so it's super scalable. But guess what? On Ethereum layer one, the proposers also don't have to execute any transactions, right. They already commit to a block without knowing what's in it. And the same will be true in these new shared sequencing layers, right? There is a party that still needs to execute all the transactions because they need to know the state change that every single transaction will cause because otherwise they couldn't compute the most efficient ordering. And that's the block builder, right? And so in a sense they say, yeah, the sequencing layer is going to be decentralized, but so far they are not really presenting any idea for how the block building layer can be decentralized, right? Because when you have a shared sequencing layer, maybe there's 100 domains connected to it, then effectively what you need is you need a super builder that simulates state changes across hundreds of different domains and it'll just be insanely resource hungry and centralizing. And so I think that's kind of the downside, right, of cross domain mev and of shared sequencing.
Speaker D (00:43:21.230 to 00:44:44.280): Yes, I definitely agree with you. It does make it very easy on the sequencing layer because you completely remove that you no longer need validators that need to re execute all these things. So, yeah, you can in theory have a very decentralized shared sequencer set, but yes, you are pushing that problem off to another layer. And that's why if you talk to the teams like Espresso astra they're like yes, this does not work if you do not have someone sitting in front of the shared sequencing layer who is actually knowledgeable as to what these transactions are because otherwise, yeah, they're just including them in a random dumb ordering most of them probably won't even work. It'll be incredibly inefficient ordering for them to be good. You do need some form of builder to sit in front of them and is that going to be one gigantic super builder who's centralized or is that going to be different builders running for different domains and they are accepting bids for different domains starts to get a little more complicated the more that they're accepting there. But in theory what you would obviously like to have is a decentralized block builder that is sitting in front of them, something along the lines of suave such that, yeah, you don't need this gigantic centralized entity to sit in front of them because otherwise they are entirely reliant on that centralized entity to determine the ordering. They're not going to be able to enforce complicated things otherwise they do need that entity sitting in front of them.
Speaker B (00:44:45.050 to 00:45:31.206): Maybe Bert to rely on you here. So we kind of talked about how shared sequencing allows for the extraction of cross domain mev through this kind of notion of enforced cross domain synchronicity. But there are other forms as well to kind of get more efficient at the extraction of cross domain mev that don't necessarily rely on a shared sequencer. So could you maybe walk us through what these alternatives are? For example, depending on whether like for a chain that is not part of a shared sequencer or a chain that maybe doesn't even have the notion of proposal builder separation, I think the sort.
Speaker C (00:45:31.228 to 00:47:06.150): Of two dominant models in the market would be or at least ideas that I know about. And maybe you're hinting at some third hidden idea that I don't know about hotsu the two models that I would know would be shared sequencing. So providing some atomic synchronicity between domains, we've already talked about that. The second is that you may not be able to provide technical atomicity, but you could provide economic atimicity to users. So another way of saying this is that users could express their preference that I want transaction one included on domain A or something to happen on domain A and I want something to happen on domain B and I'm only willing to pay if both of these conditions are met. And in effect by providing this economic ethnicity you get the nice properties of technical ethnicity but you are outsourcing the execution of that to specialized parties that can break that down and make it happen on two different domains in the absence of some actual communication or shared sequencing layer between different domains. And in that way you don't need any integration between these domains, you don't need the domain to adopt a particular type of ordering model, you don't need PBS, you just need sophisticated actors that can understand risk and are really good at executing across these different domains and those actors getting these economically atomic bids. So is that what you're gesturing at? Hospital.
Speaker B (00:47:07.290 to 00:47:36.254): I think it hints basically to the question whether all chains will end up being sequenced by another chain or whether there's any alternative to that world. And I think the idea of shared sequencing is quite compelling. But I'm personally also very interested, maybe to hear how we can provide similar guarantees without kind of just giving into that idea that everybody outsources their forks rule in that way.
Speaker D (00:47:36.452 to 00:49:56.300): Yeah, one thing I should probably clarify because I realize it is a bit complicated and I didn't talk on exactly the guarantee that a shared sequencer gives you. So following from that example that Robert gave you so say someone like you express a preference to suave of hey, I will only pay if I get my transaction. Say I'm doing an atomic arbitrage I only want the leg on roll up A to execute if the leg on roll up B also executes. So a Swab executor can try to fulfill that preference for you and bid on both of those. So what they can guarantee you is what the state will be if I execute these things, because they're stateful and they're knowledgeable of what the result will be. So they understand that what they can't guarantee is to your point of will both of these chains include these atomically in these blockites. And so that is the part that a shared sequencer can give you, is that kind of last leg of cutting out that last risk of. It can't tell you what the result of executing these things is, but it can tell you that I will include these two transactions at these two block heights and then the builder is what guarantees that other part of if they are included at these parts, this is the result of what they will be. So it is cutting out that last bit of risk. It is unclear how useful that is to be quite frank. I don't think that's honestly a selling point for roll ups to opt into it at the very least of we get better assurances around cross domain MEB. I think that the selling point if shared sequences are going to be successful, it is the laziness of plugging into them and potentially they offer better network effects. I don't think that you're going to see roll ups opting into it because we can probably extract better cross domain mev more efficiently and return more value to our roll up. I don't think that's a strong selling point for them quite frankly. The one other thing I would say as far as how to get cross chain atomicity, it's a very different approach. It's also not live, but it's what Anoma has been looking at with Typhon, where you can have what are called chimera chains pop up, which are effectively on demand kind of side chains, where if you have validator overlap between different chains that are running Typhoon Consensus, they can make those multi chain atomic commits kind of on demand, making those atomicity guarantees. But it's a very different approach and you need to be using Typhon to do that.
Speaker C (00:49:56.990 to 00:50:01.200): It's like a sort of analogous what's going on in the Cosmos world right now. Right?
Speaker D (00:50:01.890 to 00:50:40.650): Yeah. Typhoon is a replacement for tendermint, effectively is what it is. It's a kind of next iteration of that. There are other changes as part of it as well. But that is one of the major changes that would be applicable here is if you have Validator overlap between different chains that are running Typhoon, which in practice is the case that Validators tend to be overlapping between different chains, they can make multi chain commitments. So you get a weaker economic guarantee of whoever the Validators overlapping are are able to make this guarantee for you. But it is a very different way of trying to provide some kind of multi chain atomicity guarantees.
Speaker C (00:50:42.030 to 00:52:25.990): I think I want to come back to the question you asked Hasu, which I thought was a very interesting prompt of will all chains be sequenced by another chain in the future or if there's some alternative and I'm kind of musing on it as we talk here. And my intuition is that you can't get away from having your chain sequencing be influenced by another chain and it's just degrees of how much you sort of want to submit to this or be hostily taken over by external forces in the market, to be honest. So at the limit, if you want to do first come, first serve and you have low block fees, what you're going to see is a market for spam come up in the world. So some chain somewhere or some sort of abstraction that allows a user to say hey, spam my contract as hard as possible because I have this economic preference, I want to land this liquidation or something like that. And this to me is kind of a hostile auction that's taking over what should be a first come, first serve model and I don't see how you prevent that from happening. The alternative would be something like a market for transaction ordering on top of first come, first serve, where you're paying sequencers that are supposed to be honestly reporting when they receive transactions locally instead to dishonestly report those because they're getting paid to do so. So I don't see how you can prevent these kind of out of band payments. And that would lead me to the conclusion that all chains are going to be influenced by the ordering of another chain.
Speaker B (00:52:26.150 to 00:53:21.322): When you say all chains are influenced by the ordering, I think this is even true today, right? Because the biggest block builders on Ethereum are basically today those that have the best connection to binance who are best at extracting kind of sex tax arbitrage. And that shows the extent to which the ordering of one domain is really contingent on privilege on another domain that is really important systemically. Right? And so even though Ethereum has a decentralized proposal set, it has like, it has this Ginormous centralization spillover from Binance. And that's why it's so important that actually more and more trading volume moves to these chains that do not have this kind of ordering privilege that Binance has.
Speaker C (00:53:21.376 to 00:53:44.930): I think it's even stronger than that, right, that it's so important that more and more liquidity and trading happens on actually decentralized domains rather than just moving onto crypto rails, right, because you can imagine Binance roll up, which looks exactly like Binance. It's exactly the same geographic location, except it's on a roll up. And I don't think it would have a substantial difference today if it's not actually properly decentralized.
Speaker B (00:53:46.410 to 00:54:27.250): So I think at this point it makes sense to build a bridge to swap. So, Robert, what do you think? So we talked a lot about kind of a world where now all of a sudden, the execution of transactions is spread across many different domains, right? So you have all of these roll ups, you have layer twos, probably even going to have a lot of layer threes. You have shared sequences. So how do you see the role of Swarf in that world? How do you think kind of, what is the starting point for users? And how does a transaction kind of track through this new and much more complicated domain?
Speaker C (00:54:28.150 to 00:57:06.610): Very good pumped, and there's a lot there to unpack hasu. I think from a starting point, I'm making the assumption that you need some notion of a builder on all these different domains because people will have complicated preferences on ordering and other types of actions, like pre commits that they want. So you need some builder that's like premise one. Premise two, it needs to be decentralized. And we can talk about why that is, if you're interested. And premise three, in order for it to be decentralized, you need some notion of privacy and some ability for parties to make commitments to each other to facilitate collaboration among untrusted actors. And so what I see suave, as is this platform for parties in the mev supply chain to make commitments and to communicate and manage their privacy between each other to provide ordering preferences not just to ethereum but all of these different domains the L three S, the L two S, l ones. Maybe even centralized exchanges at some point in the future. That's one part of your question you asked me about a user's journey, I think. And the way that that would work would be a user needs to have some funds on Suave, which is a separate domain, so they would bridge funds to that domain and they interact with these specialized contracts that allow them to express. Their preferences over another domain's ordering or another domain's state, you may have some preference, like, hey, I want Mike to come back to an earlier example. Like, I want this transaction included in domain A and another transaction included in domain B, and I'm only willing to pay if both of these things are included at the same time. So a user creates predominantly a signature, so not actually a transaction, but they create a signature that expresses this preference and passes it to the Suave Executor marketplace, where then Executors can use the privacy and commitment abstractions within Suave to collaborate on this and make sure that the user's preferences get executed. And only after that economic optimistic cross domain preference is executed can parties claim the reward that the user has placed on that preference. So that's a long answer on how I think about Suave and its role within these different places and sort of a simple example of how a user would interact with it and what's going on in the back end with executives.
Speaker A (00:57:07.110 to 00:58:47.106): Hey, guys, quick break from the show here. I want you to imagine something for me. Imagine swapping two stablecoins on Chain, paying $0 in gas and instead getting a rebate of $2,000. This is something that's actually happened on Chain. To understand how, I want to introduce and thank this season's sponsor, Rook. Zooming out for a second. The current state of affairs at Mev is billions of dollars so far have been extracted from users'pockets using Mev. Rook is coming in and saying, enough is enough. Blockchain should drive value to their users and the applications they use. It is time to leave the hobbyist era behind us if we want to move forward and we want to get this right. That's why Rook has built a custom blockchain settlement network, and it's one that gives you full control over the entire transaction lifecycle. Today, you can connect to an open source Rook node. The Rook protocol will automatically match, bundle, and auction your orders and transactions in seconds with zero gas overhead. Also, any Mev that's discoverable along the way will be returned to you, the user. Created as a collaboration between the industry's top mechanism designers and Mev engineers, rook was built from the ground up to be scalable, safe, and programmable. You can get your own Mempool, choose Searchers and Builders, and link your Mempool with others to discover even more Mev. You can define how the Mev is shared and delivered as well. And Rook can basically process anything from transactions to metatransactions and more. This is the way that Blockchains basically should have been from day one. So if you're a user listening to this, here's what I want you to do. I want you to go to your wallets, go to your favorite app, your node provider, and say, hey, I want you to be working with these guys. Rook. I want the Mev that I create to be redistributed back to me. If you're a developer and you want to stay ahead of the game, the.
Speaker D (00:58:47.128 to 00:58:48.146): Best way to do that is to.
Speaker A (00:58:48.168 to 01:00:13.870): Follow them on Twitter. They are at Rook or even better yet, slide into their DMs. They are lightning responsive. They'll get you set up today. And if you do slide into those DMs, as always, please tell them that I sent you. I was actually just going to know in all the research that we did sort of before this season, suave was the single most topic that people wanted to hear about. So Robert have sort of a bunch of questions for you and I don't want to get too deep in the weeds too quickly but I'm very curious about how standardizing preferences is going to work in so like kind of a question that I had and this was actually sort of flagged to me as something that searchers are pretty interested in is let's say you have two different blockchains with different block times. And let's say, to use your example, you want to atomically make it so that if you place transaction on blockchain A, like ethereum with a twelve second block time and then let's say, like polygon with a two second blockchain, and there's a block time, and there's an arbitrage that you want to place. Let's say this arbitrage kind of pops up 1 second into polygon and 3 seconds into ethereum, and you lock in the polygon leg of that arbitrage. But then there's still an enormous amount of time, right? 9 seconds in the ethereum block time. So how is Suave going to make it so that you can close both legs of that arbitrage, for instance?
Speaker C (01:00:14.210 to 01:00:36.182): So Suave isn't prescriptive on how exactly cross domain MOV is executed on. So we sort of anticipated a bunch of different approaches in the market that were going to be taken, like Typhoon, like shared sequencing, like other things. And we are observing and seeing how these different execution models are happening. But we're not prescriptive on one way or another. That's the starting point.
Speaker A (01:00:36.236 to 01:00:36.710): Got it?
Speaker C (01:00:36.780 to 01:02:10.818): And different users will have different levels of risk that they're willing to take. So some users will be willing to take the risk of hey, maybe one leg will fail and the other will not. And I'm willing to execute one part of the trade and not another. On the other side of this market, there may be executors that are willing to take that risk and they may charge some payment if one leg is successful but not the other. And some users may be willing only to pay or to make their trades if both sides of their trade are executed. And the sort of abstraction of Suave supports both of these different types of models where it's all or nothing even with your trade or only one side of your trade being executed. The complexity with that is if you want all or nothing with both of your trades being executed. If you want it to work in this way, then I think the assets actually need to be settled on Suave, the domain itself instead of on Polygon or Arbitrum. And that may be unacceptable to some users, but this is a possibility at least. And Suave is trying to support all these different potential models of who is taking on what risk, how much they're paying for it, and what sort of execution is happening in the background, whether that's Typhoon, whether that's shared sequencing, whether that's a market maker that's really good at evaluating risk on many different domains. So did that answer your question, Michael?
Speaker A (01:02:10.914 to 01:02:11.894): Yeah, it did.
Speaker D (01:02:12.092 to 01:02:14.054): Appreciate it. Yeah.
Speaker B (01:02:14.092 to 01:02:49.460): So the way I think we think about Swab internally, I think is definitely more as the demand side for these cross domain transactions and then kind of expose a lot of tools that allow for supply side also to emerge. Right. And these definitely include the shared sequences that John was talking about at length earlier, but also any other form. And they kind of get easier to build and integrate because swap provides kind of the tools for trustless collaboration on private data, basically.
Speaker C (01:02:50.950 to 01:03:36.630): And what I would say too, is that by aggregating this demand side, by creating, to your point, Michael, standards for how this sort of bid or value that users are willing to pay for cross domain atomicity by exposing that to the world, you then provide incentives for proposers to pursue cross domain atomicity solutions like Typhoon. Like shared sequencing. Because they see, hey, there is all this value that I can get as a proposer if I integrate solutions that make it less risky for users to execute on these types of preferences. So in that way, it creates a market that I think incentivizes the exploration of more cross domain solutions.
Speaker A (01:03:36.810 to 01:03:56.054): Excellent. I have kind of a continuing set of questions here, but I really like that idea of kind of trying to approach things from the demand side. I think many arguments, especially investing arguments in crypto, tend to be kind of focused on the supply side. So I love that approach. What's your sort of strategy from the swap perspective of aggregating that demand side?
Speaker B (01:03:56.252 to 01:04:18.720): I think, Bert, that might be a great time to talk about the order flow auction and how it relates to swap. Mike, one way that we've been thinking about aggregating the demand is definitely by kind of starting out at the next set of features that actually the order flow side of the market wants.
Speaker C (01:04:19.170 to 01:06:30.310): What we're seeing on ETH layer one in particular is the rise of what we call order flow auction. And in case your users don't know what this is, order flow auctions is this notion that you can send a transaction to an auction and that auction will auction off the right to execute a transaction behind yours, usually. And instead of the block builder or the proposer getting paid, it is the user who sent that transaction getting paid within this auction. So it's a way for users to internalize the mev that they create and get better execution. So an auction for order flow and we think this is super interesting as a way to live up to one of Flashpot's commitments of redistributing mev. And we saw that there was a great amount of demand in the short term for this. I think you need to have order flow auctions as a starting point, but you also need these to be decentralized. So I don't think it really matters what you're doing at the block builder level or the proposer level and how decentralized those are if the dominant way that users are interacting with the chain is through a single centralized order flow auction and one endpoint to the chain, basically. And so what we've been working on at Flushbots for some time is how do you create an order flow auction that you can decentralize, that is permissionless for any searcher in the world to participate in. And the follow up question that we're starting to work on now is how do you take the outputs of that order flow auction bundles that include users transactions with searchers as well and have those be used by any block builder in the universe? Because we don't want to create a world where only trusted block builders can get the outputs of these order flow auctions. So these are really taking what we're working on with suave of these primitives, of how do you have parties commit to each other in the MEB supply chain? How do you have privacy? And pulling them forward and anchoring them in something that we see that has market demand today in order flow auctions so we can ensure that this critical infrastructure is decentralized.
Speaker A (01:06:30.830 to 01:07:01.678): Yeah, I suppose my next question to you there is I have a pretty clear and actually order flow auctions and the increasing prevalence of them is definitely something Hasu and I are super interested in exploring. I think I've got a pretty good mental model for what a centralized order flow auction might look like, but I'm having a little bit more trouble sort of reaching in my mind for what a decentralized order flow auction might look like. So can you kind of describe some of the mechanism design that you guys are playing with at Swap and what would that actually look like in practice?
Speaker C (01:07:01.854 to 01:10:42.470): Yeah, so I would point your listeners, not readers, to read something that I wrote and we wrote at the Flashpots team called mev Share and it's on the Flashbots form. So collective Flashbots net, it details our design for an order flow auction which has privacy at the core and is permissionless for any searcher in the world to participate in. And the way that it does that is through this notion of privacy. So normally a user and a searcher are going to have a hard time collaborating because if the user sends the transaction to a searcher in clear text, the searcher can just take that front run, the user extract all the mev and run off with it. There's really nothing that guarantees the user that they get mev paid back and the user has no bargaining power as well. And there's no guarantee that they won't get front run either. So mebs share gets around this and it creates a permissionless market where any user can interact with any searcher through the notion of programmable privacy, where instead of sharing transactions in a clear text with searchers, we selectively share information about those transactions with searchers who can use that information to probabilistically extract mev. So instead of sharing your uniswap v two trade or sushi swap trade trade details, you could only share, for example, the pool that you're trading on and a searcher could see, hey, this user is trading ETH USDC. I don't know what direction, I don't know how much, but if the price of ETH USDC on units v. Two moves to this amount, then I'm willing to buy and it moves the other direction, then I'm willing to sell and still pay some amount for this. So the trick here is to share just enough information that you can optimize for me, but not too much, where the user gets worse outcomes. But by not sharing the full transaction, you enable permissionless searching on this. The other thing that I think is important here is this notion of commitments. So you need some way for the user to get paid for the mev that they're creating. And we have something we call a validity condition, which is passed on with the end user's transaction that requires that the user gets paid some ETH in order for the transaction to be executed. I'm sort of coming around to the decentralized bit of this, but these two things, privacy and commitments, are what we think enables mev share to work and redistribute mev back to users. And we're designing this in such a way that you're not relying on any trusted set of parties in order for the system to work. Or at least not in the. And in the future, we expect Mev share to be run just by a distributed set of nodes within a decentralized block building network instead of a centralized entity like Flashbots itself. And we think that selective data sharing is an API that can be run within this decentralized network, as well as the notion of kind of these conditions of validity that are added to users'transactions to ensure that they're getting feedback. That's a long winded answer saying we think privacy commitments are super important to the mev supply chain, enabling parties to work together. And these are going to be features that our decentralized blockbuilding network and suave offer and they're kind of pulled forward into a centralized world. In mev share are designed for an order flow auction.
Speaker A (01:10:42.890 to 01:11:33.258): Yeah, thanks robert, that makes an enormous amount of sense. And yeah, we will link to the Mev Share article that you just referenced there. I'd like to, for listeners, kind of give people a little bit more of a concrete sense of what Swab is. Now, I know there are sort of three components to it, Robert. There's the preference environment, there's the execution sort of market, and then there's the decentralized sort of builder. I also know that Swab is kind of its own chain, and I almost think of it as kind of an alternative mem pool for being able to express these sorts of preferences and they actually sort of get routed. So can you kind of just describe, again, almost less of like a high level and more of a concrete sense, just what exactly Suave is and what's sort of the timeline for development as well.
Speaker C (01:11:33.424 to 01:13:59.138): So Suave, like you said, is these three things of a preference environment and execution market and a decentralized block builder, and it is also a chain. So these are the same thing. And the way that they relate is that the chain is the place where you bring information from other domains and you expose it through Oracles that users can condition their preferences on. So the chain is how users are able to express their preferences and the chain, plus its messaging layer or its mem pool makes up the environment for these preferences. And when we were thinking of how to communicate this, we came to preference environments as this notion of one single place for all these preferences could be aggregated and accessed together. Because there are economies of scale of having these things in one environment because you can optimize for me more and the more preferences that you have in some place. So that's the relationship between the chain and the preference environment and how that works. We also wanted to delineate the execution market because it's a super important part of Swabworks. At the core of it is this notion of a competitive marketplace of specialized actors that can take these preferences and execute on them, regardless of whether it's ethereum, whether it is polygon with Arbitrum Solana, a Cosmos chain, et cetera. But it's very important to us that we have this specialized marketplace and it provides a lot of interesting benefits to users. This is again separate from the decentralized blockbuilding network because not all domains will have some notion of block building in PBS. And really the decentralized blockbuilding network is just a specialized instance of an executor within the executor marketplace. But it is kind of logically separate in some ways. I hope that makes it a little bit more concrete. You can think of Suave and how to relate all these things as Suave being a chain where you express your preferences on the backend, you have these specialized executors that are competing on them, and the decentralized blockbuilding network is just one example of that. Does that help concretize it a little bit, Michael, or are you looking for something else? And I won't be offended if you are. John is I see you on mute.
Speaker D (01:13:59.314 to 01:14:27.730): Yeah, I have some questions on this so I just wanted to clarify on some stuff. So you're saying as a user you express preferences via the actual swap chain itself. You have funds, there is a contract to express those bids. So do you need to be going through that process of using Swabchain to express a preference? If I just want to send my order through the order flow auction that will eventually plug into swab on the front end, is that the way that you would have to communicate to it?
Speaker C (01:14:27.880 to 01:15:05.658): I think if you have particular preferences that you want to express like in this order flow auction, I only want this order to be executed if I am paid one ETH as an example, then you'll probably need to condition like a special swab preference that includes those parameters on it. I think if you're just a regular user and you want to use Suave, it'll be as simple for you as RPC flashbots net and throw it at the RPC and the magic happens on the back end for so did that answer your question?
Speaker D (01:15:05.824 to 01:16:03.120): Yeah, another follow up kind of question I had on that was so as far as expressing these preferences of like let's say there's some arbitrage that I see, I want certain transactions to close this that I see on another chain. If that going through that process requires me to express a preference on swab chain to communicate like this is what I want done, that would imply that you're bounded by the swab chain block times of if another chain. That I want to express a preference for has a really fast block time, and maybe that opportunity is going to disappear. That would mean that Swab chain needs a fast block time to be able to express that preference in the first place. So does that kind of pressure just lead to back to the point of like you described earlier, of the pressures of low block times? Does that incentivize swapchain to have super low block times and then kind of have that race to centralization there?
Speaker C (01:16:03.730 to 01:17:18.870): I think it is a little bit different on swapchain for what it's worth because we're really only posting data about other chains and settling. So I think in some ways less important than other domains that we have. So I'll just point that out but you raise a really good point and I do think that SWAS block times are going to have to be faster and should be a little bit faster than other domains. On the other hand, I think there are ways to craft your preferences upfront that don't require you to communicate those in real time. So as an example you could offer to an executor on a domain that's really fast. Hey, if you spam my contract and it emits a log that says success, I'm willing to pay you some amount. And that gives an incentive up front for an executor to be spamming your contract at the precise moment where it needs to be without you needing to communicate that at the moment that it needs to. It'll just spam your contract a priority. I think you could probably do similar things with latency, and in that way there's like upfront ways to communicate your preferences to Prime Executors to be working on lower block time domains.
Speaker B (01:17:20.030 to 01:18:24.720): John, I would also point out that so you can think of Swarf almost as like a board where basically anyone can submit their transaction requests or transaction execution requests. And once you made such a request and it's floating in the swap mempool, then the request is basically out there. And anyone who goes and executes it, they can then come and claim their payment later on, after the transaction was executed, basically, they need to execute it. And then an Oracle reports the state change from the domain, and then the payment can be unlocked on the target chain. So that means that the settlement is not bound to the block time of the target chain. The settlement can basically happen at any time later. And so that's why, in my opinion at least, swap block times do not need to be lower than participating chains because it's basically okay for executors to claim their payment with a little bit of delay because they know the payment is trustless. If they have the transaction, they can get it included anytime later and then they're going to get their payment.
Speaker D (01:18:25.890 to 01:18:51.474): Yeah, to be clear, the settlement after the fact isn't the part that I was kind of getting at there because, yeah, that can kind of happen whenever. It's more that kind of initial step of if it requires a transaction on Swabchain to communicate that preference in the first place, then if that block time is longer than the domain that I want to express that preference for, I won't be able to express the preference in time. So that's where I was wondering where that centralization pressure.
Speaker B (01:18:51.522 to 01:19:06.570): I believe you can communicate all your and correct me if I'm wrong, Robert, I believe you can communicate all your preferences just through kind of these pre signed transactions. They do not actually need to be mined in swap chain in order to be commitments.
Speaker C (01:19:07.230 to 01:20:27.094): Yeah, that does work. We do have a notion of, like, a special transaction in SWAV that will carry your signed commitments. If you want to throw it in the Swab mempool, which is what you would want. If you want the maximum number of executors to access your transaction, if you want it to be censorship resistant, if you want to skip all of that and really save on latency and it's really important for you, you could communicate it directly with executors. That is right. Just with this signature model that we have been talking about. I do think you're touching on something, John, which you can't ever really get around just because of the laws of physics. There will be some cross domain mev which is not possible to extract because of latency and there is some kind of pressure like there is today from latency and cross domain extraction on domains to centralize and have faster block times too. I think that's kind of inherent to cross domain mev. I don't know that there's anything that we can do about that, but it is one of the reasons why it is important for us to, as a community, align on having real decentralization. And as a part of that, I think probably slightly longer block times than we do today in order to reduce this pressure of centralization from cross domain immediately in the long run.
Speaker D (01:20:27.212 to 01:21:11.230): A quick follow up question. Can you elaborate a bit more on why it is exactly that you need a chain in certain cases to express these preferences and to settle them after the fact as opposed to just having this kind of more peer. To peer protocol layer, part of Swab, where I can just communicate my intent. And if you execute it on the other, like you just get paid on that? Like, why do we need to go through this process of communicating that through swab and then having this oracle problem to go back to settle that payment after the fact of like, why can't we have this more global? Peer to peer layer and then just settle on those chains and, like I'll give you a payment on that chain if you get my thing done.
Speaker C (01:21:11.380 to 01:24:17.278): It's a good question, John. The TLDR of it, as I understand I'm me, if I'm wrong, is like, why do you need a chain, right? Why do you need a new blockchain for this thing? And there's a few reasons. So, in order to be economically efficient, we need some way of transmitting preferences. We think that is both as low cost as possible and dos resistant. One way to be dos resistant is to force attackers to pay a cost if there is spam within the network and with a blockchain, we can impose a fee during periods of network congestion by including preferences on chain and this would deter attackers. But we have designed a mechanism within Swab that we think allows as low cost as possible expression of preferences when there are not periods of congestion. And that requires you to change the actual chain itself, add a new type of, you know, I may be wrong and Tim Baco can yell at me if I'm not, but I don't think this is something we could get through ACD today if you're in the core development community. So practically, it allows us to introduce new mechanisms that we think are more economically efficient to express preferences while still having the property being dos resistant and a standalone peer to peer network that is global wouldn't have the same mechanism. At least I haven't seen any design for it thus far. The second more general thing other than this one specific mechanism for how you achieve low cost but dos resistant expression of preferences, is the notion that by owning the sort of full stack, by being able to change things on the full stack, it's much more flexible and we can iterate more fast on many different design parameters and make tailored optimizations that again, would be very difficult to retrofit on an existing domain and which you may not want on an existing domain as well. So example of this would be it probably makes sense for swap Chain to have a faster block time than Ethereum L One. This is just a non starter on Ethereum L One because it's optimizing for slightly different things, right? But we can offer that at Swap Chain. Another example is we thought of making swap chain a roll up that is using sort of the derivation function of the roll up in order to get trustless access to l one data and all roll up data so as to not need oracles that have any kind of trust assumptions to it or at least use the same trust assumptions as l one. These are examples of optimizations. One final one I'll throw out is we've thought about replacing the existing mempool within Geth with a different mempool that's optimized for even faster communication. So these kind of optimizations that we think will make suave sort of a better domain and specific for me. Does that answer your question, John?
Speaker D (01:24:17.444 to 01:24:18.734): Yeah, sounds great.
Speaker B (01:24:18.852 to 01:25:16.050): The last thing that I'd point out maybe is neutrality. So I think as swap kind of moves beyond ethereum maybe in a couple of years, I think there's also kind of the question is it fair that all of these preferences get settled on an actual settlement layer or should this kind of be like its own standalone neutral layer that actually has ownership and participation and so on, that's crafted in an entirely bottom up way from these different domains? And I think that's something that we're thinking about because part of getting adoption for a system like that is basically designing for political neutrality and maybe kind of the existing ownership of ethereum may not kind of be optimized. In such a way that it kind of gets buy in from Cosmos, it gets buy in from Solana, it gets buy in from these centralized exchanges and so on.
Speaker A (01:25:16.200 to 01:25:55.054): Yeah, that makes an enormous amount of sense. Guys, you've already been super generous with your time and I know we could probably keep going for another 2 hours, but maybe we could sort of transition to wind down. So we've covered an enormous amount of ground today and frankly I think everything that Hasu and I were hoping to chat with both of you about, we've already talked about. But if you want to just leave listeners with one idea or sort of bookend the conversation with either kind of a hope for how mev might play out on these sort of roll ups or maybe something to avoid or just anything that you want listeners to kind of take away from the conversation, maybe if we could just end with that.
Speaker D (01:25:55.252 to 01:26:39.386): Sure. So I'm definitely very hopeful that it doesn't start to just take years to decentralize these sequencers. I think there's going to be meaningful pressure to probably get that done in the nearer term. And when we do that, to hopefully do that in a way that isn't just going to incentivize all of the worst centralization pressures and kind of end back with the exact same guarantees, whether that's super high, latency first. Come first serve type roll ups. Because if that's what we kind of gravitate towards with them, then I don't really know how much that achieved. So hopefully building these systems in a thoughtful way such that we actually retain meaningful guarantees and decentralization across them.
Speaker C (01:26:39.568 to 01:27:39.230): I think if we want to decentralize these chains, and personally, I do, we need to decentralize the mev supply chain, and that's what we're working on at Flashbots. We think to do that, you need some ability for parties that don't trust each other, to make commitments to each other and to manage their privacy. And this is what we're trying to do with Med Share. It's an early experimentation for those things within the Use case of an order flow auction. If you're interested in this and interested in ensuring that mev doesn't become a centralizing force for Ethereum and every other domain within Crypto, please come work with us. We are super interested in collaborating with others, and we're going to be working on a bunch of interesting problems in the future, like how to share order flow and how to make the most efficient order flow auction possible. So if you care about decentralizing mev decentralizing crypto, please come work with us. Check out Mev Share and let us know if you want to collaborate.
Speaker A (01:27:40.210 to 01:27:46.930): Excellent. Thanks, guys. This has been a fascinating conversation. Thank you both so much for your time.
Speaker C (01:27:47.080 to 01:27:47.810): Cheers.
Speaker B (01:27:48.470 to 01:27:50.066): Thanks for taking the time, guys.
Speaker A (01:27:50.168 to 01:27:58.062): All right, Halsey. That was a great episode. Big payoff for listeners. I think that was a lot to digest from John and Robert.
Speaker B (01:27:58.126 to 01:28:17.962): Yeah, I thought this episode was great. I had this mental map in my head of all the things that I wanted those guys to cover, and we didn't even have to steer very much at all. Right. It just felt like they were jumping from topic to topic and getting to all of the important points. Yeah. So I really like this one.
Speaker A (01:28:18.016 to 01:28:50.694): Yeah. They both came out of the gate with a lot of energy. And for listeners, we actually recorded this at 08:00 A.m. On a Monday morning, so that was definitely tip top performance from the two of them. One idea that I thought was very interesting hasu that I wanted to get you to unpack the implications of a little more is that hypothetical that you posed about every chain being destined to be influenced by the sequencing of another chain? Robert had a very interesting answer to that, but could you kind of I'm still kind of trying to digest what the implications are of that. Could you unpack that concept a little more?
Speaker B (01:28:50.732 to 01:30:19.842): Yeah, so the reason that I asked this was that there is a lot of value to having cross domain synchronicity because it basically allows for better bridging, but it also allows for the more economically efficient capturing of cross domain MEB. And so I liked how John was actually also going into some of the drawbacks of that. Right? So one is definitely the value capture mechanism is less direct and it's also entirely not figured out how that would work. Also, I think the loss of sovereignty is also worth mentioning. And so I think in the Cosmos ecosystem today, we see a push towards sort of actual applications taking control of their ordering, like through becoming blockchains, right? And then like ABCI Plus and so on. And I think that shared sequencing is on the far other end of that spectrum where you actually give up all your sovereignty about your sequencing. It's not even like you are on one chain with other applications and you share kind of the same sequencing rules with them like you would on Ethereum today. No, it's actually like you are on a chain with all of the other chains and all of these chains actually share the same rules. And so that's one thing that I guess we didn't say about these chat sequencing networks, which is really that they all need to opt into the exact same sequencing rules in order for that to work.
Speaker C (01:30:19.896 to 01:30:20.500): Right.
Speaker B (01:30:23.830 to 01:31:49.742): I think that was worth pointing out. I guess the question that I have is really like how big is this economic pool going to be? So how big is cross domain mev going to be? And as a result, kind of this idea of all chains outsourcing their sequencing to the same chain. I think in a sense it's like really scary idea, right? Because the more chains are going to be sequenced by the same sequencer, I mean, even if that sequencer maybe doesn't execute the transaction, then as we discussed, there are block builders who will need to do that. And so that's why cross domain MUV is really such a big centralizing effect on kind of the builder market, right? Because it really drives up the resource requirements to be a builder, not just in terms of executing the transactions at the software level, also in terms of inventory management across these different domains, risk taking, balance sheet size and so on. It really creates this notion that kind of someone who wants to be good at cross domain or someone who wants to be good at block building must also be good at kind of cross domain arbitrage, et cetera, on these different domains. And I think that's a really scary idea.
Speaker A (01:31:49.796 to 01:31:53.920): Paul sue when you talk about cross domain MEB as an idea.
Speaker B (01:31:55.890 to 01:31:56.318): Do you.
Speaker A (01:31:56.324 to 01:33:29.322): Think that ever shifts from being primarily sex to dex arbitrage? Because I remember getting the first time that I got very excited about this. I'm sure for you this was years before, but in listening to The Atom 2.0 White Paper, this idea of kind of the interchange scheduler, I thought to myself, man, that's a very cool, very compelling value proposition. But when I was doing a little bit of my research in this season, actually I found when you don't ignoring sex to Dex arbitrage, which is a great profit center for a bunch of these builders. One of the problems when it comes to cross domain kind of arbitraging price differences that you might say on two decentralized exchanges, there's a reason it hasn't necessarily been the big honeypot that a lot of people thought it was going to be, which is one that complexity around different block times that I was asking about with Suave. But then economically, the spreads of decentralized exchanges are much wider than the spreads on a centralized exchange. So when you think about how juicy an ARB needs to be, you need to basically be able to pay for transaction costs on both decentralized exchanges. And the wider those spreads are, the less attractive the arbitrage is going to be. So I'm just curious how you think about cross domain mev evolving over time. Do you think it's this enormous, very sexy sort of pot of potential profits? Is it largely just going to be arbing, the price of binance, which is kind of what it is today, which is where price discovery happens. How do you think about the evolution of cross domain mev?
Speaker B (01:33:29.386 to 01:34:55.180): Yeah, I should caveat that by saying that I'm really not the cross domain mev expert at flashboards or really kind of in the mev space in I mean, I think you touched on some very interesting points. So definitely binance is kind of the domain against which most arbitrage happens today. And so there is a lot of cross domain MUV. It's just against a centralized domain. Right. And so I guess the question then becomes, well, how does that change? First of all, what does it mean for crypto? And I think it means today that many of the top block builders are engaging in kind of binance ethereum arbitrage because that's basically how they can best monetize their blockbuster. That and so this has this spillover effect, this centralization spillover effect, almost from binance to ethereum. And so how does that change? I think basically price discovery has to shift from binance to some decentralized domain, whether that's an application on ethereum or on a layer two or on layer three, or whether that's like its own totally independent app chain. But I think once another venue becomes the focal point for trading and liquidity, then all of a sudden I think Arbitraging against that domain becomes much more important.
Speaker A (01:34:56.110 to 01:34:58.474): Yeah, very well said.
Speaker B (01:34:58.672 to 01:35:28.146): Maybe there's another thing that we can kind of point out. So there are ways that basically cross domain Arbitrage can evolve and can kind of become more prominent even without binance becoming less you as a searcher. Imagine like you're a searcher on Ethereum today and so there's a lot of Arbitrage basically where you don't have any way really to close this without taking balance sheet risk.
Speaker C (01:35:28.178 to 01:35:28.326): Right?
Speaker B (01:35:28.348 to 01:37:04.500): And so maybe you don't engage in these at all. And so maybe there's very little competition for them because kind of the binance lack might be very centralized right, of the trade. And so now all of a sudden there is this shared sequence or this way to do kind of express these cross domain preferences, maybe let it be between Arbitram and Ethereum instead of binance. Right? And so now as a searcher, maybe I can just participate in that opportunity. Whereas previously this market was close to me and it doesn't matter that I'm less efficient at closing this Arbitrage opportunity than the searcher who does kind of the Ethereum binance Arbitram lag, right, or like Ethereum binance binance Arbitrum. What matters to me is that it is very cheap now to pursue this opportunity. And kind of the party that ends up getting the opportunity is not the one who can extract the most opportunity from it, but who actually can pay the block builder the most or can pay the validator the most. Right? And so I think what we'd see is basically the value capture for validators from these opportunities will go up because there would be more competition. So ultimately kind of the winning trade might still be done by whoever controls the binance leg of the trade, but the opportunity set at large will become much more competitive, the margins will get compressed and more of the revenue will go to the belted us on these respective domains. That would be one guess.
Speaker A (01:37:05.510 to 01:38:17.626): Yeah, I think you might be absolutely right about that. One question that I had for you as well, hasid, I didn't want to get too into the weeds, technically here, but you often hear, I'm sure, folks, if you've tried to learn about this stuff on your own time. There are a lot of these sort of new actors, especially, that get introduced with the implementation of things like Swap, like Executors. Right. There's also in mev share, there's the matchmaker and there are all these kind of new entities. And a question that I often try to ask myself is I close my eyes, I try to imagine who are actually doing these sorts of actions. And when I kind of think about this, I think from a business standpoint, it makes sense for the kind of consensus infrastructure providers at main chain to sort of move up the stack. So that's why I asked that question about Validators, like people who are Validating Ethereum. If that were me, I would have the competence of, okay, securing a chain at the main chain level. Then I would try to move up to rollups. And when I think about executors and what I was hearing Robert describe, I think that actually sounds a lot like searchers, right? Am I right or wrong in describing that? Or how do you think the activity migrates up from main chain to this roll up environment? Yeah.
Speaker B (01:38:17.648 to 01:41:25.640): So first of all, I'd say I think there is a reason why I think especially more technically or architecturally minded folks, they tend to introduce a new role. I think it's because of the logical separation. So anything that doesn't have to be done by the same role, they basically say this is a new role, even though in the beginning it might be done by the same role. In flashboards, we are going to run a matchmaker and we are going to run a block builder, right, even though a block builder can also be a matchmaker. So especially with regards to the executors in Swarf, I acknowledge it's like as a term or like a concept, it's a little bit confusing because it really makes you think this is going to be a new role, whereas it's much more of an umbrella term for a bunch of roles that already exist. So especially with regards to swap Validators. So that is not actually a new role, but that is really more of an umbrella term for different parties that already exist. So a Swarf executor is anyone who can get your transaction in at the target domain. And so if we think about who would be kind of the best party to do that, because the executor is also permissionless. So it's really about self selection, who will step up to fill that role of the executor in equilibrium. So we know at the farthest end we kind of have the validator because they have kind of the ultimate monopoly over transaction ordering, but they also might outsource this to a block builder and the block builder themselves, they might run an MEB auction where they outsource kind of the ordering to searchers, right? But then you may even have shared sequencers, right? So they do the sequencing for different domains and so they might be an executor, but then you might have chains maybe where there isn't even like a builder. And so then the searcher kind of might just go straight to the blockchain in order to get a particular transaction mined. So, for example, Robert was saying, what if there's no PBS on a chain and it just has kind of first come, first serve? Well, what's going to happen is there's no reason why like a kind of latency auction as a service shouldn't emerge because if there's some party who has this preferential access to latency on that chain. What they end up doing is they end up monetizing that in some way, whether it's in a proprietary way, but they can also outsource that. And so in TradFi, I think you saw that already a couple of times. So, for example, there are multiple privately owned microwave tower networks that are basically, I think it's on the order of like two to three times as fast as kind of the regular Internet, the way that they send kind of information packets across the globe. And these microwave tower networks, they are being used by all of the big trading firms. And so that is kind of the exact same concept that you would expect to emerging in kind of a more latency sensitive crypto system.
Speaker A (01:41:26.490 to 01:44:07.626): Yeah, the more I kind of dig into Suave, it's just so impressive just in how audacious sort of the scope of the work is. And I'm just very curious to see how it all plays out and gets implemented in practice because yeah, it's almost like every time I try to learn a little bit more about it, there's this whole new rabbit hole of concerns that I wasn't necessarily thinking about. So it's going to be phenomenal to watch you guys build that. I think my last question to you on this is, again, we're risking getting into some pretty technical spots here, but that's what this whole episode was. So I'll just sort of ask you when I sort of look at this idea of shared sequencers, the mental model that I have, again, I think Robert mentioned this during the episode, is actually borrowed a little bit from Cosmos a bit, and their kind of idea of interchange security and validators securing multiple different chains. And my memory of how that works in Cosmos is there's some kind of physical limit to how many chains one validator can actually validate. And one of the questions that I didn't really want to get into, but I'm very curious about is are those same limitations going to exist in shared sequencer networks in Ethereum? I would guess that they probably are. And we talked about the hardware requirements being a little bit higher as well. So I guess what I'm kind of trying to let me steelman something that I feel like is a non charitable interpretation of how all this is going to work. I have a feeling you're strongly going to push back and then tell me why I'm wrong. All right. With this sort of uncharitable interpretation. But I'm not sure in the same way that the US. Sometimes gets accused of we have very good labor practices over here in the US. But don't look at how we actually outsource the labor in kind of Eastern markets. Right? So kind of like the Nike example of, well, we have really sterling, very upstanding sort of labor practice over here, but really our sneakers get made over in Bangladesh where there's less good labor practices. I think you could uncharitably make that argument with Ethereum where we've limited what's going on on chain. And Ethereum at the main chain is this kind of very neutral settlement layer where anyone can be a tiny little validator, but you pushed some of the less desirable aspects of it just up the ladder to roll ups. And roll ups are where all the execution and the users are going to interact. And they have a lot of the problems that some of the L ones kind of get rightly pushed back on, right? You've got centralized sequencers. You've got a very strong sort of push towards latency because people want faster confirmation time. So what's the reason why that's not necessarily the case? And how is this shared sequencer network going to solve some of these problems?
Speaker B (01:44:07.808 to 01:47:36.910): Oh, man. I think this is happening to a degree, definitely like Ethereum kind of pushing these challenges of making Ethereum scale and providing a great user experience to roll ups. So I think that's by design, I think maybe Ethereum core developers were wishing that maybe some of the roll up choices with regards to UX have been different. But I think it's kind of owed to the fact that while roll ups are kind of noncustodial and they have this censorship distance mechanism, so you can always get your transaction mined through the layer one contract. I think all of these kind of gave the teams working on these roll ups in almost like I don't want to say what is a false sense of security, but the sense of security that they really had a lot more freedom kind of to make centralizing choices and operate or innovate on kind of the UX more so than maybe the decentralization roadmap. And I think the other point is just this is almost like a result of kind of very intense competition between them, right? Because the market forces just pushed all of them to launch before having fraud proofs and so on, before kind of being really decentralized or before having decentralized sequences and yeah. So I think I really have a hard time blaming anyone for their choices. If I've been in the same position, I'm sure that kind of I had felt the same pressure from market forces, and I think they all did what they could, right? And so I don't think it's anyone's fault. And with regards to kind of what you asked about Cosmos, I thought it was an interesting question. I don't know the answer to it. The answer why kind of Cosmos, like the inner chain security, has the scaling limit, right? We should definitely ask that in our episode on Cosmos. If I had to guess, then it's maybe a mix of the Cosmos validators. They basically execute all of the transactions, right? So they don't have proposal builder separation. So that's one thing. And the other thing is maybe kind of about stake hypothecation in the sense that maybe if they restake, quote unquote, like the Cosmos Velvet Hub stake too many times, then maybe the security for everyone starts going down after a while, right, because they basically spread the security out over too many different host customer chains. And so I could imagine maybe it's like a mix of these two or something like that. I don't think necessarily that shared sequencers will have the same problem. I think they will definitely have the problem of just the builder role in these networks is just going to be incredibly centralized just because the requirements of being a cross chain builder are just so much higher than being like Ethereum layer one builder. And yeah, frankly, that's why we have this very audacious vision of decentralizing the MEP supply chain, because not just on the single domain, but even horizontally across several domains by providing different parties kind of the tools to collaborate in a trustless way just so all of these roles don't have to be played by the same parties.
Speaker A (01:47:38.050 to 01:49:02.650): Yeah, well said. And by the way, I can't say that I would make any decisions differently than the layer twos on Ethereum would either. Although I don't know if you remember giving this quote, but it stuck with me. You said it in I think it was like an I Pledge Allegiance podcast a little while ago about people crypto. The intense cyclical nature of crypto actually culling people who make short term decisions. And I actually was listening to you at the time because it was during a bull market and not agreeing with that quote. And then frankly, now with the benefit of a full year of bear market and watching how it's played out, I actually do agree with you a little bit. And this is beyond the scope of this Mev podcast necessarily, but I found myself wondering about that exactly when I was watching this Arbitrum governance snafu play out and wondering to myself, well, do you really need that billion dollars to play around and kind of be fast and loose and match the polygon deals? Or is that actually not money that's particularly well allocated that way? And if you had a longer term framework on it, then I don't know, it's maybe beyond the scope of this Mev podcast, but I do find myself wondering that quite a lot, actually. All right, let's give listeners a little bit of a tease for the next episode here. So this next episode is going to be an interview with two searchers. So this is kind of a fun callback to the original interview with a searcher that you did.
Speaker B (01:49:02.800 to 01:49:07.194): It's got to be back in 2020 or 2021.
Speaker A (01:49:07.232 to 01:49:19.834): Yeah, so this one will be fun because we'll really get into the nitty gritty of what the searching role kind of looks like today and some of the PvP activities that go on for searchers.
Speaker B (01:49:19.882 to 01:49:50.038): I think it would be very interesting to also listen back to the original episode just to hear what changed in the two to three years that lay between these interviews. Because I think so these two searchers who are interviewing, I think they are really on top of their game. And I think they give us a great overview over what it takes to be a searcher today, how much more sophisticated it is and how much more resource intensive. I think those would be two takeaways that I'm expecting to get.
Speaker A (01:49:50.204 to 01:49:54.420): Absolutely. All right hasi as always, this has been a fun one. See you soon.
