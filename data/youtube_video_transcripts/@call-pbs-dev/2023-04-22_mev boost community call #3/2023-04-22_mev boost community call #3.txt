everyone welcome to Med boost Community call number three I will get the agenda here and we'll get started uh so there that's in the chat and yeah uh it's uh been quite a busy time since we last had a call um let's just dive in so uh let's talk about capella uh generally things went really well there was an issue that we saw with prism let's see actually if Terence is here I think he'd be a good one to give an update there although I don't see him yet yeah I'm here oh great do you mind I mean uh yeah do you mind yeah sure so uh the issue was basically very simple it's when a prison validator requested a blind blog it signs it but when it returns back to the relayer using the submit blind block um Builder API call we forgot to include the POs to exact change there so meaning that if there's any block that has greater than zero pos2e it's a change you will get a hash free root Miss publisher post mortem so highly encourage anyone to read that um from the postmodern we have created what we will do moving forwards to prevent this time happening but um in terms of impact I believe around like uh 50 blocks were missed because of this and um shout out to um all the relayers for being a very fast and responsive we were able to block the prism user Adrian stream so therefore um the damage wasn't that bad from that regard uh circuit breaker also came in handy as well so we shut off and uh find this uh Blast by me slots per Epoch so uh before circuit breaker we probably would have reached to eight to nine minutes um slots per EPA so that also came out handy as well so yeah I don't know if it's just on my end but Terence I think never cut it up a little bit yeah he's cutting off okay uh either way uh quick recap was yeah there was an issue uh with prism and auditing blocks um and yeah essentially there was uh some quick work by people on this call and others to to fix that uh roll out a patch and get it fixed um I did want to talk about testing a bit uh Chris who dropped his document about the relay perspective uh yeah so that'd be nice to have actually Chris or anyone else do you want to uh give an overview there yeah sure um so be from flashbot and from ultrasound we're observing we should pay off work together with people from prison and Lighthouse and the first things looked pretty good and then we started quickly to notice the additional missed slots and correlating this invalid signatures on get payload code at which point the release couldn't know if it's actually the proposal or not asking for the payload and then we started um jumping into action um uh trying to diagnose the issue with the CL teams and giving a heads up to the other release that were mostly understand by around then two and we quickly were able to narrow it down to being an issue with prism and luckily we could identify Prism by their user agent so we could prevent any bits being sent to prison proposals this means even though proposals running the the defaulty prison version they wouldn't miss any slot anymore but just receive no bids and fall back to local block production and then yeah a new prism release was rolled out pretty quickly that had the fix um then on the evening of the day on like Thursday there were still uh additional mid slots and we try to trace them down that turned out to be a configuration issue by proposers actually running the new prison release but having an invalid combination of CLI arcs and after that was basically settled it was the missed plot rate stabilized and then text normal rates I I would say like that's the quick recap here created pretty much everyone doesn't Stand By and able to deploy a minor update pretty quickly and I would also say this um yeah well it's a pretty good teamwork across really TL clients taking pools and and everybody I'm jumping into action quickly and Chris yeah thanks I mean definitely yeah a lot of nice work uh across all those different uh groups of people for sure um yeah does anyone else have anything they want to add uh on the actual incident um otherwise I do want to just talk about you know testing and how we can avoid this in the future sounds like a no um so yeah I mean again I think from this a lot of the relays uh and probably different staking pools have rolled out a lot of different monitoring and things like that just to shore up you know when things like this happen how can we react uh I know I think I saw these Lighthouse and prism and I think the rest of the clients are working on this if they haven't already um but adding the user agents to the mid boost request uh in the Builder API just so it gives us you know when things like this happen and it's you know scope to a client it gives us a little more flexibility around diagnosing and reacting and things like that so that's really helpful and then yeah I mean ultimately this was just like a bug in uh in prison and you know presumably we have lots of testing that that would catch things like this um so yeah like at least for me personally I'm working with uh some of the testing teams at DF to work either on spec test for this sort of thing or uh you know even into intestine Hive so we have this like uh pretty pretty awesome testing software called Hive uh that does more like end-to-end or like Network couple tests and it would be like a great place for this type of thing um so yeah definitely for the next hard fork and really just moving forward with any client release uh this is definitely an area to focus on okay anything else on capella that anyone wants to to address I mean generally you know like the hard Fork itself went really well and there were some things here uh with another boost ecosystem but again people work quickly and I think resolved all the issues okay if not uh we'll move to the next agenda item another fun thing that happened the week before uh this category of unbundling attacks so I think it's worth just having like a high level of what happened um yeah I mean I can give a high level summary or if someone else here wants to to give an overview um I believe this is with ultrasound relay itself so I don't know if uh Justin or Mike on the call and want to give an overview of what happened is Mike on the call uh yeah yeah I'm here Mike do you want to do it otherwise I'm happy to do it but I think you're a better place sure I guess just super high level probably most people have have read it by now but um basically on on Sunday night April 2nd we noticed that there was uh we got an alert that basically said that a a delivered payload didn't match the signature or didn't match the block hash of the block that ended up on chain for that slot um I poked around at it for a little bit but then kind of didn't think too much about it and then Justin in the morning correlated it with the with a unbundling attack that people were talking about on Twitter so basically what happened is um a proposer was able to send an invalid signed header to the relay the relay sent back the body of the block to the um to the proposal because the signature was valid even though the signed header itself wasn't valid so uh then the relay tries to publish the block the block is invalid but now the block has and the block contents have been sent back to the proposer for that slot so the proposer in the next seconds basically is able to to unwind the the sandwich attack uh the sandwich transaction so basically they had they had baited Mev searching Bots into into sandwich attacking some transactions now they had those transactions um in the clear they were able to run them backwards and propose a different block with a different header that was valid because that block was valid it became canonical very quickly and and got all that station away from that slot um and so even though they equivocated even though they they signed two headers the valid one was the one that they proposed that one ended up on chain and um they they ended up kind of walking away with like something around the order of 20 million dollars of search or money so that's kind of the high level perspective we were able to get them slashed because they did double side the header but obviously the slashing penalty was only one eth um they they made a lot more than that in the attack so that's kind of the the high level overview all right thanks um and yeah there's various uh docs that Chris put in the chat um one of them is essentially postmortem around what happened and yeah uh again at this point I think most people here have uh you know are familiar with this I think it's worth mentioning you know again what are some like mitigations or takeaways moving forward I think a big one is like just uh you know if you're listening and you're like a Searcher or a builder thinking about you know what risk am I taking on if there are you know reorg like things that happen what can happen to you know what I'm doing on chain um things like this so uh again generally I think people understand that mevs are very like adversarial space and you should basically assume that if something's possible uh it'll probably be uh triggered at some point I will say it's uh you know I've got us all thinking more about these different types of complication attacks uh and I'll just call it this uh this blog post here in the chat uh this is Mike and Francesco um yeah again Mike anything dad I mean I think you kind of went through uh the high level just discussing the attack but there's nothing worth calling out here this would be a good time yeah I guess the the big takeaway here is that the what made this attack super easy not super easy but what made the the um equivocating block that they proposed end up on chain was the fact that the the header that they signed for the other block was invalid so they didn't have to race and and try and compete against the block that was being published by the relay because the relay block was invalid so that at least that part of the attack surface has been patched and the the way we resolved that is the relay Beacon notes now check the validity of the of the block before they broadcast it um so this kind of protects us against the base case but the more General version of the attack is where they send a signed header back to the relay that is valid the relay publishes a valid block and then the the malicious proposer gets the gets the block from the P2P unbundles the transactions and publishes a competing block at the same slot so they are equivocating in here and in this case now they have to to win the test like to win the race for attestations in order for their unbundling block to to become canonical so it's the attack becomes more of a network level thing because they have to essentially beat out the the relay proposal um but yeah it's it's kind of still possible and under the current boost design there's not really anything the Relay can do to protect against it so in the blog post we proposed a new idea called headlock which has also been um talked about by by Dan in his his post um which is one idea for a solution but kind of as is nav boost is is vulnerable to this type of attack the blog post also talks about the relationship to this and and I tried PBS too so if that type of thing interests you um yeah give it a read I guess one one small clarification on the on on the attacks I think Alex you mentioned that you know the attack happened on on on the ultrasound relay so there's a couple points here like one is that um you know it it was a software bug effectively uh in in the in the math boost relay uh code base but the other thing is that um the attacker could have and I don't know I don't know if I haven't really have looked into their logs but the the rational thing for the for the attacker to to do actually is to uh to actually uh trigger the vulnerability on on all the relays that were vulnerable which was pretty much all the rear days and so you know the fact that we you know in a way we're on top of things and we look at the logs and we we correlated things you know suggest that yes the attacker did go through our relay but um you know a rational attacker would have gone through all the relays um and and you know potentially I've been able to to leak all this all these blocks and and and and make the the unbundling yet even more potent uh relative to just just targeting one of the relays yeah definitely I mean it points to more of a general hole and uh well in this case I mean yeah I think we can pretty squarely say this was just a bug uh in the relays but um yeah it does you know Point towards a more general idea of a bundling uh which even has consequences as far Regina say epbs so um yeah not to call out ultrasound in particular I guess one of the reasons why we notice is because we have like the the custom alerting so this is an alert system that that was built uh by by Nicholas uh kind of outside of the uh the flashbots uh implementation so we we get alerted through a telegram bot as soon as uh as we as we miss uh well basically as soon as a proposal signs a header which doesn't make it in the canonical chain um and maybe that infrastructure that we could uh we could open source I don't know it is open source okay so yes you're welcome to uh to use that if if other operators think that that could be helpful yeah sounds cool is there a link uh yeah Nicholas will be finding a link and posting it in the chat okay awesome okay uh just taking another look at the agenda so yeah uh anything else I mean again I know this happened a few weeks ago and this was uh you know something all of us are very involved at the time uh anything else on these sorts of sandwiching or sorry unbundling attacks that uh are worth worth mentioning on the call right now I did want to maybe just bring up the topic for discussion of uh the trade-offs involved in some of the mitigations applied to prevent these unbundling attacks so some of the the main ones being that relays now wait one second before returning the um the transactions to the proposer giving their the published block some time to propagate through the network the other mitigation being um a four second cutoff into the slot where if get payload request is received after that time the relay will not return it at all um and that likely lead to a Miss lot so right the uh the one second delay means that the relay has the high responsibility to have their own Beacon notes push out this block and if that fails for some reason then the proposal is going to have a one second less to get it out and uh have everything complete in time and uh the four second cutoff means that uh if proposers infrastructure isn't as good or they're not able to get out there get payload requests in time that might just lead to a Miss slot and um so that this has all been uh tested out and it people are looking at uh how this is playing out in practice but uh I just kind of wanted to bring this up for discussion if there's any like how is how is this looking um the the effects on Network Health versus uh um prevention of these attacks are we hitting are are the parameters kind of looking correct for the one second delay four second cutoff what else we need to consider here um to jump in here I think to me the timing looks correct we started with a three second cut of time that may have been a bit aggressive but at four seconds into the slot there is a zero percent chance that the block is uh is is landing for this lot so this kind of time seems correct also really have responsibility to have a redundant Beacon notes that means they should also have that appearing overall and and broadcasting from the relay should so so work very well um so I think generally it is the four second cutoff there is not a reason to push that any later uh because that would just lead to Orphan blocks rather than mid slots at the end of one second um delay to the returning for the proposal seems um okay to me because it gives the blog more time to propagate before a proposal could still a transaction and submit a competing block for the same slot um of course it also depends on the real is having good peering of their Beacon notes and running redundant sticker notes this is just my opinion I think the parameters are correct maybe four seconds is even a bit High we could reduce them a little bit but we are also monitoring the the numbers on the release and and it is looking pretty good so far I see uh hand over the mic yeah just one small clarification there um so the four second attestation deadline actually there's no guarantee that a block that's published after that will get reordered um only in the case where the subsequent proposer is running the forcible reorg logic will it will it get rearmed so there is like the estimates are something around 70 of the network is running that right now so pretty good probability gets reorged but um a late block could still end up um on chain if if the next proposer decides not to reorg it um and I just wanted to say that this is the this is kind of like a subtle relationship between proposer boost and um this this forcible reorg PR which is getting kind of discussed in the spec and and made the made the timing games around the attestation deadline like particularly potent as we were responding to the unbundling attack so this is something that uh that I'm writing a piece with Georgios on so definitely keep an eye out for that but yeah I just wanted to clarify that there's no guarantee that a late block will get reorged out only if the subsequent proposer decides to do that will it happen okay cool yeah good to get uh a couple comments there just wanted to make sure that uh we chatted about that a little bit to the broader community yeah and I mean it is worth bringing up that like all of the changes that we introduced immediately following the attack um many of them we ended up rolling back because they added so much latency that that it wasn't worth the risk to the to the honest proposers right so um even if we we slightly make the attack if we make the attack slightly harder even if that increases the risk of an honest proposer missing a slot by like one percent maybe that's too much of a too much of a damage to Network Health considering a sophisticated attacker could still pull it off so that was kind of the back and forth that was happening in the immediate response to the attack was was mostly us just trying stuff and realizing that it wasn't worth it largely but yeah great yeah thanks everyone uh yeah there are a lot of a lot of details here and you know uh seems like there has been a little bit of back and forth in like figuring out what makes sense and uh yeah if anything again I think it just points to these beings more longer term research questions around as we move to and shine PBS you know how how do we make this thing uh safe and secure so uh unless there's anything else there uh we can just keep moving through the agenda the next one was something metacris brought up about block cancellations Chris do you want to give a little tldr there yeah absolutely um block cancellations what are we talking about here this is the ability for Block Builders to override higher value submissions with later lower value ones this is used by startup strategies especially class exchange and effects and um yeah this is a huge performance impact on relays because sadly everything block submission has to be validated rather than on the those with a higher value and um one second streaming hit incident here um yeah and stuff yeah and in general there is no guarantees that these cancellations take effect because the proposal can anytime ask a relay for the highest speed even multiple times and just time whatever is the highest one that they found and not like they are not like there's no guarantees to the builders that the cancellations actually um uh in effect because you never know when a proposal is asking for for this lot because for the for the Beats because get header call is not signed so any call could be the proposal and for anybody really delivers there has to be the guarantee that the payload is available so in general this means it's not really quite incentive compatible proposals are actually incentivized to just watch out for the high speed whatever and then request that with a science header also in the current PPS designs it is not supported it cannot be supported because Builders just broadcast their headers their beats in the network and resources will time whatever is the highest one that they have seen so it's not that perspective there is not a long-term future for completion either and it would make removing cancellations would make uh relays in general a lot more cost effective because rewards are removed over like 99 of all the block validation which are actually producing the um most of the latency and most of the cost so this is a rough introduction I think um yeah in general it would be nice to move Beyond log cancellations I know that some builders that do statistically Arbitrage uh to to want that feature and yeah let's open the discussion about this topic okay yeah great thanks so you know concretely you know there's this Builder cancellation feature and the idea is essentially to remove it uh that would be a change to the medmiss relay software and you know any other relay software out there um changing essentially uh the guarantees provided such that you can't sort of cancel your bid in this way uh by simply the lower value block um yeah anyone have any any thoughts about this yeah Chris yeah just a quick question um from uh dominicus on you you mentioned that 99 this would reduce 99 of validation loads so uh are you observing then um that 99 of blocks are not incrementally increasing in value is that why why you mentioned 99 yeah that's a like a back of the napkin estimate but yeah that's pretty much the case I can't follow up with like very specific numbers Mike and I have been collecting some some numbers in particular about how often cancellations were used in practice to lower the value of a proposed block and I will also follow up with more uh data the data is by the way public about the bits that lead up to a payload so that's something that everybody can also just run on their own but from my um yeah quick uh check of the numbers in our infra it's around like it's over 95 it's probably around 99 maybe even higher we've received about 700 block submissions for a given slot and yeah that the majority of these is not is like maybe below the top value well so is a suggestion then that the that the relay basically it only considers a block submission from a particular Builder uh that is higher in value than what it is previously accepted and uh and the relay is only therefore going uh could only we only need to sort of hold on to uh to uh blocks for which it has serviced a a get header and all those two combined would then sort of reduce the load on the on the relay is that the general idea yeah exactly um without the need for cancellations for a builder it really does not need to validate or store any any bits and information about that bit because it will never be used again if a builder has cancellations enabled you need to validate every single submission and also always store the latest one because if a higher value fit by another Builder is canceled it may be that your previous lower value bid is actually now the new Top Value it's like yeah I'm probably not too good in explaining it but um they are without the cancellations there is no need to validate or store repeat content uh in in like the hot path yeah that totally makes sense are there any concerns about um uh what would happen are there any there be an issue with if not all relays are operating under the same regime like if if some relays allow bid reduction uh and some uh and some don't uh that that could potentially Drive Builders to use uh bid reduction ones specifically builders that are doing let's say some of these stat arbs um that could you know eventually end up with with better blocks that and therefore they can win even though during the slot period the block value might be going up and down I'm just sort of um ideating here I I don't have an answer or a position I think this is a great question and I want to let others join in like just in Mike do you have any opinions on that just for reference I think right now like we made it up in on the flashbots relay calculations and we're seeing about a third of the submissions being with constellations enabled so one of the comments uh one of the remarks here that we can make is that cancellation actually starts to break down once it's over multiple relays and the reason is that for a cancellation to be effective you need to cancel on all the relays because if there's even one relay that's kind of maybe a little bit slow to cancel you know because it's under load or because it has it has some sort of sub-optimal setup then actually you you won't be able to to to cancel because the the proposer are going to be querying multiple relays and they'll just pick the highest value across the all relays and so all relays need to to cancel and so I actually see cancellations as a little bit of a centralization vector or at least some sort of incentivizing uh searches and Builders to only submit uh certain types of bundles to to to one relay so you know maybe in a similar way that we we have like a specialized relay for you know so-called ethical blocks uh you know maybe we could have a specialized relay for for cancellations and that could be like a stable equilibrium because it's kind of a relay that specialized to do this one thing and the service starts to break down if if it's provided by multiple relays foreign Yeah we actually have noticed this recently um I think that y'all mentioned that we wrote up a document that we're going to be publishing with that where uh lower value blocks coming in uh to realize that are optimistic or trust those Builders to not simulate those blocks uh we see that those lower value blocks are coming in right during the time that'll get head of request is being returned uh so optimistic relay is like ultrasound or anyone that's trusting uh the Builder uh will have the lower value bid where relays that are assembly in those blocks will return the earlier block so uh definitely uh currently if not everyone's doing the same or not operating the same performance these cancellations aren't being honored by all relays so uh but I also I'm also curious uh I thought from reading the discussion on telegram that the idea was not to have a specialized relays but to have no relays so reporting block cancellation foreign most relays um may choose to not support cancellations but what might happen is that one specific Builder whose business model you know depends on this cancellation feature they might spin up their own relay potentially that that could be a possible future right so like we'll never be able to like prevent this unless yeah there's some like you know crazy embargo that you know we've all built uh with this like specialized group of relays uh uh but yeah assuming that doesn't happen then we just have to assume this is maybe possible um there's like a couple things here because like I think there's like an important point to make uh that has been made which is essentially like looking forward it's harder to see how to do cancellations within trying PPS uh which I believe is you know a go overall generally working towards and uh that's just because like if I try to cancel something I've already like say gossiped it's like you know someone on the other end of the connection could just ignore the cancellation right and there's like not really any recourse so the feature you know doesn't have like great foundations where we're heading so I think pretty much again is a big Market instance uh it sounds like it's also a pretty big resource during and relays today um one one comment I want to add here which is you know we can have a bespoke API for this if relays do want to offer it uh for example you know maybe when a builder submits a bid you get back some build like bid ID and then this this cancellation endpoint could just take the bid ID you could then not have to like say oh here's a whole different new block that happens to be lower in value simulated maybe store it all these things so that would remove some of the load that's sort of like a very short term uh pressure valve sort of release um but yeah again I think longer term we should really think about like okay if this doesn't make sense with EPS then why I haven't really offer it uh Chris do you want to explain yeah yeah I'm not sure what you're into cancel by canceling through like an ID it might mean that you don't need to store only the latest submission by a builder but also like all the previous ones which would be like strictly worse than just overriding a previous one with a new one right I assume realize we're already doing this uh but yeah if they're not uh no no in the hotel okay only the latest submission and the Builder cancels by submitting a new submission that's lower value but then you still don't need to fall back to any earlier ones like it really all only keeps the latest by every builder in memory in the hot path like in redness of error right okay yeah that's helpful uh Ben do you have something yeah originally when we first added uh block cancellation we were allowing Builders to cancel blocks by block hash which was causing a stat to store a lot of data locally um but using the latest bit obviously it's more performant than doing something like that I there are just lots of performance issues storing all of those headers and payloads just wanted to chime in one quick thing here um so Chris and I have been looking at some of the data I think he mentioned it earlier and I actually found a few examples of situations where uh a bid that kind of ostensibly should have been canceled ended up winning the auction because it was higher value than the the canceled bid so basically what I what I was looking for was situations in which both the the winning bid and the attempted cancellation were received before the beginning of the slot and the winning bid was higher than the cancellation and before the cancellation so all of this kind of adds up to the situation where the winning bid should have gotten canceled by the second bid but didn't so this could be the situation where validators are kind of playing this playing this game where they ask for maybe five different bids from each relay and then only return the highest among the five bids from any relay um so yeah the the data is kind of still a little murky but there's there's a few examples of this and I think in general might try and kind of write up a a short one-pager on on if it looks like people are actually playing this game or not which is I think an interesting data point because from the validator side um I haven't heard of a good way of of protecting Builders cancellations because validators can simply like ask for for many headers and if you start restricting the the number of times validators can call get header then it gets difficult because how are you going to restrict it are you restricting based on IP then you incentivize validators to just spin up multiple uh machines and call get header for multiple machines and then only sign like you're kind of encouraging sophistication in that direction so yeah I guess that's another point of view to take here which is that from the validator end like they are incentivized to do this um to do this strategy and and we see that it might already be happening basically so I I I know that we decided that we shouldn't be signing get headers because it's privileged information uh is there is there really any real justification for that though or a sign aside from just not wanting to do it uh because the the relay the relay knowing that it's the validator making that request just helps the relay send a send the right header to it right uh like if the validator wants to make multiple requests and they sign it each time that's one thing but uh is there is there really any harm to the system if we want the header to be signed I think it's a good question I just posted a uh math boost issue from over a year ago uh about a year ago but this was discussed I think part of it was uh why we didn't add signatures back then that there was a lot of time pressure on TL clients to get everything done and finding another call was deemed like uh only if it really really worth the effort but another part of the coin is that there have been a few people vocal about not giving privileged actors more privileged information and privileged letters in this case being the proposers that are about to propose a slot they're not requiring signatures means repeat information is public to everyone that is interested in whereas the data API afterwards is a strictly voluntary and there is like no guarantees but the get header request is actually like guaranteed information um just wanted to add a little bit of the historic context here I personally am not sure about the overall implications of one day or the other I think another big part of it is the the builders use get header to decide how large bids to set right like this is a this is the auction that's that's running real time and the builders can see the current winning auction price by calling get header so for example we see like almost half a million get header calls each slot um only maybe 10 or fewer probably actually come from the the proposer themselves so cutting off that data from everyone else I think would have big implications for the builders too not just um people kind of collecting data I guess yeah what did that kind of incentivize CLS to want to do that because that turns it into more of a blind auction that could benefit the proposers more to like have fire bids come in but it I guess it kind of incentivizes proposer Builder integration like vertical integration because then if you're the proposer suddenly you have like great insight into what the auction is um and so the builders would want to be cozy with the proposers to enable that that's fair I mean I think it's a good question like I'm not sure which is the best approach it also incentivizes collusion between the Builder and the relay because now the relay has this information which can be sold to the Builder with plausible deniability right it doesn't lead to front running and unbundling and all of these nasty and obvious things yeah I think one one thing in a bit of a different direction here is um apart from the get header polling there has been discussions about the math boost feature of a get header subscription of like a bit subscription where proposal could subscribe or even Builders anyone could subscribe on release to a stream of bits and then request a specific one whatever they would like to to have this would certainly be more performant and also more valuable to the proposers because they don't necessarily need to rely on a single point in time for a given bit but they can actually receive a stream and and then choose whatever we want which which would also not support cancellations obviously but maybe this is like the logical step in the direction how the game is actually set up because currently there is uh very likely already my Boost uh implementations out there that just repeatedly call get header right like um there is no no strong guarantees on cancellations anyway and and no way to force them so maybe the actual a way to to to move forward is just um making this mod like a privileged path for sophisticated actors but just a default path so if we fast forward to the end game like we should expect these endpoints either the polling or this the stream to get to get dust you know similar to arbitrim having whatever it was a hundred thousand websocket connections open um and like one way that we were thinking of doing the anti-sibil is using um actually using the collateral from the builders so now we actually have quite a bit of we have this this pretty robust anti-sibil data sets for the builders we we know who they are uh we know what their pop keys are and uh we have some collateral assigned to it so that's kind of a great way for example to only open a stream uh or only allow polling to builders that have submitted um collateral or have some of some reputation otherwise because the end game is just that the relays will get swamped by tens of thousands of of connections to the stream and then that that could lead to to degradation of service no I call Claire here I think the bombardment of streams is um you have to take it to account this is a purely read on the stream like on arbitrome this was because there was an incentive to push blocks like a transactions through a thousand that sockets at once they have a higher inclusion chance but here we're talking about a read-only stream anyway so I think this is pretty easy to scale out and it's arguably even easier on resources than you get header polling because also like limit this Justice if you if you play out the latency game to to the extreme um basically the way that these streams work let's say you have 100 um nodes connected to the stream fundamentally you need to send one ethernet packet at the time um and now if the latency game goes down to the whatever it is microsecond or tens of microseconds um but then uh then people will try and connect as much as possible so that they will be one of the first Builders to receive this notification in front of the other builders because fundamentally if you have one ethernet cable or whatever it is a fiber optic thing uh you you can't send an information to everyone it has to be a sequential process not necessarily no it has to be sequential I mean you can easily like if it's all if it's read only subscription you can pretty easily scale that out horizontally and and have like um I mean yeah we can investigate but you can't necessarily um you know guarantee that everyone gets it at the exact same time and by exact I mean down to the nanosecond of course you can't guarantee that yes but I don't see this being a huge dos Vector really um because you can still put in pretty effective rate limiting and and it's probably just an improvement of what still is cool we should explore this in more detail but I I think we've solvable in particular because it's just a read-only stream um that yeah I think we can investigate this more but I don't see this as a necessary layer blocker so otherwise um one thing to consider is um if we just um start the game where Sun release has cancellation and some don't have it then there is like a weird set of incentives maybe all the builders get more incentivized it's been up their own release to have more vertical integration um yeah I I wonder whether we should just um properties to the next logic conclusion through um yeah on the way towards e PBS so yeah remove cancellations from the from the real little together from F2 is pretty much that that would be like I think anything else gives a certain advantage to um specialized sophisticated actors uh any other thoughts I think blocks route for some uh had some a different differing opinions like would you care to why stem yeah Aries not here he uh he definitely had some strong opinions on that in telegram you guys probably all saw that but uh yeah I can I can probably say that whatever whatever they decide is the approach they want to take is what we'll take so I'm sure Irene y'all will share and telegram eventually when they decide foreign side of things on the proposal side of things would it be fair to say that um you know the polling of of get header is like a low hanging fruit that can be merged in pretty soon and that change alone which should be done in my opinion relatively soon because if if a sophisticated sticking operation listening to this call somehow decides to make this five line change to to math boost then they might be getting more Mev and so um you know just to keep the the affair uh you know a fair game for everyone we should try and make meth boost optimal so that everyone can can enjoy the the best Mev and this change alone will just nullify cancellations and and close this this conversation yeah I would agree I would be curious if anybody would would disagree I think that the the relays could protect against that by not returning get header requests until slot start time and then also rate limiting get better requests more strictly even if the MV boost starts making requests early um obviously like uh if there's a version that's out there that is trying to take advantage of places that are doing cancellation then there will probably be another Fork version of MVP boost that goes and that is suggested to be used with cancellation uh but I mean even even if cancellation goes away that you can still cancel a block with a higher bid that's that's the goal right because of the highest good is always the most accepted one um but I think I think we'd say we'd see a back and forth of of forked mov boost versions um after that wait but why would a validator run a fork that gives them lower payments yeah I guess that's the that's the point right like if if the relay is producing higher Mev uh but it doesn't let you make requests until after uh slot start I because these Builders are not sending to relays that don't include cancellation uh then people would want to use the uh mov boost version that can get them to those blocks right so that's that's the idea right if if the if the relays that are accepting canceled bundles or canceled blocks are producing higher Mev then people are going to want to use them yeah but this doesn't check out in my view because these uh Fork math boost would give you less less value as a proposal um because it takes calculations I mean maybe this set for this like splits up they really set into those that support cancellations are not and then you have the math boost but but I think overall the the mainline with Boost would give the higher value also I don't see really if rate limiting this strictly as practicing without signatures because for instance the flashback infrastructure does not uh uh pass IP addresses through so we can only uh rate limit based on a p on our Auto firewall and this um we run in AWS so basically there's always we can do is like 300 requests in five minutes I think or 100 but it's like nowhere near practically to a great limit to get header effectively without signatures foreign accepted cancellation right and those blocks were higher value and they only accepted had a requests after slot start time and they limited how many requests they'll return uh then there will be a there that that could be a shift right that could be the sell-in feature of using this Fork of mvv boost instead of the main mov boost but I still don't understand like okay let's say that they only accept bids or they only accept header requests after the slot starts then me as a sophisticated proposer I just spin up five OG and like uh you know math boost with with uh with the cancellations disabled and then I can get the best block even though I I had to wait until the beginning of the slot right I think there's ways to game pretty much any any solution for any problem in current PBS right uh but the the general case the 90 case of validators are going to be running a vanilla version of whatever software is available uh in large staking pools are going to want to uh behave with relays right like right now people want Meb so so if if there's game if people are gaming the system people could block those validators in the future um there's I mean there's a lot of things that could go back and forth right it's because that's kind of the game right now so I guess what I'm saying is more the 90 case of validators that are going to be running vanilla software and not trying to uh game a system okay so a concrete Next Step here sounds like prototyping this uh streaming bids right Chris this makes sense to you yeah that is something also publishing a bit more data would be another thing because yeah then the idea is basically you know you just don't have an opportunity to go with uh the higher bed assuming you're running on a software it's possible relay will be implementing um an SSE stream and I guess it might be consumed by Builders First and that could be a way to test it um and then I think we we would be happy to submit a pull request in the canonical repo and uh have potentially at some point in time um I have give give the options for proposers to also connect to it through map boost yeah sounds great uh keep us updated as as that develops but this streaming bid any proposer could subscribe to it right so the relay is going to be supporting streams to maybe hundreds of thousands of validators right so at any given slot there's only one relevant proposal so the way it would work is that at the beginning of the slot the relevant proposal connects and then at the end of the slot they disconnect um and basically what we expect should happen is that we have that I know roughly 20 Builders listening to the stream plus one proposer so 21. um and if it gets dosed for whatever reason then we can look into the uh the anti-sible mechanisms that we have using either reputation or collateral so are you proposing that the in order for a validator to receive the stream they have to authenticate with their via signature um no I uh well yeah that's a I guess what I was suggesting is the default implementation of math boost kind of connects at the beginning of the slot and then disconnects at the end of the slot so that we're not overwhelmed by the the default implementation or we could accompany it with assigned get header because the proposer still needs to like get the thing to sign over you know and assuming that we have this like SSE endpoint for the bids we probably would just publicize the bid size we wouldn't necessarily want to send out the entire execution header to everyone um so maybe that would be a good way forward like builders get subscribed by posting collateral validators get headers by calling get header with the signature would still work for sure um but it's a fair question whether um providing this SSE subscription uh what does that mean for a relay infrastructure we have just spun up exactly this for the launch of math share where we have public SSC subscriptions that are not rate Limited um at that time so we we are experimenting in production with that right now too we will make all the software of course uh also public that would be needed for that on the really side um but yeah for sure it's a good question on what would be the impact on really infrastructure providers um I do expect it would be relatively um manageable but then the other like the only way out for proposals would be to add a signature here and then we are at the same square one where we didn't want to have get header signatures in first place uh because of the centralization vertical integration and privileged information topic so I guess the code would be to have the SSC subscriptions public and I guess like the minimal set of information through this would be like uid value and Builder and I wouldn't expect a lot of traffic going through that right like it's a few kilobytes at most per message maybe a few hundred bytes and then like 50 to 100 messages per slot well Chris it would we would only submit the the bids that increase that basically will only broadcast the top bit if that changes yeah we can also only publish the Delta so it could be like really a small amount of information foreign side I think you mentioned that you can provide this stream to builders because they are you know who they are and that maybe they're did I understand correctly that they're because they're bonding with the relay right so we we could have like a builder signature for the builders and we could also have if we wanted to propose a signature for the proposer and and the Builder you can trust you can accept that because they're bonding with you that they have some they put up something I have a reputation or some some bonds yes so I I guess I might have a little bit of concern that um right now there's bonding with the relay for optimistic relay optimistic relaying which is an advantage then to the Builder that bonds and now there's uh an additional information stream that is helpful to the Builder that they only have access to if they bond and I wanted to inquire if that is consistent with what epbs would be able to support Downstream later I shouldn't say Downstream right um so with epbs the way that I'm imagining it is that we have this peer-to-peer gossip Network and the gossip network will only forward bids that increase the top bid um and so as any Market participant like you can just connect to the pay to pay gossip stream and and and and see all of this information in terms of how the anti-dos is is done um one interesting yeah the builders would be collateralized in epbs so that's that's one anti-sibil mechanism um so yeah I guess it would be the same thing like in epbs Builders are collateralized and that's what allows us to know that you know that that bids are real first of all uh but also that that could be a way to enumerate them you know there's only going to be so many and we could put some constraints I know at the peer-to-peer level there could be a constraint saying that we should there should only be one bit per whatever 10 milliseconds per Builder or something like that um it's a little bit similar to how I think transactions are broadcasted so any given uh address non-spare can only you probably know this better than me but like can only broadcast so many transactions per second and every time they broadcast they need to increase the the tip or something so similar types of peer-to-peer Network networking anti-dos strategies could be put in place but yeah I guess for bids is extremely is this situation simpler because you only need to forward the top Bid And so 99 of bids that are not improving the top bit can just be discarded and pruned and dropped and then also instead of having millions of addresses now on ethereum that could all potentially be sending transactions at the same time there's only going to be I don't know 100 builders yeah that makes sense I mean if epbs basically enshrines Builders having to collateralize along the lines of what ultrasound is doing now with bonding then then that would work but that that opens up another question for me which is the uh the the bonding right now is has a limit I think it's one eighth and um you just go into sort of the the slightly slower Lane if it if you have a block value in excess of that so an epbs how would that work because uh that would you know Builders definitely want to have higher value blocks to their valid validators if the opportunity exists right um so I actually have an if research post draft uh which I'm happy to share with you previously I should be publishing it very very soon which basically describes some of these uh the the the the the the the the design space um basically one of the things we can do is we can cap the amount of collateral per Builder to let's say 32 if and uh they're still allowed to to make bits that are greater than 32 years so they can still make uh let's say a 1000 if bid and that that bid will go through the block will go through if it's if it's valid and the block body is submitted on time and they pay the proposal the full amount the the promise 1000 if if any of these conditions don't hold true so if they don't pay the 1000 if if the block is invalid or the the block is revealed the block value is revealed too late then they would lose the 32 Eve and so basically the question that that we need to ask ourselves is like how much do we want to penalize uh builders for forcing an empty block like most of the time forcing an empty block is just a lose-lose situation um but that like basically one of the things which is a little nasty is that you can if you can force an empty block then that could be used for unbundling so for example if you if you force kind of a I don't know we need you need to we need to think through this but there is a design space where we kept the amount of collateral let's say it is 32 if and that's how much you need to to uh to pay if if you uh if you misbehave as a as a builder and so the reason why I'm looking into this is because I'm a little bit worried about you know potentially the very rich builders that can afford let's say ten thousand four hundred thousand if um the advantage that they could have of a small build is because my prediction is that we will see single blocks with 10 000 or even 100 000 if and the simple reason here is that at some point in time there's going to be some some hack on on you know very large contracts or even a roll up you know there could be a million Eve hack and all the Mev will be swooped in one fail in in one block um and we don't want these these richer Builders to have an advantage foreign agree on that I think we saw we just saw that uh slashing mechanisms cannot keep up with potential gain right but yeah I mean I I discussed this in in in my next if research post it's coming very soon cool yeah sounds excited Justin um okay that was uh surprisingly fruitful conversation I thought uh there's one more thing on the agenda uh let's see I don't know is uh Matt here from the pawn relay team yeah we're here great yeah uh so this came up a few calls ago um as an example of a different relay design and you know just pointing to different sort of innovation uh in this ecosystem and yeah I thought it'd be good if you could just come and give us a little overview of what you're working on maybe how it fits into the bigger scene uh anything you'd like to share yeah totally thank you for that and um it's an absolute pleasure um can I share my screen oh you should yeah uh try I might need to give you permissions or something but let's see all right cool can you see my screen yep cool all right so basically like you know I'm mad I'm from blockshop I'm joined with um uh Winston here in the room so we basically doing an experiment um it's all so so basically it's it's called Uh Pon proof of neutrality uh though it's called relay it's not really a relay right so that's the first thing it's like I'm I'm sitting in a relation room I'm gonna talk about something like okay how are we going to go at less reliant on what I said forgive me on that one that you know that that's what we're trying to do um we're trying to we started this in Defcon Bogota uh usually like you know there's a lot of talk about censoring and things like so we start looking into it what we can do how can we have a neutralized relay experiment and it literally started as an experiment um from from Defcon so so looking at that and and we took an approach is like if we want to push things in a little bit sideways like you know the pre-block and post block and how do we get this the the italic approach is like you know it's an aggregation um way to to to address it how do we make sure that Builders can be aggregated more effectively if you can game the system um in such a way that could be incentivized to all the parties so that's the way that we took it so essentially like you know looking for all the the things that we did um Justin had um an amazing um uh presentation about the witness and witness encryption so we took that approaches like you know slicing this into two pieces so one is like the off-chain coordination another one is like more of a non-chain so that's we call the Dual framework so one thing what we did is um how can we have this kind of more guarantee um on what what we what the what the proposal is receiving um so the proposal is receiving all this marketers need to be have like invalid blocks and all those kind of things so we want to have all this kind of things to be validated before even reaching to to the supposed relay part and that's what the off-chain does and the on-chain part is more of like you know how do you identify and serve the the parties like the builders are registered also registered and there's another party so that's more of a on-chain registry that we created that's a full-fledged smart contract sheet and that's also deals with the payout pool and that's kind of a SMI smoothing but it's it's more than that it's it's a very interesting architecture um right so this is what happens now and in the proof of neutrality is one one way to look at it is like you know encrypted block with a payment proof and then it just goes through it and the Builder gives a good plus and inclusion list it doesn't really give all the whole transaction just to block header and then it goes through a second price auction and the winning bid will will be selected this the whole thing is currently please is getting some loot into one slide period so we don't really have like multiple slots right now so and the winning bid will just go to the new Boost and the proposal will just pay the amount to the payout pool um so you know in a way to look at this is like very close to how the optimistic relay currently like they have the architecture but it's the difference is like there is no penalty for the relayer because it's a guaranteed by the the rpbs scheme um also it's all getting validated right so um what we did um at the beginning like you know when we want to do this configuration markets to Marketplace we want to accelerate this is sort of the the specification that we set inside and so like you know what we want to achieve we want to accelerate this Builder proposal cognition in such a way and I also like make sure the builders have this kind of strategy they always have a strategy but they that we just want to make sure they have like you know more and more opportunities coming um you know from the from the proposed aggregations and the relays are should be like kind of an add-on feature like nice to have the way that we think internally is like an ipfs hosted service or in a cell phones or service like you could work the whole system without having it over there um but really with the relay you can actually bring a lot of other things like this and better markets so but the one thing is very important is like how do we get this kind of a Loosely connected actors or proposers as we go ahead more and more decentralization comes the soul of secret comes these are like very hard to come by and they're very less sophisticated active what's incentive for them to be in in a in a kind of this kind of a registry membership table and a smart contracts and work they commit they will sign the builders proposal whatever that's coming through the system right so that's why we we set this kind of a three plus one actor it's in the con system has four actors like Builder proposer uh which is like the Builder and the validators and the relayer is the the the code meter in between and this new actually is coming in place it's called the reporter is that watching out everything in its violation will be recorded to the on-chain so one thing that we need to understand is like most of the option coordination will happen but the on chain is worth a settlement for the proposals and and the builders have a collateral there and they have a minimum stake deposit that is also being watched so everything is controlled by on chain right later so that that's where the reporter is completely new so since this was an experiment we started obviously we got to start somewhere um you know so we we took this normal the whole software which is available and started like you know um how do we do this how can we remove how can we have more direct coordination between the Builder and proposers so we wrote a custom software on the get um obviously the layer that we use a flash root relay for the testing I think um we're done with that so we're just going into a minute so all the things is like the new Boost and relay it's very very tightly connected so just have to use that and we use that now we're moving away from that complete relayers in going to be a new software so it's not going to be any more relayer kind of stuff it's more of a sequencer so the proposal is out of the box for me boost we also stumble up on a lot of problems like you know how do we make you know changes like launching registration and things like that I'm going to patch on the next slides and the report is completely bespoke and I just want to say uh one thing I I very interesting experiment and I am curious where this is going um I I'm happy to collaborate and I'm looking forward to what you guys come up with I do want to help you get off on the right foot here and I have a small beef here which is that um I think you you guys may yeah I don't know let's put it this way the way you started with the really cool business odd attribution and with um launching this under an incompatible license is not great I think you should not just take another code base scrub the commit history and um dump the code of your own and then publish it in a non-compatible license so what I'm asking you here is that please um have the correct commit history and um have the correct license The Flash but really caught this is under the hpl license this does not permit to release this under the MIT license and I would ask you that you do not scrub the commit history from the original authors but keep that and just build build on top in a nice collaborative open source fashion that would be my ask yeah that's fine just uh as we said we're just not going to use any more relate this we're going to throw it out anyway it's fine great thank you so looking at this scene um you know the on-chain on chain uh we have it on chain um component so the users yeah you know can all the proposers reporters and the Builder um they will register through the on-chained smart contracts that's the proposal registry um so that's like more or less like less than 60 seconds um you know us um they drag and drop and they will just go so it's very mainstream driven so we want to make it like more adaptable so how does it work right now it's like it's this is like single single slot currently like you know you get a blinded block sign Blinder block with the rpbs from the Builder and it goes to the layer and you know then the bid will happen so kindly it's like two plus eight plus two it's it's not a latency very very good but we were just working on it so so that within the two seconds that you get all the blind blocks and get validated in the in the second price auction will run it and the the biggest bid will go to the uh proposal before that the the host service will actually validate with the rpps the payment proof is there and then it goes to the proposal sign it not after really neither layer or proposal has any kind of you know visibility to the block contents goes back to the Builder and the Builder published first and this is one other thing that we noticed when the large bug happened like in a few weeks ago we're looking at the flow and luckily we had this kind of a you know first Builder will publish everything to the the blockchain that was that wasn't okay so we're also learning when when things are coming in the production right so and the Builder will publish things in the production and then then sorry the blockchain and it will go to the proposal and the proposal will just get the block on it so proposal will never see anything neither the relay will see any any block content before it's getting published so having said that all this experiment what we want to see here is like you know more of like a free Block building and possible post block building supply chain to be more modular and plug and play so we have this kind of um collateral announcing repetition timer proof on on chain so we can have like you know get more and more optimistic approaches to how the builders can directly coordinate with the proposers and then you know without having any kind of dependencies with the relays but the relays can have like additional logic to come into have like in a subsection based or any kind of additional you know uh the bidding strategies and just in time um kind of strategies or in a later delay settlement like a gas token kind of stuff and on chain sequences Dow sequences protocols to take that kind of things all kind of innovation is possible when we have this kind of on-chain tamper-proof information is available for all the relays currently it's like one relayer information is only belongs to them but here it is like you know everyone can play with that um the current payout pool contract construction is like um there's a major individually one inflow is the block payments and then obviously there's a staking contract that will the builders will register they have a minimum stake deposit but then there's another payment flow is like when there are the the violation happens the reporter will slash it and then it will just redistribute to the other you know pool members and it gets settled on a weekly basis right so this is a really interesting contract we release our audits by the runtime verification we have a documentation please read it um a detailed walkthrough also we've been given there um so so what is this the more of an optimistic path on the PBS in the on-chain way that we can do it it's like we want to have this kind of a balancing act and one of the major things that we would try to to address here is remove the fear of loss right for The Proposal Part and that's where majority of the coordination need to be there because people just say that oh the biggest the all I want is the the biggest payment out there but how do you quantify that is that a one time or you can just get the payment like in in a long term or you can get a later payment you have other kind of self-interest who knows we need to have this kind of more coordination to come so this is the way that we want to see it and also from the Builder side we want to give more opportunity for the builders to have their own strategy that shouldn't lead to any other parties and they they will have more Avenues right so I'm just trying to speed run it I know we have a limited time so where we are now is we are trying to work out this kind of new simple software and we release under the wtfpl license so no one has to worry about it so it's kind of a a an alternate version for not having any Reliance on the relay part like more of a coordination so that's that's all all there so the relay part is completely gone and that will come and also we want to have this kind of blind communication the blind if there's a blind communication it's like it's everything is validated you don't really have to worry about in the relay part like I do need to do like a bunch of checkings again and again you can have this push further backward in the pre-block building exercise then we can get like more and more optimistic way of getting the blobs using the reputation on the on the smart contracts you can have a different kind of biddings can be enabled in between right so what we want to have like very close to nothing like a combination of optimistic approaches to using the the historical data and also like you know enabling the builders have much merger Avenues um also we are taking we had a lot of issues when we're working with this kind of validator proxy software so we're putting more effort on that side as well we're trying to put together a plan like to have some kind of a types and um um to have a different validator proxy software that will be in line with the builder API but an alternate version of um um you know I mean boosts the problem is like amiibus is very restrictive for example it need to have everything to be registered before we don't really need to because it's already registered on chain so we want to have something which is very compatible with the on-chain element so that's something that we're doing so that is the the next slide um yeah we call that as an Mev plus a very small in a kernel this again this is the new experiment that's an e plus that we want to do it we want to share it with the community as well as soon as possible like based on the Builder API and and plug and play so we can have like you know NPCs can be plugged and play um all kind of like Dao shaking so can be plug and play and using the smart contracts but it should be very minimal um and any relays can be built up on top of it not only the existing relay construction so that's where we are with the proof of neutrality as I said it's not a peon relay anymore but it's official neutrality Network that that's what we want to bring it close to the PBS I'm sorry I just ran out uh time any questions happy to answer oh it's okay we have just a few minutes um yeah I mean thanks for this uh seems seems quite ambitious um yeah I mean I you know at a high level it seems like you know there's different things with the current boost construction that you know we're all pretty familiar with that you know are still fairly trusted and you know obviously it'd be really cool to uh relax trust assumptions you know for example moving different parts on chain um it seems like a lot of the work here is moving in that direction um so that's really nice to see um yeah one thing is like if you have well yeah so there's actually a lot here uh and I don't think of all the time probably get into everything um but like one question I had was have you well first yeah is there like a website or something just like you know if people want to learn more is there a place to go so uh website is peonrelay.com so and that's a website um you can learn more of it I'll just post the link in the chat so everyone can get it so that that's that's the link um happy to give this uh presentation if you want to to uh later okay thanks um and so the question I had was do you have any like timing on you know all these steps like one you know for example if you have Builders like you know commit to like encrypted blocks like and you're doing this all in one slot like is there enough time for this to you know all play out or you're not sure yet no no we we already have that on testing advice so that you can play it with the Testament right now so kindly it's uh two seconds for all the black in a validation that the bids will close to plus 0.8 there's a seal good is there so second price option will closer 2.8 seconds and then it's about eight seconds um eight seconds so um six seconds that that they'll be given for to get the the proposal signature back and then uh four seconds will be used for the the propagation that's how it works and there's um yeah we have one Builder uh happy to post that that information I don't have it handy the how many blocks um that we published and everything so I think there's somewhere in the we have a yeah disability test Nets you can see that how many Builders are there uh it's actively building um so we're testing now everything you say but the only thing that we're trying to to remove it before the main net is that's the the the they do the new sequence are completely ground up um um less relay kind of stuff so gotcha and so okay these block Builders they're it's not that they're using the perfect neutrality Network to like use the current web boost system and just build in blocks in this way so yeah currently it is just a meth who's a validator proxy but they use the Builder uh reporter reporters using our reporter software the builders the Builder software that the builders that software that we created and the the layer software they're using is the the custom version of that we did in the flash but that's going to get swapped out in a week or two yeah cool yeah I mean Super exciting and uh hopefully people here can uh follow along at the website and other places that leads uh we are a time so I'm going to go ahead and close the call but thanks everyone for attending uh I think we had some really nice conversations around you know both these different bundling things which in the last couple weeks and then also looking forward with cancellations and uh you know again just pardoning this whole whole construction that we have in that boost um yeah thank you all and I'll see you soon no problem thank you 