okay well everybody uh welcome to the financial data science seminar and i'm really happy to have andrea minka today who is one of our own ori professors and she will be talking about clustering uh heterogeneous financial networks and i'm i'm really excited about this topic specifically because you know i know that whenever we we do uh financial engineering projects we often end up wanting to cluster things or or stocks or you know groups of customers or you know clustering is sort of a you know a recurrent theme in in quantitative finance and um and i'm really excited to see the the kind of applications and uh and theories that andrea has been working on recently so um andrea welcome to the to the seminar and uh the floor is yours all right thank you so much sasha and thank you for the invitation it's really a pleasure to give the seminar um and it's it's really interesting because i've been at cornell for for for quite some time and uh um i associate cfam with uh working with practitioners and i think this is uh yet another way to to to reach you guys um and uh indeed just over the last decade i've been working a lot on systemic risk and uh financial networks and it was mostly from the modeling perspective so how do um how do nodes how do banks how do hedge funds um how do they connect to each other and um how can we represent this in terms of a network and moreover trying to understand how how you get a cumulative and it's going to be a highly non-linear effect that comes from what happens at the level of a few nodes and then propagates to the entire uh to the entire uh network and the work that i'm going to be presenting today is joined with uh yudong chen who's one of our former colleagues and with the student uh shin khan and um this was not so much on the modeling side but rather on the algorithmic side so that one was about uh so so this work is um going deeper into the theory of clustering and developing um algorithms that are um that are uh adequate and that are more equipped to deal with the challenges that we have in uh in financial networks so now the the problem that we observed that um in general uh when and and i'm sure that this is what you observe also in practice is that when you try to use clustering algorithms to detect financial network structures they remain ad hoc so it's it's it's generally off-the-shelf algorithms that uh that that are coming uh in general from uh from either the the network's science and they've been developed with other applications in mind and not necessarily um or not necessarily developed with the financial network viewpoint so uh we we try to go one step uh forward in in in the direction of uh matching these algorithms to financial networks and uh i'm gonna be talking about two examples so so the first one is going to be about networks of overlapping portfolios so that that has application in portfolio selection and second application is something that is more related to what i've been doing and that is systemic risk analysis and for that one we are thinking about the network of exposure so exactly what i was saying who is connected to whom and exposure of one node to another node and thinking really about this as uh as as an entire uh network and trying to understand what is the level of risk uh in in this network because um if you um if this network of exposures um is going to have a meaningful clusters then essentially what this means is that uh each cluster is going to be um is going to be vulnerable to um to the default or to to to illiquidity happening to one node or one institution in the in the class so the larger the clusters uh the larger the potential impact of uh of the stress of one member in that in that class whereas i think that for the network of overlapping portfolio um the the application is twofold so you could also think about it in terms of risk analysis but more importantly you could think about it in terms of how how can i design um how can i do portfolio selection in a way that uh that avoids large clustering or how can i do it in a way that um that that that enhances diversification and getting into this uh story of diversification also the clustering uh viewpoint so um so yeah the the problem of clustering of course is not as i said is not it's not uh specific to the financial networks disappears in all real world networks and so nodes tend to to form groups so what is the cluster is uh is such a group uh in which we observe uh that the connectivity within the group uh is larger than the connectivity between members of different groups so we're going to say that the intra group connectivity is larger than the intergroup connectivity um and and in general detecting clusters is is challenging and there are a lot of generic clustering algorithms out there um from the now classical modularity maximization so this is up it appeared in 2006 uh by newman um and graph cut cut-based models and then likelihood based methods and probably what is familiar to many spectral clustering uh and hierarchical clustering um so so all this to say there there are a lot of of algorithms out there and each one is going to be uh adequate for different types of problems um so so really the question is why not take one of these existing algorithms off the shelf and apply them for financial networks so there's there are several reasons so an important reason is the heterogeneity so financial networks have heterogeneity in terms of degrees so the the degrees exhibit what you call uh um heavy tail distribution so um you're you're going to get uh [Music] that that of a very few number of the nodes have very very high uh connectivity and then most of them are are having uh small connectivity so small degrees uh there is also heterogeneity in terms of weights so if you think about these networks as being weight in networks uh you are going to observe also a large heterogeneity in terms of in terms of weights and you you you also observe that there is different inter-community connectivity so um thinking about clusters perhaps as sub-sectors um you are going to have that uh each uh each set of uh of sub-sectors are going to have uh different inter and inter connectivity so you're not going to have um the so so each one of these sub sectors is going to behave in some in some sense uh differently uh now another specific um uh problem that that um that that makes it challenging to apply of this of this of the shelf algorithm is the presence of outliers so outliers you cannot treat them as being one cluster by themselves you can't say that okay you you have this these sectors and and then each each cluster is going to represent one sector and then you have a bunch of outliers um and then we can think about them as uh as as a cluster themselves so you can't do that they are going to appear um and uh for for the purpose of the of the algorithm they are gonna behave at some adversaries because they may have also the same uh connection patterns as in-line nodes um and at the same time they will oh so somebody tells me that they are still uh seeing the um the title slide we see the first slide we thought it was a very big introduction but you were showing us different styles yes okay so let me see um okay um [Music] so i'm not sure why you are not seeing that did you share your full screen or just the just the powerpoint application the powerpoint uh let me try again and it was not in presentation mode it was in sort of um you know um oh that's okay so i will do let's try that then now we see it move how you still move but how about now if i do a ctrl l okay so that is not going to uh l okay so it's very interesting okay so now you see it right there you move yeah now you can see so i'm going to keep it like this because there seems to be um yeah that my my screen sharing is not really working uh as intended um so now you can see this move right yeah we can see that okay so so i was talking about these different uh challenges that you have um when you cluster this this type of um of networks um so so um yeah i was talking about outliers so these nodes they they don't belong to any clusters and they have arbitrary connection patterns so for all practical purposes they're going to behave as an adversary and if you want an example so thinking about financial networks and thinking about uh networks let's say of banks you could or investment banks you could think about about a node like aig financial products as an outlier because they were having um many of their portfolios uh derivatives portfolios they were um they were in essence like uh those of uh investment banks um but um but but uh aig financial products was of course not um not uh not an investment bank so it was uh part of a large uh insurer so um they they they they tend to behave like one of the cluster notes but not in all respects and moreover they can be different in nature also uh among themselves so as i said you can't you can't think about them as as as one cluster and known algorithms such as spectral clustering are known to behave poorly in the presence of outliers so if you if you have a data set and simply will apply spectral clustering then um even then it is not going to identify cluster so it's uh the presence of of outliers is going to hinder the the performance of the of the algorithm and many of the nodes are going to be misclassified so that is that is one of the of the problems so um what we are going to base this work on is uh on a relaxation semi-definite programming relaxation of the modularity maximization approach and i'm going to be talking about that in uh in a second um and now the clustering problem itself it is uh an np-hard combinatorial pro problem where you are trying to assign for you are trying to assign um a group to each node uh in the in in the network and of course you uh this is npr so so you have to search for all possible such uh attribution um schemes and uh now this is going to be relaxed for a semi-definite program that uh has structable that that is going to to have tractable algorithms and um of course you once once you have this what you want to do is uh prove that uh guarantee the quality of the of the clustering so so so give give up give certain conditions in terms of the tuning parameters that the the about the the the uh the recovery of the of the of the true clusters and um of course this this problem have been solved in certain context uh before so what we what we what we aim at is finding uh certain regularization that are going to solve the previous challenges so deal with outliers deal with heterogeneity and deal with the different intra and inter cluster probability of connection so what we do is we introduce a novel regularization term that penalizes outliers with unusual connection patterns so that's basically what you want to detect you want to detect that um you want to to to to say okay this is this and this is not this um this is an outlier and you want to do that based on the on on on the connection pattern and um you you want to say that that the way this outlier connects to the in layer is different than an inlier would connect to other in layers so you want to say that essentially even if the outlier has a large degree it is going to behave differently than an inlayer that is going to have similar degree but perhaps is more central to their own uh their own cluster and so right and so this is this this is a work that is uh based on stochastic block model with outlier and on degree corrected stochastic block model by entirely respectively chennai and what we do is we provide theoretical guarantees for the exact recovery and we don't make assumptions on the outlier nodes so we the only assumption that we are going to make on the outlier nodes is that we know their number so we know the cardinality of the of the outlier node set and um we have explicit non-asymptotic condition for exact recovery so the density gap that this is going to be a condition on the difference between the inter intra and inter cluster edge density so of course the intra cluster edge density is going to be higher assumed higher than the inter cluster edge density so we assume that there's more connectivity within the cluster than outside than than among nodes that are in two different clusters um and um the this density must be larger than an expression that is based on natural uh problem parameters such as the edge densities the heterogeneity the size of the cluster and the number of in layers and outliers and uh as what these algorithms are doing and what what this um what this uh theorems uh and the guarantees are doing are saying let us uh there they're essentially existence theorems they're saying there exists a range for the tuning parameters uh that makes sense and that will guarantee uh the recovery of the of the uh of the clusters and there are and in terms of comparison with prior literature the results that we have are more robust and uh contrary to previous works where the density gap conditions uh depend on the on the on on both the end the number of in liars and m so it's going to be a product um of the number of in liars and number of outliers um ours is going to be robust to the number of outliers and it's only going to depend on the number of uh in liars so no matter how many uh outlier nodes you have you this this results are going to be robust of course they will depend on the on the cardinality uh uh of course the model itself will assume that we know the the the cardinality of the outlier uh set so um now how how we dealt with all of these uh things um and how how are these models uh different uh than than previous works um now the stochastic block model is perhaps the most well-known uh model of clustering um and it has um so usually it has homogeneous degrees for the inliers and um now in that particular case a node if you don't have outliers then what will happen is that when as soon as you see that as soon as you observe that a node has an unusual uh degree so so it's it's it's it's it's unusual with respect to the to typical degrees that you observe for the in life you can immediately say that this is going to be an outlier now the problem that we have in in financial networks is that you have this heterogeneity so so a node can have a very high or low degree uh simply because uh the the of the structure yeah in the same uh of nodes in the same cluster so you can't say that so it could be that there's this heterogeneity and this heterogeneity is normal for the cluster so so so in banking systems you observe um the same thing and i'm going to show you a banking system that is the korean banking system and in that one you you you have that banks in the same sector um and insurance companies that are also going to be same sector they all exhibit this heterogeneity so so just because of the heterogeneity they shouldn't be classified as outliers so that you should the algorithm should correctly identify these are banks and these are uh insurance companies and so on and uh now um because of uh the degree distribution that is heavy tailed then we need to look not just at the degrees uh but also the connectivity pattern so who connects to whom and um this this this other thing is is uh this other point is uh more of a technical point that um that in contrast to the previous work we uh we need to look at bounds not just in terms of the maximum degree but rather uh rather look very carefully at the contributions to the to the to the bounds um for the for for each uh for for for notes uh that have us uh degrees in a certain uh range and uh because of um heterogeneity we can we um in contrast to previous work where there was a homogeneous penalization on diagonal terms uh well in this in this case the adjacency matrix will uh we will the definiteness of the adjacency matrix will become worse and and and then we need to make sure that the penalty terms that we uh we introduce in in in our algorithms um they they they are going to account for the for the degree so you cannot use the same uh the same homogeneous penalization for uh for for for all nodes so just let me dive into the a bit of notation so there isn't too much of notation let me see um okay um all right so a few questions if there are any questions i'm monitoring the chat so please uh please forward them as as we go um yeah this was just a small question about whether you would be sharing the slides and i i and i was announcing that we'd have the video um and maybe um you know maybe you want to share the slides as well um yeah sure sure sure um yeah so i i think that's a good idea because uh there's there's a lot of uh notation to parts of it so i'll try to i'll do my best to to to go over that um so now what are what are the fundamentals of this uh of this of this model um so we assume that we have a graph and this is going to be the vertex set and it's going to be the the adjacency matrix uh n is as i said the number of in liars and m is the number of outliers so in total you have uh capital n vertices and you assume that the number of so this is something that we know so we know the number of clusters so you know that the in layers they are partitioned into a bunch of unknown clusters so c1 c2 crs and all this um so you know the number but of course you don't know who is who belongs to which cluster and you're going to observe an adjacency matrix and the model is saying that so so you this is a primitive of the model you're assuming that uh you know uh some heterogeneity parameters for the for for each node and in practice these heterogeneity parameters they could be for example the size of the size of the portfolio they could be um well an expected number of counterparties uh of of a bank so there would there would be a measure that that that is uh in the charge of the modeler um so that would be a measure of the heterogeneity so this would be specified as you as you specified the the model and so uh again you also specify so other than the uh this heterogeneity parameter now we are assuming that we also have some connectivity uh matrix of the class that is given at the level of the cluster so now you're assuming that there is a basic uh connectivity matrix they said okay on the on on the basic level uh a node um a node from uh cluster a is going to be connected to a node in cluster b with a probability b a b and now i'm gonna adjust all of this and saying okay well uh nodes um i'm gonna adjust this by the heterogeneity parameters such that nodes that are are are more uh and again what does it mean to have have a higher theta means or maybe they are larger if this data represents size or they are more connected if data represents um represents um um connectivity uh so it's really uh these parameters better they can uh they can accommodate a lot of modeling uh choices so now okay can i interrupt you for just for a quick question um often in clustering uh you know as you say the off-the-shelf clustering methods the output is often a pretty picture and you look at it and you sometimes get insights sometimes you don't and here it seems that you have a very specific more specific um application or goal in mind related to these in liars and outliers and and i guess i'm uh i'm wondering a little bit more how that fits let's say in the context of you know the systemic risk or you know like the the output beyond the nice picture is um you know it might be a dangerous node or something like that or so so the output in clustering is going to be a like i'm going to show you a picture of that and and then you're going to observe the clusters the problem is that those clusters for certain in the present when you have outlier nodes so when you when you can assume that you you some nodes will not belong to any of these clusters but may look may be apparent as a node in the cluster well your your your pictures are going to be somewhat damaged by them so so in some sense you need algorithms that make sure that in the presence of these outliers they can still produce these pictures and how do you assign how do you how do you uh how do you define the algorithm and i'm going to show you it's actually not very complex but but the the the you you do need to make certain adjustments to exist so the the goal of detect of these outliers is to fold them into a cluster or is it more to sort of put a red alarm alert and say exactly yes to put a red alert and to not hinder the clustering itself to because you could you could be mistakenly uh because you they could they could hinder the classification of the in layer notes so um now you are again assuming that in the basic model so in the you there there is this connectivity that comes from the from this parameter stata and the connectivity is is happening is adjusted by this heterogeneity parameters so what you will observe is you will observe a realization of the network so you'll observe a realization of the network so for instance somebody tells you um and i i'll be showing you the the korean network somebody tells you okay this is the this is a picture of the korean network now now go figure out which are the banks and which are the which are the um the insurance companies right so and then you're going to assume okay these are clusters so they're more likely to be connected to each other so uh so in the initial picture in the initial network if you want to call a network a picture um it's you you are going to to have uh the the degree which is obtained from the from this adjacency matrix and it looks uh a bit like this so you you're gonna say you're gonna make an assumption that there is uh the the adjacency matrix can be written in this way and here there will be the connectivity um so the adjacency matrix of the first cluster and the adjacency matrix of the r cluster and this would be the adjacency matrix uh that correspond to the to the first outlier and of course you your your your goal is to find this permutation matrix so finding this permutation matrix that is going to tell you okay this particular node that you call let's call it number one well this this particular node number one is actually belonging to the cluster number one and this particular node that is called number n is going to also belong to to the first cluster so it's really finding this uh this this uh permutation so this is this unknown permutation matrix in which there is a single one in each row and column and under the under this unknown permutation the nodes are ordered according to the underlying structure of clusters and and outliers so other than this you're assuming that there is this structure and and and in the in you you're trying to find how do you map from the picture um that you observe to this known cl unknown structure and this mapping is given by this permutation and once you find this map this permutation then this permutation is going to tell you who which node belongs to which uh cluster so so basically for each uh now for each candidate partition uh of the of the nodes we can associate it of course with a partition uh matrix and the the the details of that is that uh x i and j um it's it's going to be equal to 1 if and only if the nodes i and j are assigned to the same cluster so here if i'm going to put a all one uh matrix uh that all one matrix uh that that says okay all nodes in the first class so they are connected to each other um then you uh you will uh you you this will correspond to the first cluster and the second one is uh going to correspond to the second cluster and this is going to be again an all one matrix and so on um and and here these arbitrary entries they will correspond to the to the outliers so what you want to find you you want to find a partition that that has this form and this partition which is going to to implicitly give you this uh this permutation as well so this this permutation again is the unknown permutation uh matrix that um is is going to uh in which the nodes are ordered according to the uh underlying structure of clusters and uh and outliers um okay so now what is the algorithm doing so the algorithm is based on uh perhaps i can show you first before i go into the details of these algorithms maybe i will go and give you a uh an example so this example is the example of the korean network and what we observe in terms of the of the of the korean network this is data that we that is published by bank of korea and it is actually a snapshot of the of the entire uh financial ecosystem in korea and so you have um a bunch of so you have the this the banking system uh insurance and then you have um securities uh credit specialized financial companies asset management companies uh mutual credit cooperative and you have um you can think about it it is not at the level of a network really um but you can map it to what i was uh saying in theory so you can think about each one as being a cluster and within this cluster you are going to have of course you know going to be a 20 i think 24 banks in the in the in this sample and uh i think this this uh mutual credit cooperative there's a bunch of there's uh 2 000 of them and so so some of them uh some of the sectors contain lots of institutions but looking at it and mapping to to to to the model you can or you can say that there is a connectivity pattern that is observed here so for example you observe this connectivity pattern between insurance company and banks and you observe here a connectivity pattern between security companies and banks uh and so on and this this is this is not going to be um not everybody is connected to everybody else so there's a weight over there and you can think about this weight as the probability you can you can um so everything here is of course expressed in a trillion wants and you can think about it also in terms of probability of of connection the larger the flows um in relation to the size of these sectors the larger the probabilities of of connection um so so this would be uh somewhat if you want uh oh okay so so i didn't look very well so you have here the number of institutions as well so you have 17 banks 56 insurance company uh 56 securities 93 asset management credit finance uh 86 and mutual finance over 2000 and so each for each one of them uh in fact you um now this is the snapshot of the of the connectivity of the sectors and uh we will test the algorithms and the goal of the algorithm is to recover and to identify correctly um which one is the bank which one is the insurance company which one is a mutual finance company um so uh and the way i test it is that i will create a network and of course in in in this case since this is a going to be a semi-synthetic network i will know the ground truth so i sorry so i'll know the and know which is which and then the algorithm has to correctly identify uh which is which um and um now how do i create the the the the network well from this you see that this is this is just at the level of the sector so i will have to create also a connectivity pattern this insurance companies you see there there are 56 of them and the this this aggregate flow it can be from any any of the 56 insurance companies to any of the 17 banks and on top of that there is data that is also published by by bank of korea on the size of assets also assets and liabilities for all the for all the financial institutions that you see over there what is not published by bank of korea is is the connection between each individual bank and another individual bank or for that matter between any financial institutions and any other financial institution so in other words everything that is published is at aggregate level so from this aggregate level i'm going to recreate a network that represents connectivity at the level of the at institution level and um and the the heterogeneity parameter is going to be the size of the institution and this is also something that is given um in in the data so the algorithm is supposed to find the this uh correctly this sectors from only one observation of the network so kind of imagine that i'm i'm fitting the algorithm a snapshot like this but it's gonna have more detail because it's gonna have connectivity between any two between any any insurance company and any bank so so so it's going to be a much much more detailed um picture and again uh the this is going to be a semi-synthetic network because it's going to be um it's generated uh using uh using the uh another uh algorithm so let me now go back to the algorithm itself so now um the algorithm itself is a variant of this modularity uh maximization and the and the the the modularity is defined as the link density within clusters minus the expected link density so um so this is the what you observe so this is a this is part of the adjacency matrix so again i'm giving you the snapshot of the korean network and you're gonna observe connectivity uh in that network and you're gonna be looking at the actual link density minus what would have been the expected link density if if the edges were chosen randomly and uh this is what you would be uh what what you would be uh getting as a definition of the of the model of modularity and the more clustered the network is the uh the larger this uh this uh uh modularity uh uh uh measure um now in uh you this this uh quantity here uh this represents the expected number of edges between two nodes so if you took just two two nodes at random then this is what you'd observe this is what you would have as expected number of edges between those two nodes and of course in your snapshot in your picture you're gonna observe more or less than this um and so you uh this this uh this um uh um this uh the modularity maximization is going to attempt to uh to to recover which two nodes are in the same cluster based on this uh based on this difference um and and and uh a tuning parameter has been introduced so so instead of people realize that instead of dealing with this uh expected number of edges one can replace this with the tuning parameter which gives you um essentially very uh it gives you a uh the following uh modularity maximization problem uh actually here is written uh this uh this this difference is reversed so it's becoming a minimization problem um and you this this constraint here subject that x is a partition matrix this is what makes this problem uh np uh and be hard um and uh so you essentially you'd have to search for all partition matrix matrices and uh then this is uh this is this is of course intractable so and that is because of that particular constraint so if you introduce ah relaxation of the of the problem you're gonna you're no longer going to to impose that this is a partition matrix and remember what was the partition matrix the partition matrix was simply saying uh which nodes um which nodes uh are going to be um in the in the in the same in the same cluster so it's gonna have one if uh one uh for an entry if uh those two nodes belong to to the same cluster and now you're in the relaxation you are simply going to say that um you you're no longer going to impose this so you're going to be saying that the entries are between 0 and 1. so essentially x is between zero the matrix zero and the uh and the all one matrix and it's also positive semi-definite um and uh some technicalities here from this particular uh statement of the of the of the problem you need to take care of this heterogeneity and this is done by introducing this uh penalty uh penalty terms and um now the the so what we are having here in in terms of penalty this is uh this is a penalty uh on the on the diagonal term and this um this uh this will um uh what this will do is um is going to penalize a potential outlier whose degree uh is unusual uh whose degree um is uh is beyond the the normal variation um that that would be implied by uh by stochastic block model and um now you you you have a a bunch of tuning parameters actually two tuning parameters here and alpha controls the strength of this diagonal uh penalization and the importantly this has to be done uh in accordance to the observed uh degree of the of the notes so um and and this is how you are going to determine which one are going to be the outliers and which ones are going to be uh the in layers uh so if you see that uh no this has high degree and uh this this term dominates so this this penalty dominates this uh this this difference then it means that that it does not have a strong community membership but it will it simply has a large degree and we can identify this safely as an outlier on the other hand you could be having that this uh this this um well this this this term dominates this this penalty term and what this means is that the access so this term represents the access or the difference between their community links so this a represents the community links and the difference between the community links and the expected community uh link so so so if you have this difference then uh those are those are the ones where you you have a large number of of links within the same cluster and then we can safely say that this i have a strong community membership andrea another question regarding the um so it comes from the q a are we talking about cross-sectional data for the korean industry example yeah so was their time series they wanted to know there was some time series component or yeah so there is no time series component it's just the snapshot of the of the so there's no time series in this particular um in this particular example it is about the a snapshot of the exposures in the korean network as published by the korean central bank and so so it's just the snapshot and it says who is connected to who is it says well this is the total number of assets uh total size of assets of which institution and uh this is in fact given uh so this is for each it is reported the total number of assets for each institutions but you don't know if those assets uh are because of a loan to another institution so you just know it in aggregate and you also know the uh the the the exposures of each sector to another uh sector and these foreigners are just measured in in the currency so basically yes one right they were in in one yeah okay and there's the direction right so yes and the direction is something that you that is also a snapshot that they published in the uh by by the central bank okay so so you see like uh and then like mapping that idea of the korean network this uh this b this is the connectivity matrix so this is what gives you the probability of one sector being connected to another sector and again this is a modeler's job to say how do i map from this picture that that the korean central bank gives me to to my to my model and uh and then what we are saying is that b represents a connection probability so so when i see a lot a big um a strong weight on uh like like here a strong weight like here i'm gonna say that there is a large probability of connection between those two sectors so imagine insurance is a cluster a and bank is cluster b but um so so now that i will uh i will skip all of this so the so just to tell you what are the theoretical results so the theoretical results they are essentially intervals for the tuning parameter for this alpha and and lambda that uh that guarantee the perfect clustering so this this uh this these are as i said interval so it looks like this they will depend on a bunch of the parameters of the of the of the model so so this is where you recognize the connectivity between clusters and this is how you would be mapping to what you observe in data the bab would be for example in the korean network would be the probability of connection between uh lsa as i said a or a insurance and b uh banks um and then if your lambda is is in in a right interval then then we are going to obtain um we are going to obtain a clustering uh and the clustering is via this this matrix x that says which nodes uh um so it's going to have uh one if two nodes belong to the to the same cluster so this one correctly recovers in this case uh the true cluster structure of the and of course this product this this is a statement that is uh non-asymptotic and is valid for a finite end so in this case i have to tell you that what is the probability of recovery so the probabilities of the form one minus o of one over n so so uh tends to one as uh with this uh with this uh speed um as n go as n becomes large okay so now going back to this korean network um now the uh so what how we test our algorithm is uh from this picture from this picture that you that you have here i am trying to recreate uh 20 sample networks that would be consistent with the aggregate um the aggregate values that we observe here because the korean uh central bank that's not published of course who which insurance company is connected to which banks so this is something that i have to generate in a synthetic network and the aggregate values of assets and liabilities they are given and there is this algorithm that is in fact uh very useful and and um many of the uh well in many stress testing environments uh this type of algorithms are used because we the central bank uh does not or or or the regulator does not it does not um [Music] does not observe uh the entire uh network and so you kind of have to fill in the gaps so you have to use uh either you have to use a method to fill in this uh these gaps and in this example we use a gibbs sampler to generate sample interbank liability matrices that are consistent with the aggregate uh value of assets and liabilities so and and this korean uh korean sector example is a perfect set up because we know uh the the ground truth we know the the ground clustering and we can test the performance of the algorithm via the misclassification rate um and uh we can see what the uh what we it should have been assigned for a bank and then does it get is it assigned also does the algorithm uh assign it correctly so now how how the picture looks like so if i am to generate one sample network um with this uh starting from the starting from the global korean picture it is looking something like this and you see my picture is gonna have here this bunch of this little cluster is going to be i'm seeing it visually as a cluster this is a cluster of banks this is the insurance companies um i don't recall what this one's this uh this one are the mutual credit companies um and so so you have it's not this this number of uh as many financial sectors there's six of them and there should be six one two three four five six on the diagonal right and so the algorithm is is given this picture and so the algorithm now has to say well these first guys here uh they are the banks and this this other um institutions over here those are the insurance and this one are the credit uh finance companies so it's supposed to identify correctly um the to which to which um sector they belong and you see visually it's not entirely obvious that this uh this this uh set of institutions is not one cluster it's actually two clusters one of them is going to be the the banks and the other one sorry this this small one is going to be the banks and this one are going to be the insurance and it's not obvious uh in the way you observe that it's it's it's actually a different uh different type of institutions you observe the connectivity pattern so you observe the connectivity pattern that the banks are more connected to a bunch of other institutions whether whereas this one's here the the the this other cluster here those are the insurance and they are not con they don't have connections to this group over here so let me look again the third one is securities so so there wouldn't be connection between insurance and securities but of course there would be connection between banks and security companies so the algorithm um has to has to be robust to all this uh heterogeneity in connectivity um and and uh this is uh one of the performance criteria for the for the algorithm so when you look at um at how this algorithm performs you m you you have to measure it in terms of misclassification rate so we measure it in in terms of other uh existing algorithms so this would be the the rate at which the algorithm uh misclassifies so says okay credit uh company is a bank and and you see that this this algorithm that i presented has the lowest misclassification rate um in general i would say that misclassification rates uh tend to be on the low side for all of this algorithm but for certain problems the difference in algorithms can can become uh significant all right and this core algorithm which is based on spectral clustering is the worst performer um and i'm saying this because these are algorithms that are widely used in practice so in the presence of of this kind of challenges that i described for financial networks um this is one of the reasons why you wouldn't want to use of the shelf algorithms and you want to look into more tweaking existing algorithms to account for for for all these uh issues that we uh encountered and now the other uh so i'm gonna stop uh on on this the korean network i just want to take one minute to or two minutes realistically on another application which is um something about uh that could have applications to investment portfolio management of course i i didn't um look at the performance uh i didn't test it uh out of sample but this is something that that could be considered um so let's say that you you you can look at funds let's say mutual funds uh or hedge funds or or basically any institution that reports their holdings on the thirteen f in the certain f filings um well in this case what you could do is you could recreate a network of overlapping portfolios so you could you could measure the strengths of interaction between two institutions based on their portfolio holdings and how much this overlap so you could do that we described this in a bunch of papers and it's not just our work a lot of people have worked on on this notion of overlapping portfolio um and so so for our purposes assume that you have a way to uh describe the the portfolio structure via a network and so now you want to say let's let's look at this network i'm giving you a snapshot um this is what you observe in terms of strengths now i don't know the strategies of these funds i i don't know anything about them all i know is what they have uh declared in the 13th filing so and from those ones i can infer how strong are these connections and how overlapping those portfolios and now let's let me do the the what i've been describing let me now cluster these fonts let me say apply the clustering algorithm and say okay these funds are a big cluster this other one are a big cluster well the the the application of that would be uh you you you could achieve diversification um by let's say allocate risk uh to to into each classes so for example if you were to invest in mutual funds you would not invest you you do a cluster risk parity uh in this in this case uh you you're gonna account for the fact that a bunch of mutual funds they're all in the same cluster so they they're they're there they have huge overlap so now this algorithm is going to tell how to cluster this this this this uh network of overlapping portfolio and now of course as i the algorithm is assuming that we tell how many clusters there are so you can guess how you let's say you have you assume that you have five clusters and so you when you do that you can recover um you you the picture is gonna look something like this and it's going to recover uh five clusters and then um you one two three four five um and uh you you can then say okay and and moreover it's going to tell you which of the fonts belongs to to the first cluster the second the third and so on and so you can do that uh i did it for the 2003 data so connectivity of 2003 and versus 2012. so in each of these cases it obtains a cluster and what you observe is that from over this uh less like a a decade uh you observe that the the bigger cluster becomes bigger so that would indicate that there is more more overlap in the in the portfolio so of course what remains to be to be to be seen is if you allocate your funds according to this what would be the the performance but for our purposes what we are providing is a tool um with which to to to generate these clusters and here it's very interesting example because if you compare this to existing clustering methods what you um okay so i have a question for the korean example how does the algorithm decide the relationship between the specific companies based on just aggregated data from the government how does segregated data get mapped to the more granular names so as i was saying uh this is done via an additional algorithm which is not part of our work that is uh that is this is doing exactly that how we it is filling in the gaps it is creating individual networks that are consistent uh to the with the aggregate data from the government and moreover we can impose certain heterogeneity heterogeneity properties now back to the from the korean let's go back to this overlapping portfolios um and and in the overlapping portfolios uh i think that so the clustering other clusterings um are less desirable because they observe uh so this is what you would get into by using um one of these two methods um so spectral clustering and sili i uh so in spectral clustering you would have um high inter-cluster connectivity um and so tyler would be uh will be this one where uh sorry so chile would be this this one where you have one big and intra connected cluster and the other ones are barely connected so so then it you wouldn't achieve anything uh in diversification because you you wouldn't know uh this this here it would be almost uninformative they they they they would be completely um [Music] like almost almost uh unconnected uh among each other so so it wouldn't be really uh looking like you'd expect for for a cluster and there would be one huge one huge cluster which would be very undesirable from a diversification standpoint whereas this other one that is uh you you you'd have spectral clustering and that one would would have one um one cluster which has extremely high uh inter-cluster connectivity um and uh and then the connection uh and and then here you you'd give you you'd have uh uh on on the other extreme you have one cluster that has uh basically uh no connectivity no no no inter cluster and no very very low intra cluster connectivity as well so in general both of this would would be wouldn't achieve diversification and as opposed to looking at this picture over here where you get somewhat um [Music] five alternatives where you could uh diversify and under um a cluster a risk parity approach all right so thank you all so if we have uh that's that's all i wanted to say uh so thanks again for uh for uh for the questions and if there are more questions i'd be happy to to quickly take uh more of them in the while we wait for some questions to come in i have a little one that i wanted to ask you so many of these clustering algorithms i think including yours has as an input the number n the number of clusters and the numbers are r is the number of clusters okay so uh what i think was really interesting in your work with the korean networks is that there was an an imposed uh r by the structure you know you knew that there were six groups and then you tested your clustering algorithm by seeing oh how well do i fit into those groups or or i i think one actually in that one as well we are also we we said we knew that there were six sectors so we knew the number of clusters yeah you knew six you knew the number six but then after you used the number six you were able to say oh we misclassified or you were able to quantify how well the the algorithm does and i wonder if in the other application the portfolio one at the end of the day these mutual funds often have styles like growth mid cap large cap value you know they also have natural names and i wonder whether there's some potential to use that you know the number of categories as the number seven you know we need yeah exactly and this is what what we did so from from that one we used exactly the usual uh yeah types of investment as number of categories and this is what you would do but then it would be very interesting to see if this completely data-driven approach where you just look at the holdings um and you would get a cluster if that actual cluster is matching the investment category that the fund declares so that would be something also interesting to to see or if they are overlapping across investment strategies and also the at the end of the day it depends uh so it's it would be uh now what do you really care about in practice do you care about actual holdings or do you want to care about their uh their target strategy uh because the fact though they will have some overlapping portfolios and these are the ones that are going to to to determine the the impact price impacts and and so on um so so probably probably you do you want to assess whether the recover clustering using this data-driven approach are matching the the target strategy and then you would want to to to uh you'll want to take into account both of them okay well um we have some questions we have a bunch of questions uh one about the preprint yes the preprint exists and it's posted publicly um the same as the title of your talk so people can google yeah yes yes yes and you can search it on my website as well and and uh you mentioned a tool is there something that's available uh i yeah we uh it's not publicly available the tool it's it's uh it would be a simple implementation of the algorithm using the standard tools in optimization um and a lot of algorithms are sensitive to the number of cluster is this the same case for the new algorithm i think so yes i think that indeed the algorithm depends on the number of clusters and now what you would yes so so you you would ask yourself um [Music] are the as i let's say that i'm considering a clustering algorithm and i'm assuming that i'm having five investment strategies and then i'm gonna be recovering uh five clusters and then assuming that i'm going to say next that um the algorithm takes six clusters you you'd want to see a certain relationship between the two so i cannot say whether this how how the clustering under two different uh r uh choices are going to uh to compare so we don't have theoretical results on this um and so this would imply that you'd have to to have very good domain knowledge about about the number of clusters of make a very good uh standing assumption about this number of clusters okay andrea well it looks like uh like we've answered all the questions and we had a lot of uh interesting feedback and questions today so um i really enjoyed your talk and thanks thank you thank you thank you all right good evening everybody bye 