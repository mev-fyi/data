hi everybody thanks for coming we're gonna get started I'm very happy to have Gordon Ritter here talking about reinforcement learning applications in finance and it sort of we're very happy to have him because it falls nicely under our umbrella of financial data science and machine learning which is kind of the theme of the semester of the seminar and and we're we're here eager to learn and well I instead of going on and on about Gordon's achievements and you know great greatness I'll let I'd let let his talk speak for itself thank you for not going on and on about that but clearly the right decision okay so I'm going to talk about tasks requiring complex multi period planning and strategy in the presence of uncertainty in the presence of an uncertain environment making a plan a strategic plan to be long term greedy so in some sense that sounds like my job I think all of us at some point in our lives have faced a problem like that where where we're trying to come up with a strategy that you know in in games of strategy such as the ancient Chinese game of Go you might give up part of the board as part of a longer-term strategy to encircle your enemy the for as many people know the for the longest time the ancient game of Go was held up as an example for how you know human intelligence was still superior AI hadn't really gotten that far sure I could solve simple little problems but you know for anything requiring detailed reading a real strategy humans were still superior that's no longer true the best go player in the world is now an agent trained by reinforcement learning methods that while very sophisticated in some of the details are conceptually exactly the same as what we'll talk about today and you know we're seeing a lot of interest in these methods I mean I think arguably there was a there's a there was a period you know up until a few years ago when a lot of the focus and machine learning was on deep learning and I think you know then that's still very active but to some extent we're seeing a lot of people are now shifting focus to looking at reinforcement learning so let's start with what is learning so I think learning is specifically learning how to choose your actions to optimize your interaction with your environment and in the sense that we maximize rewards received over time so a robot is is said to intelligently learn to to navigate through a room if it can optimize a reward function that looks like you know penalties for the number of collisions minimize the the path minimize the energy taken with so that's you know that's that's what what is intelligence is in the context of mobile robot navigation I would argue that a lot of a lot of colloquially a lot of cases where we look at something and say you know hey that thing that thing is really smart but by what we mean by that is that it is precisely this that it optimized rewards cumulative rewards over time it found some particularly clever path through some complicated problem space as another example a gazelle is born not knowing how how to walk it learns how to walk in the first hours of its life as brain learns how to send signals to its leg muscles so as to optimize again after my interactions with its environment yeah so the learning happens the the thing that unifies all those cases is that the learning happens through experience this is what we all know already as trial and error so learning through experience is nothing new in psychology it was first studied in detail by by Ivan Pavlov in 1897 and a famous dog experiment I've summarized Pavlov's results here that's a joke but but but in all seriousness we're gonna build a little model of an agent interacting with an environment here ok so the it's a sequential process the agent takes in action that that the the action might might be sort of something the agent does to the environment the agent then observes the state of the environment and and also observes some sort of reward signal achieved in the process and and it repeats itself so it's sequential the agent after observing the reward signal and observing what their action did to the state of the environment the agent can then potentially update their strategy and as a and learn to to continue to optimize the reward signal okay and when I say state here it's quite appropriate to think of an analogy to what statisticians call states based models alright so a lot of a lot of classical time series analysis was organized around hidden Markov models and and and similar models where there's some hidden state is a key aspect of the model and the state is assumed to have some dynamics typically Markovian dynamics and so there's a Markov process and a lot of classical statistics that was sort of where it ended you know so yes you there's a Markov process you try to fit it to some data you don't learn in classical statistics typically you didn't try to interact with the Markov process and make make decisions that would would you know allow you to better interact with the Markov process so here we do and so this is an extension of a Markov process called a Markov decision process but still very closely related to it you know what what everyone calls States based modeling okay so the goal is to maximize the expected cumulative reward so I'll follow the notation of Sutton and Bardot here who are two of the the fathers of the field so in such an and bardo notation so we want to be long-term greedy so we want to think about cumulative rewards over many periods in the future with the caveat that it's always nice if an infinite sum actually converges so given that the reward signal could be something quite general if you're really going to make it infinite you typically introduce some discount factor which makes sense as well that mean that makes the agent care slightly less about rewards that are very far in the future so it's it's long term greedy but not to the point that you know it's thinking about a billion years from now and the Sun has burned out or something alternatively there's a all of these results that we're going to talk about also exists in the sense that if you have a finite number of periods it can learn to optimize average reward over you know rather than accumulative reward it can learn to optimize average reward either way the key point is that it's long term greedy it's not just thinking one period into the future which is key for for strategy real strategy right you might you might give something up you might accept a small short-term loss in order to get yourself into a position for a larger long-term gain okay so reinforcement learning is defined as the search for policies to maximize the expectation of the long term reward so again quoting Sutton and Bardo the fathers of reinforcement learning they say the key idea of reinforcement learning generally is the use of value functions and to use use value functions to organize and structure the search for optimal policies now anyone in the room who has had a kind of a you know serious education in in the theory of financial derivatives is already quite familiar with the notion of a value function I actually first encountered mathematical finance when I was I was in graduate school and in mathematical physics but I heard that over on the other side of the river someone who had won the Nobel Prize was giving a series of lectures that at the the Harvard Business School and so I went over to hear them and and that was Professor Robert C Merton who was lecturing from his book on continuous time finance and that was actually I I think the first time I encountered the the hamilton-jacobi bellman equation which is an equation satisfied by the value function of an optimal policy so that's exactly what I mean here but we're gonna look at cases where you probably can't explicitly solve the hamilton-jacobi bellman equation right I mean there are there's sort of a number of explicitly solvable models in hjb equations but sometimes in real world problems you know it's it's just too hard so so so what do you do maybe the problem has only a discrete set of actions you can take or various other aspects that make it not particularly amenable to a kind of a classical hamilton-jacobi bellman type approach but nonetheless that is the value function I mean here and there's a value function that's defined on the state space and then there's a slight generalization which is defined on state action pairs so that's the the value function I'm going to use most here so it's called the action value function it's the expected long-term gain starting from state s and taking action a as your very first action and then following the policy thereafter so each policy has a value function okay now if we knew the cue function or action value function corresponding to the optimal policy call that Q star then of course we'd know the optimal policy itself right you would choose you would look at what state you're in and then choose the action a that maximizes the value function for for all actions in that state okay so that would be called following the greedy policy so the problem the whole problem of finding the optimal policy then reduces to finding Q star finding the optimal value function and so reinforcement learning is all about techniques for finding Q star or you know iterative techniques so typically it's an iteration one produces a sequence of functions which under some idealized conditions converge to Q star okay so so that's that's that's a kind of a 10 minute introduction to reinforcement learning Sutton and Bardot have actually made their book free online so I highly recommend it so now coming back to Finance okay so a couple of worthwhile questions I think one question I think a lot of people have probably wondered about or are curious about and I mean it's sort of the the holy grail of this thing is you know can artificial intelligence discover an optimal dynamic trading strategy in a realistic scenario so in in in real world trading is expensive right I'm a quantitative trader by day and I teach some courses in in the evenings so but in my day job I'm frequently confronted with the fact that you know all but the smallest trades move the price of the asset we're trading and and cost us that way there are other costs such as bid-offer spread and and and and commissions and and so on so in the real world any action we take ie any trade that we do is very costly so given that so typically we look for strategies that overcome those costs right so given given that given a kind of a semi realistic scenario could it a I discover an optimal trading strategy if there were an arbitrage in the system in the market could could an a I find it I know a lot of classical finance was devoted to the question of no arbitrage type results you know so assuming no arbitrage you know what what what can we say how do we price derivatives you know you know no arbitrage is equivalent to existence of the queue measure and in a famous famous theorem so I find a connection to all those those results here because you know essentially were able to we're able to say if there were an arbitrage we will be able to we will produce an agent that will find it at least given enough data we'll come back to the question of how much data a bit later but so for example you know I could I could give this I could give this thing a simulation of a system that that did have an arbitrage hidden in it somewhere right and you know I could go back and ask classical finance experts like Steve Ross you know how do you tell if there's an arbitrage in the system and you know Ross would probably answer well you know if there's a consistent set of arrow-debreu prices then there can't be one and and and things like that whereas this is sort of a much more practical approach that it's it's say if I want to know if there's an arbitrage in a given dynamical system I will you know I'll I'll throw this at it and and and you know train it and and see if it finds a strategy with a high Sharpe ratio it will also find so-called statistical arbitrage is not a pure arbitrage just a very good trading strategy okay so if there were such a thing and I would say it's sort of the financial analog of alphago zero right alphago zero learned to play go with the zero means zero human guidance so it was you know I think a lot of early approaches to AI for games like chess and so on they sort of modeled their AI on very good human players you know the deep blue the system that beat Garry Kasparov had some huge database of moves of you know human chess grandmasters and you know past games of Kasparov himself and so on alphago zero was not like that at all it had zero human games zero moves from human experts zero human strategies for playing go which is really what makes it all the more impressive it was told the rules of the game it was given a simulator and run for for many many days or months on some of the more powerful computer clusters at Google and what came out was a go player that was a thousand times better than you know the human champion or approximately many times better than the human champion so okay so can can can I say I discover an optimal dynamic trading strategy and what does optimal mean exactly so I want to discuss that for for a minute you know going back to well really to to daniel bernoulli discoverer of the discoverer of the bernoulli principle in fluid dynamics that's why an airplane wing works Bernoulli also studied the problem of decision making under uncertainty and is is generally credited with the idea that to resolve various paradoxes such as the st. Petersburg paradox people do not make decisions to maximize expectation of wealth they have a utility function and they make decisions to maximize expected utility of wealth this this theory was formalized in in the 1960s by arrow also a Nobel laureate in economics and and John Pratt and and is the basis of a lot of economic thinking today so and I suppose I should also mention here the von Neumann Morgenstern theorem which says that if your decision-making follows satisfies certain axioms of rationality then then you are in fact choosing different different risks to take according to according to this prescription according to one point to for some utility function and even if you didn't know it I mean you may not have known you had a utility function you may have somehow just made your decisions rationally anyway so whether you know it or not either you're irrational or you have a utility function so not surprisingly many problems in finance come down to this sort of thing so this is this is a brief review of von Neumann Morgenstern theorem right so when applied to trading of financial assets the outcomes are different levels of wealth at some future time rational agents maximize expected utility of wealth not expected wealth since maximizing expected wealth leads to various paradoxes and would ignore risk right okay now it turns out that under certain assumptions maximizing expected utility of wealth is equivalent to maximizing a mean variance form of that problem so let's clarify the assumptions they're more or less summarized on the next slide essentially these assumptions say that returns are our for various periods in the future are independent for different time periods the assumptions say that changes in our wealth come from portfolios that we will hold in the future the assumption one assumption says that the utility function is increasing concave and continuously differentiable to second-order so it's a c2 function for the mathematician in the audience and one assumption says that for each T the multivariate distribution of returns is is elliptical that means the ISO probability contours are ellipses note these assumptions don't say anything about fat tails by the way I mean the asset returns are allowed to have quite fat tails i've not assumed normality so a student T distribution is fine here Koshi distribution is fine okay although we might have a problem with this bit if it were a Koshi distribution but in any case I make those assumptions and that the first and second moment exist one can mathematically then prove that maximizing expected utility of wealth is is mathematically equivalent to maximizing the mean variance quadratic form and this is quite similar to the prescription that Harry Markowitz wrote down in his PhD thesis in 1951 although he didn't derive it this way not surprisingly then that this mean variance form keeps coming up over and over again you know so Algren and and Rob longren and Neil Chris in their formulation of optimal execution more or less take this as an assumption but it's the only reasonable assumption for elliptical returns and and a rational investor okay so I'll focus on on you know how do we train agents to maximize the mean variance quadratic form this it is not a utility function by the way sometimes I always I always you know grimace a little bit when I hear someone talk about a quadratic utility function Kenneth arrow himself pointed out that makes no sense a quadratic function goes up and comes back down it can't be a utility function it would say that after some level of wealth you actually prefer to be poorer which you know even if you're a philanthropist you prefer to have more wealth to give away so no one actually has a utility function that goes up and back down so if you hear someone talk about a quadratic utility they're either an expert paraphrasing and they mean this or that they just don't know what they're doing I like to think they're all experts paraphrasing so okay so let's let's think about can we train agents then to act like von Neumann Morgenstern rational investors so rational investors have a utility function and stare equivalently maximizing the mean variance form so what if I could invent now we already said reinforcement learning is about maximizing cumulative reward over time in the usual formulation it's the arithmetic sum of discounted rewards what if I could invent some rewards signal that makes the agent care about variance as well it makes the agent care about risk it makes the agent risk averse then well then I have a hope of training rational agents okay so here's a simple one there are there are various improvements one could make to this prescription but very simply right I want a reward signal where if I if I sort of averaged it over time I will get something like the mean variance quadratic form okay so here's one the first term is just that the change in wealth itself which traders sometimes called a pl the second term is essentially the square of that and it's so if I average if it were mean zero and I averaged the square that would give me the variance so so that so I have a reward signal that combines the increment in a single period minus a constant times the square of around the mean and if I average that over time it will it will converge approximately to the mean variance quadratic form so this will this will be what I choose for for the reward function okay so what about so that's the we've answered then the last part of this what is the reward what are the states based in the action space then so this is my one slide advice on on choosing what to put in the state so sometimes a question I get a lot from students what do I put in the state so the the main principle here is that if you don't put something into the state then the agent has no chance to learn to use it okay so if there's some some kind of signal that you wanted to learn to use to make a trading decision if you know if it's trading an option let's say well then it need there are certain things it absolutely needs to know like how many how much time until the option expires you know that's that without that crucial piece of information it you know it can't possibly learn an optimal hedging strategy right so things like that you have to you have to put it into the state anything that you want to it to learn to use because I mean the state is in some sense all it sees from the environment okay so you don't put in some something in there it won't see it it's sort of like you know the state is is its lens into the world okay so for example you know if we're thinking about optimal execution which i think is probably an area where this this technology will will be applied in the next 10 years the current state of the market right so the current microstructure are limit order book details you know is that the you know is the is there an imbalance in the book and and various state level order book state level information like that okay so then what's the so that's kind of how in general how I advise you to build a state vector and it's I mean that isn't even specific to reinforce if you just wanted to model the the system as a dynamical system you would have to sort of decide what the state is that's experiencing that dynamics so it's the same same decision so then what should the action space be if you're trading something if you're training an agent to make trading decisions yeah the most obvious thing that has to be at least part of the action is how much to trade so how many shares are you going to trade there could be more to it though again coming back to the the market microstructure example the agent could decide which execution strategy to use which algo to use that that could be if you're trading entirely through broker algos well you have a menu of them okay so choosing one or the other or choosing a parameter in an algo that changes its behavior is a kind of a discrete choice and that's that's on your menu of AK actions if you're actually trading in the microstructure you know you would have to decide do you cross the spread or do you join a queue on the near side of the book or do you cancel all existing orders and weight or various those would be those would be your menu of available actions and this is a case where you know this has a chance to outperform something like imagine some kind of naive implementation of Allah and Cris on where and Chris's original paper now almost now exactly twenty years ago it does assume that that the the agent is being aggressive and just sort of crossing the spread right they have a cross spread cross term for every every trade if if one of the assets is an American option there may be other actions as well early exercise is in action if you're talking about American option I haven't actually tried this but this thing should should absolutely should be able to learn the early exercise strategies for American puts before a dividends and so on okay so let's now come to a kind of an engineering example the Ornstein and limbic process so this is a process that's very commonly used in finance to model mean reverting dynamics okay this this is this example is in in my paper on SSRN so it shouldn't be necessary to take notes so I'll assume that there's some equilibrium price to the asset reverts to so law the log relative price has as mean reverting dynamics the the log of the price divided by the equilibrium price you know is that's of course zero if it's at equilibrium and otherwise it has a force that pulls it back to equilibrium so this is this is a well-known process I would know that if if the price of a tradable security followed these dynamics then that would imply some something like an arbitrage at least a statistical arbitrage right so this is this isn't a martingale so there are there are profitable trading strategies one could build around this right so I've emphasized that in red here I'm building a stochastic simulation which I know has at least an approximate arbitrary as a statistical arbitrage in it in the sense I don't know by the way I don't like I don't like calling this statistical arbitrage because I also call my job you know what I do statistical arbitrage and it's really not that similar but anyway it's a statistical arbitrage in the sense that it's you know it's not a guaranteed profit you could you could you could take a position you could let the asset price get out of equilibrium taking position that bets that's going back to equilibrium and you could still lose so it's it's it's it's a high probability of profit trading strategy not the the typical the standard definition of an arbitrage in smads but any case I don't allow the agent to know anything about the dynamics okay so when I set up an agent I'm not I'm not telling it try to find mean reversion I'm not telling it you know fit a model and make a prediction I'm not telling it anything about the dynamics it doesn't have an equation it doesn't know what lambda and Sigma are or even that some parameters like that even exist it has to figure out everything for by the same way as alpha goes zero by by playing games and and I mean alpha goes zero I'm sure you know lost a bunch of games while it was learning what to do right so this this similarly we'll have to play and lose a few times so trading caught it also doesn't have a trading cost model I mean I built a trading cost into the simulation but the agent does not work by assuming a trading cost function and fitting parameters to it or taking on grand rob long ruins trading cost function or anything like that it doesn't it initially doesn't know there is such a thing as trading costs but the returns it's going to observe will of course be net of the cost so it will it will learn the hard way that for example if it trades too close to the equilibrium price such trades are expected not to overcome their cost so specifically I just assumed a spread cost a fairly standard way I assume there's some tick size and a linear cost proportional to the tick size I also assume there's an impact cost based on a linear price impact function which leads to a quadratic total impact cost and sometimes I like to play around with a just a multiplier in front of the overall cost so I can simulate for example I can simulate a very illiquid market by setting the multiplier equal to five or something like that like five times a kind of a standard market cost function so this is kind of the omron Chris cost function but so it's again I chose it to make connection to the literature the state I took in this problem looking in this problem in some sense the forecast is in the price right so it's the price itself getting too far out of equilibrium that that is sort of the signal here so the state vector can be quite simple it's just what you what you hold and the price in if there was some other data source let's say that you think is is going to be part of a predictive signal then that would also have to be part of the state and this in this this and this is a price only you know the price the price contains the signal so I don't need any higher dimensional state for this so this is what happened when I trained it on 10,000,000 training steps and I then evaluated the system on 5,000 new samples of the same stochastic process so you know don't get too excited about this PL I'm not saying that we could you know that we can realize trading strategies that look exactly like this but this is this is a proof of concept in the sense that when I started when I started looking into this and started doing the research for this paper I was kind of I was I was pretty pretty skeptical about all this you know machine learning stuff to be honest I sort of thought that you know there's a lot of hype and maybe not that much substance and so I started off with it with like the mindset of well mecan't find an arbitrage in a system where I know there is one then it's totally worthless so like in some sense if this plot hadn't been more or less straight up then the you know I would have been able to conclude that this is all total baloney and we should throw it all out and go do something more useful but but this this does not allow that kind of total rejection it it says that look I created a simulation that has an arbitrage I didn't tell the machine to look for an arbitrage or look for any I just told it to be a von Neumann Morgenstern rational investor maximized expected utility and I didn't tell it I didn't tell it to do what a lot of quantitative traders do which is you know have a return forecast model a risk model a trading cost model fit those parameters in some way and you know fit them to historical data and be careful about overfitting and look at out-of-sample data and then you know maybe use the model but it didn't have any of that it all it has is the bellman value function right so it and in some sense you know if you have the optimal bellman value function you have the optimal policy that's all you need you don't need them all of those modeling steps if you have the optimal value function I mean those are just ways of getting too close to to the optimal policy so okay so the initial results are encouraging in the sense that we know this is a game where we know it's possible to win right a system where we put in a statistical arbitrage if you like and I mean to be fair in you know many many real games pac-man or whatever you know you know there's a way to win the question is can the machine find that the answer for pac-man is yes the answer for trading a mean reverting process is also yes apparently but okay but I say it works by learning a value function and it works by it by a classical method called Q learning where I have essentially a matrix that represents the value function so I discretized state space i discretized action space a function on the cross product of two discrete sets is just what we call a matrix or a spreadsheet if you're an economist sorry though I wasn't meant to be a diggit economist but so the Q function is is literally a spreadsheet here so I don't particularly like that model for for a q function by the way because it essentially has to learn each matrix element independently it doesn't you know it doesn't really know that for two very nearby states it should have closed value I mean there's no continuity so I just you know learns each element of the matrix separately by having experienced that state and tried various actions in that state and it starts to know something about that particular element of the matrix for that state in action and then I essentially would have to start over for nearby states so I don't I don't love that that model let's approximate a function as as a matrix with no relationship no no continuity but and and so this is what the value function looks like so just just so you know what you're looking at right I this is the value function starting from zero holding right so in some sense if you somehow got to this price level and you didn't already have a position all right so that's why that 0 and P remember the state space was two-dimensional you know initial holding and or the holding you come into the period with and the price so I then I took the trained cue function that generated that high Sharpe strategy and I plot it as a function of the price for various actions so here I have actions ranging from buying and selling 100 or 200 shares or doing nothing the greedy policy would choose the action that has the highest value function and I know it's a little hard to see but the zero action do nothing is green here okay so we immediately see the emergence of a no trade zone the equilibrium price is 50 there's a region around 50 where the highest point is always green meaning that the optimal action if you didn't already have a position and you're close to the equilibrium price the optimal action is don't take one don't take a position wait until you know the the process gets further out of equilibrium I mean in a you know in market microstructure Theory one of the oldest and most classical models is the role model where you know Richard role pointed out that if you have an order book that's static so you have bids and offers that aren't changing if the last trade price were if the lad last trade price movement was up the next one is either same or down if the last one was down the next one is either staying or up so there's an chiral autocorrelation negative autocorrelation in short-term returns it's not a trade and trading opportunity it doesn't mean we should all go fit in AR 1 model to short-term returns and and you know think we're gonna make money out of it and so a criterion for any kind of fitting procedure that we're gonna actually use in finance minimally it ought to look at look at that kind of short-term return and say nope sorry no no opportunity here please do nothing so that was also kind of you know in the back of my mind when I was so so I can I like this in that respect it does realize that it should do nothing if there's not much of an opportunity but another thing we observe is due to the due to the dynamics the probability of getting a price very close to zero is very small that's just the way those dynamics work if if the log price is an orthogonal impact process so it never visits states that are that have low prices and as a result the like we initialized the this Q matrix to zero and it's still zero because it has no experience there may be something more clever would be to extrapolate based on what we see getting close to that region it also seems a bit schizophrenic right so if you look as you get close to the zero region you know they're like points right next to each other where sometimes buying a hundred shares is believed to be optimal and other times buying 200 shares is believed to be optimal just based on which color is highest in this chart so I think this is a very noisy representation of the value function so so basically because the tabular method estimates each Q value individually with no nearest neighbor effects okay but in this case the I think the optimal action does have a kind of a natural monotonicity right if our current holding is zero and for some price P the optimal action was to buy a hundred shares then for any price P prime lower than P the optimal action still be to buy at least 100 shares I mean again as it as a traders intuition you know if if you liked it at price P don't you like it even more at a lower price so there is a there is a kind of a monotonicity and the value function just from from intuition so these are the problems we've talked about it collapses in the left tail and so forth and and all these problems are related to to using a finite state space so I've in my own research I've transitioned to essentially always using a continuous state space has a lot of advantages one of which being this would require us to numerate all the states right so you know what if you had you know ten state variables and each could have a hundred different values the state space would be you know hundred to the tenth power you aren't going to get very far you know representing things like enumerating all the states and representing it as a matrix that's one problem so so I went over to using a continuous state space and I went over to using what you might call well I used a function approximation method a very simple one which is called model model trees like so neural networks are universal function approximator x' they're all there's there but there are many universal function approximator x' another one is is called you know a forest of of regression trees so I use that kind of function approximator here instead of a neural network mainly just because it was much faster to train okay but the the key idea of all these continuous states based methods is that the the bellmen value function Q is as an unknown function we through experience we are going to build a kind of a set of training data if you like that we're going to to then try to approximate we're going to try to find the the best the best unknown function for the value function that fits the training data we've seen in a way so another way to think of it let's say you think of a neural network as a brain you know then this is basically okay now put that brain in the context of repeatedly interacting with a Markov process and getting back rewards and and then you know and you sort of you know the brain learns that way through experience I you know I hesitate to call something as simple as a model tree averaging function of brain it's but it's it's a statistical machine learning technique right so there's an excellent book by hasty tips Ronnie and Friedman called elements of statistical learning where you can learn all about statistical machine learning or SML and and that's that's the kind of technique you know it just gives another function approximator right and that function approximator is trained with essentially inputs and outputs right and the inputs are States states that it's visited and actions it's taken in those states so state action pairs and then the outputs are some version of the reward of some approximation of not just the single period reward but the long term reward and how are you going to know the long term reward well you could you could follow what happens many for many periods after it or you could simply take the reward from that period and use the Q function itself to approximate the the really long term reward because that's what the Q function is and so if you've already built up a reasonably good approximation of the Q function then you can use it to approximate the long term reward and use the actual rewards to approximate a short term reward so that's actually what I did and I get a value function that I think makes a lot more sense it extrapolates to the tails very well and you see you can now see all the more clearly than no trade zone where zero share action is optimal and you can see it crossed over to where for a certain range of prices that are higher than that selling 100 shares is optimal and then for the highest very highest prices I didn't I didn't for this simulation I didn't let it by yourself or than 200 shares in any one period mostly because having more than about eight lines here would we make this this graph a bit ugly okay so so I'm by using something better than a matrix to represent the Q function namely some function approximator that's better than a step function which I hesitate to call a brain but it could be a neural network you can you can get a much better approximation of the the bellmen value function you can get a continuous function for example okay there's some test results now as time is short I want to move ahead to two derivatives so as the second kind of case study I took the simplest possible derivatives hedging an example so European call option and the the motivation here right is that you know the dream is okay let's say that you're a bank and you have some derivatives that are sort of on your book that for structural reasons you have to maybe you have to hold those derivatives maybe you know you created them for a client or something but the bank would like to hedge its own risk it would rather not have those derivatives on its book but due to some constraint it can't just dump those derivatives on the market it would rather just kind of neutralize its risk in those positions what I would like for the bank to then be able to do is basically press a button and you know in a few minutes train up a very specialized AI that knows how to optimally trade that that particular basket of derivatives and then you know let let it let it handle the trading until they all expire so automatic hedging you know I've got a derivative position that I'd rather not but I can't sell out of it what I can do is try to remove its risk by Trading the optimal dynamic replicating strategy okay but again in real world trading the the dynamic replicating portfolio would incur a lot of costs and it depends what the cost function is and what the market impact function is and you might want to use some sort of realistic market impact function which maybe looks you know like linear impact for small trades but then goes over to a square root law for large trades so you know there was a very nice paper bye-bye boo show and collaborators you know on that model from from a couple of years ago so if that were the cost function how would we how would we you know or maybe it's a black box cost function I mean maybe some other part of the bank has a closed source C++ library that will will tell you the cost if you've done if you give it a trade but it won't tell you what the actual function is it's just it's like a black box you function that you can evaluate but you don't know the function that's fine too okay so then so first of all for European options as we mentioned the state must minimally contain the the underlying price and the time to expiration so tau is expiry minus the current time and these are essentially the the variables that would go into computing the Delta in a black Scholes type type world right now there are other variables that would go into to computing the Delta that I don't need to include here the strike price of the option is part of the definition of what the option is and if I'm training an agent to hedge that specific derivative rather than to hedge any old derivative of any kind then I don't need to include the strike price it will just learn to hedge adoption struck at at you know 50 and if I want something to hedge an option truck struck at 60 I'll have to train something else to do that that's at least the approach I'm taking here that's why the strike price is not part of the state okay the state does not need to contain the option Greeks why well because I expect the agent to learn those nonlinear functions themselves okay so I'm first going to consider a frictionless world no trading costs and try to answer the question is it possible for a machine to learn well we teach students in their first year of business school and formation of the dynamic replicating portfolio strategy okay so unlike our students however the machine can only learn by by experience where to and we want to generate a large experience set so we do that by simulation what's what's important here is kind of analogous to the the mean reversion case where you know the agent was not explicitly not told to look for mean reversion it wasn't told fit in or in Steinem aback process or anything like that in this case the agent does not know the strike it does not know that the price process is a geometric Brownian motion it certainly doesn't know the volatility doesn't know the black Scholes Merton formula it doesn't know the payoff function it doesn't know any of the option Greeks okay it has to infer the rows things are relevant but the point is that it will it will learn precisely what it needs to and nothing else from these very it'll in other words it'll learn the value function at which point the variables that we would use to try to calculate the value function right the variables that Merton used to calculate the value function you know it if it has the value function it doesn't need to it would no longer need those variables anymore so this is this is one particular simulation run in the frictionless case okay so should just explain here so this is one particular simulation of a geometric Brownian motion there is okay so there's the the optimal delta hedge so the theoretical just compute Delta with the black Scholes formula so that's in red and then there is the position taken by a trained agent trained by a large number of simulations that's the blue okay the red and the blue track each other pretty well the blue is kind of trading around the red but this is in a frictionless world so it's not paying cost for that and then we see that the the stock PL which is the green and the option PL which is the gold are mirror images of each other so it is it's hedging so that's good here's another one you know again stock PL and option PL mirror images of each other our agents position is in blue is tracking the black Scholes position in red pretty closely ok now let's introduce cost all right so in a world with frictions there will be a baseline agent which the baseline doesn't doesn't actually know about the frictions it just always trades to the trades to a delta-neutral position our reinforcement learning agent ought to sort of beat it because you know it has it will be trained knowing it will be trained on a simulation where the PL and the simulation includes the cost also it's not specifically told to try to fit a cost function it's just trained in a world that had the cost and so the question is basically would the the Delta hedge the black Scholes Delta hedger trade too much would it trade too often and perhaps you know and I mean the the the theory of decision-making under uncertainty is all about the trade-off between you know if you could if you could save a lot of costs but only only increase your risk a little bit you know that that might be a good trade depending on your risk aversion of course so right so we're using a reward signal that converges to the mean variance utility function so so we expected to make that trade-off we expected to realize lower cost than the Delta hedging baseline but possibly at the expense of higher variance right it won't be a perfect hedge so the P&L might have some vol but I mean I would always accept some vol for a large cost savings it's just a question of how much of each right so I ran this with 10,000 out-of-sample simulations and then we can run a horse race between the baseline agent that just uses Delta hedging and doesn't know about the cost but is charged it and the reinforcement learning agent that trades cost for vol she does a risk reward trade-off okay so first of all here is one representative path now of the the baseline agent ok so what's happening here is the baseline agent is always trading to a delta-neutral position so Delta the optimal Delta hedge in gold and the blue which is the agents position are just offset by 1 that's because just you know it observes the Delta and then trades and then has that position and an instant later so and look at the cost it's paying so the the cost P&L is in red that's going up rather quickly the total P&L is the bottom purple line which is going down just because of all the cost mostly okay here's another example where again the the baseline agent is is is trading a lot it's always trading to get delta-neutral even I mean even if the variance savings is not so much yeah the total P&L is quite negative because of all the cost it's paying to be fair these are run with a fairly illiquid market right so you know this is kind of a high trading cost function relative to to what I think might exist in reality but it to make a point okay now here is a simulation of a trained agent which was trained and and in an environment where it got to see costs I got to see the result of trading costs anyway got to see the negative results of trading too much and learned from experience okay so what we see here is the Delta hedge so the the black just the black Scholes hedge that would take you to Delta 0 which is the Gold Line is bouncing around quite a bit the blue line is our trained agents position right so the blue line tracks the Gold Line but it's much smoother so it's not bouncing around with every little bounce right and and and to some extent that's good because I mean the the Delta the the Delta of the position is you know subject to the same randomness that's driving the stock it's a Brownian motion after all so the Delta the Delta of the position is you know stock goes up the Delta has to change too right but a lot of those fluctuations are just random random noise you shouldn't necessarily hedge for each one but you should hedge if if the the risk reward trade-off is says it would be optimal to do so if it's not hedging would would lead to an unacceptable level of risk relative to the cost you would pay to remove it okay so here we see that the the agent trades a lot less it it maintains something like you know approximately Delta neutrality but just without all the churn we call it churn like trading based on just some sort of random noise in the model here's another another sample where again the blue you can see the blue the agent's position in blue is roughly tracking the gold but the gold is bouncing around quite a bit the blue is trading just enough to to sort of track it optimally keep keep the Delta in control but not trading in response to every little fluctuation I also wondered if that was true for across all the simulations and we can see it is so these are histograms of what happened and all the simulations the just doing the naive Delta hedging versus the dotted curves are the reinforcement learning method the trained agent all right so the the Delta agent at least you can say for it it has a very predictable ball right the realized vol of the the the delta agent just simple you know black-scholes Delta hedging it's a sharply peaked right so it's usually right around you know the value of the peak so it's it's predictable which I mean the fact that there's any any vall at all there is is primarily due to the discretization error right versus the on the left pot we really see the dramatic cost savings so I mean the again these are histograms generated from all the all the out-of-sample simulations 10,000 of them so in well it might not be true in every single one of the sample paths in every single simulation statistically it's true that that the total cost is much less for for the the trained agent while the the realized vol is is you know in the same range so so so so you know it's sort of roughly speaking it seems to be working you know there's a lot more a lot more tests would be needed and you know I'd be very interested to see for example you know I think if you talk to some real derivatives traders they might say well okay you know that that's that's that's great but you know the real way you might avoid trading too much for hedging purposes is you might try to get your book approximately gamma neutral as well at which point you know if it's gamma neutral then you know you're you're kind of a little bit more protected against the Delta changing you don't and you would also hedge less often and you know this there's no way to become to be gamma neutral with a single instrument but one thing I wanted to look at is is to look at this for simulations which have several different and allow the possibility of gamma neutrality that would be interesting you know if the agent is really finding the optimal value function as we think it is then it ought to discover these these gamma neutral trading strategies as cost reduction methods and maybe it'll discover something you know something we didn't think of okay so that's pretty much it I mean this is a summary paying careful attention to the actual nonlinear function approximation otherwise known as supervised learning that's used for the brain inside of the reinforcement learning paying attention to that is is is is encouraged it can help quite a bit and agents can learn to price and hedge derivatives and markets we're a perfect replication would be impossible or just too expensive and it can do this with a good simulator right so we're not we're not helping it by telling it what humans know right what humans know if things like option Greeks we're not helping it in that way we're not training it on you know real trading histories of good derivatives traders or anything like that so it's again it's it's kind of like alphago i all it needs is a good simulator so yeah you know and i mean should it really be a surprise that that value function based methods are incredibly useful in in derivatives trading problems no I mean the derivative a derivative price is a value function right so so the the partial differential equation it satisfies it can be derived by noting that it's a it's a value function so without elaborating too much on there on that point I I think it's kind of a natural you know intersection of these two fields reinforcement learning is a set of engineering techniques for getting to an optimal value function driven prices are value functions you know if something can can can learn too can learn a policy that that that maximizes a mean variance utility function well then if there is an arbitrage that's obviously going to be a pretty good mean variance utility function it'll find the arbitrage so it's it's connected to a no arbitrage result in the sense that well what if you had a stochastic system that in fact did not admit arbitrage well then what you expect is that this thing would train and train and train and run for and you know you know millions or billions of of simulations used to train it and it would still not find anything if there was something there it should find it so thank you very much it's an honor to be on this beautiful campus and thank you to Sasha for the invitation okay yeah so say to two questions and and then sadly I'll have to run yeah it's a good question I mean I so first of all I mean I think I think overfitting is primarily about like you know finding a good model finding a model that will work out-of-sample and that isn't only a problem in reinforcement learning it's a problem in everything so you know I think this takes over after that's been done so you know if you once you think you've found a good model it's not over fit then you can use that model to generate billions of training examples and you could then find the Belmond value function and the optimal policy for interacting with that model now if the if the overall if the model was wrong or over fit to begin with then this is going to be solving the wrong problem right well I never I mean this is all discrete time I don't know how to do reinforcement learning where you can trade infinitely often but in infinitesimal amounts I mean reinforcement learning is about sequential trade problems so sequentially choosing an action observing a reward choosing a new action I don't know how to do it with infinite you know oh these are short so maybe one more oh I don't think it would be very good at that I mean to be honest it you know for this to work it has to have already visited pretty much every region of state space you know and and then also had the chance to observe at least several actions for states at least you know in a mathematical sense close to the state you're talking about right so if a state is nothing like any of the states it's it's ever been able to observe previously I I don't have very much conference it would extrapolate you know I would handle I mean but I mean I'm I'm not sure that um Grand Chris would you know would handle a case particularly well I mean how would how would an optimal execution algorithm based on some based on a simpler method you know how would it handle some totally unseen scenario like you know the flash crash or something I mean I think that that's a problem I don't have much confidence in any model in those scenarios this one or any other 