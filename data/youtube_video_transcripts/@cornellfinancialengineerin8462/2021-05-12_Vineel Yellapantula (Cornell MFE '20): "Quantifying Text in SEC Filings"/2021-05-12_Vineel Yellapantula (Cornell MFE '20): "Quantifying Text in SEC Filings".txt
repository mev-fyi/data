yes uh thank you sasha uh hello everyone uh this is uh this is a presentation on my summer project uh that happened in around august 2020 so where i worked under my professor uh eileen aldrich so the project involves uh so the goal of the project uh involved uh trying to trade based on the text present in sec filings essentially the md and a section of sec filings so every day these companies publish uh up to four uh reports they're required to publish four reports uh 110k and three thank you filings and they all have this mdna section like everybody looks at uh the fundamental like the quantitative numbers they derive financial ratios from it but the text is largely ignored uh but because of these advent of new nlp techniques uh we're able to uh utilize the algorithms to find a sentiment of this text find a score around this text so the objective is to trade based on the text in the mdna section and why would we like to do that well because because time uh you know a human could read the mbna section and go make a prediction but you would need a lot of human arts to do that for all the stocks present in the us market so something that uh does this in a systematic way quickly uh would be very helpful now coming to the data what i worked on so i managed to uh have a clean data set of 430 stocks point in time uh and my time period involved from i wanted to include both the financial crashes so i started my time series and started my data set from 2007 january to 2020 august and uh the filing types were i was looking at were 10ks and thank you reports now my strategy was uh to somehow figure out a score for each of the report that's published by this stocks and analyze the performance of the stocks after floating them into five quantiles based on the score and try to create a long start strategy see how uh how they're performing in each of these confines so the fundamental problems now comes down to figuring out a score for these reports so there are a lot of ways to figure out a score for a particular report you could do a comparison based code like just find out how much is the change in length of the text related to the previous filing and how much is the similarity of the text with uh with respect to previous filing uh other than that you can also uh so these are like analogous to the uh traditional methods that nicholas was talking about and then you can use a sentiment based course where you have a certain defined lexicon defined dictionary where you find out what are the positive words what are the negative words neutral words and uh and the way you can do is use deep learning and try to figure out what's the sentiment of the paragraph itself so today i'll be looking at two of these methods i'll be going through two of these methods so from a project like mostly for text similarity what i did was once you quickly uh like vectorize the document i use the simple vfidf transformation and once you uh reprice the document uh you could find similarities called uh using uh these two uh methods called cosine similarity and jakarta similarity now cosine similarity is simply just the dot product of these vectors normalized uh so you get a score between zero and one for similarity after you find that similarity score uh you can split the stocks in your universe into quantiles and look at the performance of each quantile uh throughout history and i did that with cosine jakarta similarity and you can notice here that uh with cosine similarity i noticed that the performance that i'm able to achieve through these quantiles is very high but there's a lot of risk involved the the quantiles performances looks very risky and they're not able to distinguish between quantiles as well consistently throughout the time period whereas when i use jakarta similarity though i was able to figure out a consistent difference in quantiles and uh if you look at the stocks which are in the zero and the five bucket which are essentially the extreme quantiles where the similarity of the document is higher and and higher like more similar with the previous document and in the quantile where the document is least similar with the previous document you notice that the performance is a little bit higher which which which tells us something about the that there is some information with the how companies are changing their uh sec filing with respect to the previous filing and that there have been many researches going on with there have been many factors similar to that that have noticed and i've linked some of those papers in uh previously through lazy prizes uh now now this is though this is very simple this is this this involves traditional methods but what about deep learning uh so we can use rnns essentially to find the sentiment of the text so if you have something like uh that was a great movie as input you essentially just vectorize the text you just give them tokens initially but then this is a very important step in nlp where you figure out the embeddings of the words uh to try to which you can further use into any of the deep learning models that you wish to do so you figure out the embeddings you essentially uh reducing the high dimensional space of all the number of unique words in your document to something that is that like each word can be represented in a let's say a 32 length vector instead of a 500 length vector uh using if you use one-half encoding essentially you're you'll get a like number of unique words length vector but instead of that you can use embedding reduce the dimensional space and then input and put them as as a sequence uh similar to the architecture that nicholas showed for rns use using bi-directional models and then you can input a dense layer after that to convert the uh basically using an activation function like sigmoid to convert it into a to get a result of zero or one essentially like logical regression so i just prepared a notebook where i would like to i would like to take you guys through uh it's essentially just like a starter code for somebody who's starting out with rnns and would like to apply it on text data so so what i'm doing here is i'm going through imdb reviews and i'm trying to see uh i'm trying to see what's the sentiment if i if we can flat classify these uh the reviews uh properly uh based on the labels so we have this data set available in keras called imdb uh they have the they have cleaned and tokenized uh the words and uh it's uh i do recommend to check it out so here i'm just loading uh my data set i'm making sure that i'm only i'll only be looking at the most 5000 used words in the reviews everything else is going to be just uh let's say just masked over we're not going to look at those words but you can include all the words that are present in the dictionary for you for your problem once you load the data set like each review is stoked nice like this like this is an example of one review and uh you have a mapping available uh where each word is given a separate number so if i were to decode this review it would come down to something like this so this film was just brilliant casting locations seem really something of that sorts so and we have a label uh in our train and test dataset so we can we can see if our model can predict well so what you would do is uh essentially most of the reviews were between uh 0 to 2500 words but most of them were between 00 to 500 words and an important step in neural nets is you would like the each of the sequences to have the same length so we'll i'm trying to stick to 500 lengths 500 words essentially so any review longer than 500 words is limited to 500 and every any review less than 500 words is padded with zeros to make sure that it is also finer of 500 length so once we process the text and we uh we build an embedding layer uh an lstm layer and a dense layer sigmoid layer to classify the uh to get a classification to get a zero or one essentially uh i this is this will be the architecture of rnn this is the core of our rna essentially and uh we compile the model we uh specify a loss metric that we're looking into which satisfy a scoring metric that we're looking into we're essentially looking at accuracy here and i trained uh the uh rnn over our 25 000 training samples and uh tested them over at 20 again another 25 000 testing samples and uh we were i managed to achieve a like this is a very basic rna and i managed to achieve an accuracy of around 87.5 uh you can do you can use this similar setup for any of the text uh data sets that you have so here i created some examples just to see if if i throw it some new uh text how it's going to perform so the like look at these three reviews like the movie is all right we'll definitely watch it again uh utterly boring character development needs some work and review 3 this was a great movie similar to logistic regression so if you look at here the review one and three are classified well like into the one category which means they are positive reviews which which ranks true and the second review is not given a great score because uh their view is bad so it did manage to learn english language essentially and uh this can be a good starter code for somebody who would like to apply uh rnns to text data yeah and also it would be very interesting to apply tcns as well now that we know of their power um in conclusion to my talk i would like to just mention a few points let me go back to my presentation so in conclusion i would just like to say that there's a lot of information available for text data uh through these 10k centenqueues you can develop a lot of factors uh and especially the factors which utilize the previous document as well instead of just focusing on present document do work well and apart from that if you're looking into alternatives other alternatives like using deep learning there are many kinds of embeddings available so that is some of the challenge when it comes to nlp there's glove there's water dock there's bird there are rnns so there's a there are a lot of choices that you have similar to the leaderboard problems and most of them are said to perform well so you you need to learn and figure out which one to use for your problem and apart from that this again when it comes to more information you can use more 8k filings or new cycles uh to incorporate more information and more data to have more data to essentially train your network overs you would require them so that that can be some idea of future future work for somebody who wants to get into uh nlp and text mining so i hope this was helpful for you guys uh thank you um i would like to take any questions to you guys thanks for neil yeah we have a few uh a few new questions uh regarding your flash talk um the first one is were the 430 stocks picked at random and i guess just as a follow-up uh because the performance looks uh well in in this since 2007 apparently the s p 500 returned 300 percent while this model does a lot more so is there some kind of uh maybe selection bias or so there i i would say that there is some selection bias because i i only picked stocks where which were present in the index from 2007 to 2020. so essentially they were already well performing stocks over time uh my only goal was to figure out if i can split them into buckets and notice some difference between the long and short buckets uh so yeah there's definitely some selection bias and there's also uh i've also had to leave a lot of stocks essentially because uh they have not published their uh 10 case intentions properly or i did not extract the mdna section because a lot of because due to the regex functions and due to the different formatting that these companies follow it might also be the case that like okay the companies which don't follow that best practices were maybe bad so it that that way i was able to eliminate all the uh you know uh not well functioning companies essentially so that might also be one of the reasons yeah okay um we have another question for you can you um elaborate on the numbers did you use any random simulation in uh in your procedure randomness uh when it comes to relevance no i don't i don't think there's any randomness when it comes to the snow okay um so and another question which data vendor was behind the 10 q and 10 or are they real time or lagged the that that is the soul of the project essentially i uh web scraped these 10ks and thank yous throughout the edgar website so that is the most that is like half half of the work that went into the project so yeah that is my own uh work i provided the github link where if you if you give a cik essentially the uh cik related to any of the stock you can you can you'll be uh you can download a data frame with the with the filing type and the uh mdna section of it and if you are able to like uh uh change the code a little bit maybe you can uh you can you can uh extract other sections as well so do look into that yeah okay great and and i have a uh maybe a final question is uh so you when you we're looking at these uh these sort of uh five deci five quintiles and comparing them is what is the performance metric are we what do we consider uh you know for example i noticed that the ones that do better uh also are more volatile so in have you have you thought of you know uh ways of uh scoring these or uh okay like where i was so essentially to see if one factor is better than the other uh you want uh we want to look at like the performance of each of these quantiles so one scoring metric i initially looked at as information coefficient so essentially the correlation of the factors with the future returns and how consistent are they over time because even even if the total performs like at the fine if the end of the day quantile one performed better and quantile five performed did not perform well but is that is that the case consistently over the years that is something i have looked at to ensure that this is a this is a factor that you can invest in going forward so just the information correlation is an important factor i think when you're looking at if a factor is working or not uh i've done like i would like these this is the same thing that i would do with a standard factor like momentum or value any of these as well information correlation i think is a very important factor yeah yeah so i thought it was very interesting how uh if a management uh you know text section is exactly the same as the previous which is like a very similar uh text it could mean two things either this company is so stable and so healthy that it just has good news all the time or it could be that the manager is just doing a cut and paste job and and and the similarity is being detected and really nobody's doing any work in this company so so i find that and interestingly that's that's how the one of or this quintile was the one that performed the best but also had the highest volatility um so i i thought there was something interesting there and maybe something promising yes uh yeah absolutely uh like that was an idea i got through the lazy prizes paper but yes uh people have noticed that companies who are much more stable in their reporting uh uh uh reporting uh needs like which are like how publisher reports super like periodically perfectly and have clearly defined sections like even things like this uh the pump leaves which i stick through them uh do perform really well so that's probably a good factor to explore yeah okay well uh thank you both of you we've gone a little bit over time but uh thanks uh you know are i think we had a very interested crowd and and um so i wish you all a great summer and i look forward to seeing you again uh in the fall um and um and uh thank you for this great presentation 