I'm really happy to introduce Kevin Webster as our speaker today for this uh cfam UBS financial data science AI machine learning some I think it's take all these words and jump of them up together that's what we're doing um so anyways I'm very excited to have Kevin Webster it's actually the second time he's a speaker at this seminar this year but first time in person so welcome to to our little campus here um so Kevin is currently on a garden leave um from Citadel so he has two years of non-competes in which he gets to sort of uh you know wake up in the morning think of uh wonderful interesting problems to work on and then goes about and and and does it and so this has been a very uh creative year for him and uh one of the the topics so he's an expert in price impact and um you know one of the topics that he's been interested in is a b testing algorithms which I find very exciting and he has these uh this he has sort of rediscovered causal regularization and and has these uh really interesting insights to give to us so uh please welcome Kevin and I look forward to this great talk hi can you hear me perfect so it's great to be back in person thank you Sasha for um for the kind invitation and thank you Roselle for organizing all this um so my name is Kevin Webster and I'll be talking about live trading experiments and uh causal machine learning this is uh based on joint work with my uh co-author Nick westray and you can find the paper on ssrm um to start off I'd like to describe a real life scenario nope sorry okay um so the scenario I'm going to focus on is the case where a fund a mutual fund a hedge fund trades not directly in the market but through a broker um they could be trading with a single broker or with multiple Brokers and so when they do that they essentially Outsource their trade execution to another firm and the broker in question then trades on their behalf and such a broker has uh two mandates First Trade on the best um in the best interest of the client so this is typically called best execution and is a regulatory requirement for example in Europe called method two um the second thing is the broker needs to prove that they acted in the best interest of the client and they typically do that with something called transaction cost analysis and a key component of transaction cost analysis is measuring and orders price impact how much it cost price moves however there is a significant difficulty for the broker their clients don't trade randomly they don't send nice IID orders um and they don't trade uh buys or sells without a reason typically clients trade because they want to make money and they make money by predicting where the price is going to be so in traderlingual clients trade based our Alpha signals and they will buy when they think the price goes up and they will sell when the price goes down what this means is that when you observe a return during a trade it can be uh it's going to be a mix of two things the price move caused by the trade price impact and the price move predicted by the trades also to quote Daniel Narron large-scale trading will often occur in the presence of Market drift Alpha and the realized execution cost is a combination of Alpha and the price impact foreign these two terms a little bit more precisely so Alpha describes exogenous price moves price moves that would have happened regardless of your trading whether you trade or not the price would have moved these are the price moves that you want to predict um how could you build such Alpha signals easychenko gives a couple of examples of data sets that you can use to estimate such Alpha signals you could use Material Company news M A's earning surprises index balancing or dividends and splits um what all these things have in common is they try to find the financial driver of the stock and they're not trying to estimate how much your trading will affect the stock price that's the responsibility of the pricing back now the other component of returns so price impact is a causal model for trading it describes price moves that are caused by your trading SO trading of stock cost price moves for the stock that otherwise would not have happened this is in contrast to Alpha which predicts price moves that would happen even if you didn't drink stock um and I'm repeating myself quite Vivid but this distinction is really key um how do Traders use pricing back Knolls typically to answer what-if scenarios to answer questions such as what if I had traded faster what if I traded slower what if I had submitted a larger order but if I had submitted a smaller order um these are questions that you cannot answer with an alpha model you need to be able to simulate how prices would react to your trades so here I give a very simple simulation I start off with a historical price and which includes my Baseline strategy and if I change my strategy to trade faster or if I change my strategy to trade slower I observe a different price and that gap between the historical price I observed and the price that I observed if I trade fast or if I trade slower that is the price impact of that decision my decision caused this Gap now why does this distinction matter why did I say this is very important and hence why I'm repeating myself a couple of times that is a little unfortunate it does not seem to work yeah but it gives me an hour okay I'll just move it down there and so I'll quote Daniel n Aaron again if we find that most of the cost is due to price drift the best option would be to accelerate the trading and pay more impact to capture more attractive prices conversely if the cost is mostly driven by impact then would behoove us to slow down our trading to minimize the impact so even though an alpha signal the price impact null might give the same prediction the price is going to go up the action that you take as a Trader is diametrically opposed if you think you are causing the price move you should be trading slower if you think the price move is going to happen regardless of whether you trade or not you need to get ahead of that and so this is why Traders care so much about Alpha versus price impact um and this leads to uh what we show and co-authors call prediction bias which is basically just a fancy way to say that this is hard um it's hard to distinguish whether you caused a price move or whether you predicted it um so if you cause it that's pricing back but if you anticipate the price movement to quote they follow-up paperback the impact of a meta order can be affected by several artifacts and biases one of the recurrent criticism is that meta orders are not exogenous they're not submitted randomly in IID fashion and possibly condition on trading signals you send orders because you have a signal otherwise why are you in the trading business um cfm's proprietary data allows one to eliminate many of these biases since the strength of the trading signal is known and can be factored in the regression CFM is capital fund management the hedge fund that we show co-founded and what he describes here is a best practice at CFM which is if you know the reason why you traded if you know the trading signal that cause a trade they can just put that in your regression and that fixes the prediction bias um there's another paper by three different authors where they um also observe prediction bias and resolve it with the similar technique um since skilled Traders correctly predict short-run future Returns the cost of their trades appear high as a result measured execution costs may not be an unbiased estimate of the true cost of trading so if you have a proprietary data set that gives you the alpha of a trade you can fix prediction bias um so if you are funded if you have access to all these variables you can do this regression so on the left hand side I have returns let's give my I'm giving myself a parametric price impact model I'm just going to assume it's some function of my trade for example square root of the order size and I have some alphamal it could be a factor Model S may be a linear regression or it could be some fancy machine learning model um I'm completely agnostic all I care about is that you have recorded the value of your Alpha if you have this and if You observe the alpha then there's a reasonably simple method to fix prediction bias you just subtract the Alpha from the return and you fit your impact null against alpha-adjusted returns and there's no bias that's great um this sounds easy but it's not completely trivial because uh you need to be able to store and reproduce your historical Alpha and have there's a technology component to this but mathematically this is very straightforward however the situation for the broker is very different why because they don't observe the alpha to quote bassador Traders had funds I understandably reluctant to pass the alphas over to a broker due to the potential for lost intellectual property front running Etc so basically the client won't give you the alpha what do you do um at face value you can't do very much because if you're not given the alpha and you do this kind of regression without ignoring the alpha you'll get an overestimate because the trade is correlated with the alpha I run this linear regression and the true model is actually this my Lambda will be fundamentally biased upwards so that's a bit unfortunate and most of the stock is about finding a way around this issue this prediction buys for brokers so why why is this a problem because if you overestimate your price impact you will ask your clients to slow down they will lose out on Alpha that they could have captured conversely actually Alpha researchers face the option problem if you're an alpha researcher you have the alpha signal but you might not have a good price impact model you might be not collecting your own trading data and so if you don't use a price impact mall if you just fit your Alpha ignoring price impact uh you'll have the converse button which is you'll overestimate your Alpha you'll submit too large of an order you'll pay too much impact too much trading costs and you will also lose out on some profits that you could have so this is a problem both ways if Brokers don't know the alpha they will trade too slowly for the client and if the client doesn't know about their price impact for example their broker doesn't give them reliable trading data then they'll overestimate the alpha and submit orders that are too large so the problem is bigger on the broker side but it is a two-way street um that's essentially what my talk is going to try to solve um prediction bias and it's a core problem in trading so the rest of the talk the lack of our technical part of the talk um is going to formalize uh prediction bias it's going to prove uh mathematically that there is a statistical problem here to be solved uh it's going to introduce randomized trading as one solution to the problem so if I submit random trades they have no Alpha on this data set I can use traditional econometric methods to estimate price impact in a bias-free way however you random trades are expensive I'm literally sending random trades just for the purpose of measuring something and I'm paying trading costs to do so and so what we want to do is do a little bit better than traditional econometrics to get more value out of our random trades so we'll use a method that comes from the technology industry specifically Amazon came up with a method called cosm regularization and what it will do is it'll use both randomized trading data and historical data and I'll combine it in a intelligent way to get a little bit to get a better performance than traditional econometrics and finally I'm going to use a simulation to quantify to basically do a horse race between these methods um so before I go into all that then you give a sneak peek at the conclusions that people know what we're working our way towards so at the end of the talks I'll give myself a simulation estimated trading environment in which I will compare three estimators for Price impact the first estimator is the naive estimator where I just ignore the fact that there's Alpha that's going to give us that first row here where I have a bias of 30 for realistic uh simulation parameters so if I ignore the alpha I just overestimate my impact null by 30 I'm very confident in my in my model uh falsely so but I have a t stat of 30. the second method is the traditional econometric method which is to randomize some trades and measure price impact only on the random trades um if you do that you will get a bias fee estimator this is this row here and this row here by definition the random traits don't have Alpha there's no prediction bias I have no bias however um because these experiments are costly they will be small and sample size and therefore the confidence intervals will be large so the t stats will be small and then I'll propose my last method which will combine both types of data the bias data and the unbiased data and what you'll see is they'll outperform traditional econometric methods using five times as little data it will get a higher key stat now explain where the method comes from um and so this will have applications in trading so the main application I'm going to talk about is estimating price impact when Alpha is unknown this is the broker's problem also uh solve the converse problem the alpha researchers problem which is estimating Alpha when impact is unknown when I don't have reliable trading data I won't solve problem C but the method would also apply to other cases of a b testing and trading for example the typical exploration versus exploitation problem of evaluating trading algorithms I have an algorithm a I have an algorithm B how do I determine that a is better than B I randomly choose one algorithm of the other I compare their performance and then I allocate more trades to the better algorithm I will not cover this application but the method would also apply to that um so to do that I need to talk causal graphs and I'll give a quick bibliography there or quicker history um so causal inference is a mathematical Theory it was developed in statistics science departments in the 90s in 2004 one of the main authors uh concluded fortunately the days of Statistics can only tell us about associations an association is not causation seem to be permanently over this ability to distinguish between correlation and causation is why it has become a focus in the machine Learning Community the touring Award winner for deep learning identified causal machine learning the intersection of causal models and machine learning as the next big step in AI but even Beyond Academia causal inference and cause machine learning is already actively used in the tech industry Microsoft made it one of its seven core Science tracks emphasizing the Practical impact at the intersection of Academia and Industry and contributions at Microsoft but also other research communities Industrial Research communities so they had the speakers from Uber Lyft Netflix um and a couple of other places uh so Netflix also had a summit dedicated to causal machine learning and um they also emphasize the fact that this is not just something that their research team works on but their business as well um a last example is uh Uber who uh published an article where they describe their a b testing architecture and their causal inference system and uh it's a semi-technical uh publication but there is a sentence here which I felt was quite important from a business perspective so in it they describe one of the objectives of the system for their clients and for their business partners for their stakeholders the results are universally trusted allowing teams to align on the ground truth quickly and act on it without endlessly reinvestigating surprising findings rerunning faulty experiments and second guessing decisions this these three goals not reinvestigating surprising findings not be running experiments not second guessing decisions is what every Trader wants out of TCA out of transaction cash analysis um so all this to say that Finance has allowed behind the curve on this topic um and yeah so for my uh prediction bias problems I'm going to give myself a very simple causal graph what is a causal graph in your probability class you've probably learned about Omega FP the probability space um so in causal inference you add a fourth element to a probability space which is a causal graph and a causal graph is just nodes and links that each node corresponds to a random variable and each link points from a cause to an effect in my case I only have three variables I'm interested in the alpha of the trade the size of the trade and the returns during the trade I'm going to propose this causal graph for my study I'm going to assume that the underlying features of my Alpha models Drive different fundamental price moves on the stock so this supply chain data that we were talking about before this m a data that we were talking before those are the fundamental drivers of the stock so Alpha causes returns I'm also going to assume and this is an easy assumption to make that my trading algorithm is going to react to Alpha signals I can make that assumption because I own the trading algorithm I know that it reacts to the alpha signal otherwise there's a bug in my code so Alpha is going to cause trades or changes in trades training reacts to Alpha signals finally I'm going to assume that trades cause price moves that's why I called price impact the reaction of the market to my trades so I'm going to add this strap and this is an assumption I do not prove that this graph is correct I'm assuming that the scrap is correct so you have to believe me that Alpha causes um price moves and the supply chain that isn't the reaction to the stock market the stock market is a reaction to the supply chain and that trades cause price moves and that you trade because of an alpha signal so you will have to take this for granted um yeah under that assumption does this graph is correct you can prove using new calculus that for any probability measure that's consistent with the graph this inequality holds now if you haven't seen do calculus what does that mean in words it means that no matter what fancy regression technique you use you're not going to be able to estimate price impact without observing Alpha this may sound like a completely intuitive statement or not I don't know but the fact that you can prove this not just for the linear regression model but for any model is quite a powerful statement of our inability to do anything right now um so you cannot estimate price impact if you don't observe Alpha um so that leads us to randomized trading expense if I can estimate a price impact because of alpha I'm just going to randomize it away so this is already actively used in the finance industry um here are two practitioner examples so Deutsche Bank in their TCA research emphasizes it is important to acknowledge that when we run these analyzes it's done only for a subset of orders that are affected by controlled Ada experiment with routine changes so we can really quantify their impact I highlighted three sets of words um they're estimating impact of trading they use a randomized a b experiment and they only do it for a subset of orders why substantive orders because these things are expensive you're not going to use it use randomization on all of your trades crucial also outlines the randomized trading experiment we have actually shown that the short-term impact of cfm's Trades are indistinguishable from the trades of the rest of the market or for that matter from purely random trades that were studied at CFM during a specifically designed experimental campaign in 2010-2011 in a companion paper they provide more details on their experiment and the kinds of conclusions they can get from it so at this point I hope I've convinced you that this is a reasonable thing to do so now I'm going to go back to some high level math nothing particularly fancy but I'm going to introduce some terminology from this puzzle entrance at literature so um we defined this initial causal graph here and we said this is this represents our data and what the causal inference Community calls us um is observational data or what Traders would call business as usual so unless I do something to change my system I'm going to generate data according to this graph so the nice thing about obsessional data is I have a lot of it this is by definition business as usual I have probably years worth of data um that I generate every day that's fundamentally biased but I have a lot of it when I randomize I create data according to this graph here where I've cut the link from alpha to Q my Q is now randomized instead of depending on Alpha so the causal inference Community calls this Interventional data or randomized interventions so my thing about international data is it's unbiased it's not affected by this horrible theorem here that says that we can't do anything on this data we can do something but the data is small because it's costly I think of a concrete examples that we have some numbers that reflect uh this are similar to trades um so assume that you're a perform manager trading the Russell 3000 on the given day you might trade one third of your names so you might trade a thousand uh you might submit a thousand trades so if you trade over a year you have 250 trading days in a year you end up with a quarter million trades if you've been trading for the past 10 years you have 2.5 million trades that's a lot of data you can fit a a good non-parametric model on that amount of data on the flip side I might submit a randomized trading experiment unfortunately that training experiment is going to be limited in size and duration why is it limit in size um it could be for example three percent of your orders it's limited in size because if you start randomly trading 100 of your orders you will get fired um why is it limit in duration because you probably didn't do it historically typically you do live experiments after asking a question somebody asks a difficult question he suddenly decide that you want to submit a random training engineering and because somebody just asked you a question you probably want the answer reasonably soon you can't say I'll see you in 10 years I'll have the answer so every reasonable set of Founders is to say that you're going to randomize you're going to set aside three percent of your trades that you're going to trade randomly and you're going to give an answer in two months under these parameters you get about a thousand trades a thousand random trades unfortunately people might ask you more than one question at the same time it might not give you more budget to randomize so if I have five questions I want to answer I end up with an even smaller data set 250 trades for experiment this is just to highlight that for somewhat reasonable parameters I can get a very large observational data set in a very small Interventional data set and that's the regime you operate in when you trade you will not have the luxury of a lot of a b tests and because of that you want to design experiments prior to deploying because if you make a mistake it's expensive um so this is where you want to use a simulator to answer before you start submitting random trades how much is it going to cost you yes seriously it's been focusing on the number of Trades instead of like the size of the trades um mostly because it's easier to communicate and essentially assuming that my trades all of equal size yes some trades are going to be more meaningful we could have a waiting scheme I could give all these numbers in some kind of weighted way and we could be taught yep sorry I should have repeated the question why do I talk about number of Trades rather than trade size this is a simplifying assumption essentially um yeah so before I submit a lot of random trades that will cost me money I will probably be asked how much is it going to cost and what kind of confidence interval can you expect and so this is where we use uh the order simulator of header common request tray to essentially answer that question before submitting any live trading expense we essentially simulate the live training experiment to get a sense of its cost and to get a sense of what kind of confidence interval I can get um so this kind of uh simulation is key not exactly a new result in statistics statistics tends to rely on summation results to um estimate the estimate confidence intervals um so what we do in the paper is we simulate the exact example I described before I have to give myself the Russell 3000 I have 2.5 million trades with Alpha that's going to be my observational data my bias data and I'm going to give myself three different uh designs essentially are three different sizes of experiments tiny expect or small experiments realistic experiments and slightly unrealistically large experiments and so these experiments are going to be without Alpha and um I'm going to give myself a ground truth so I'm going to in my simulation know the true impact now 21 of these three now I wanted to ask the question of whether I can recover that impact model and with what confidence I can recover it um yeah so if I ignore the alpha then I don't recover it I overestimated for a correlation of five percent between now for my returns the bias is 30 the bias is going to essentially be a direct function of that correlation the higher the correlation the larger the bias could use this if you have no Alpha then you have an unbiased estimate of your back but you will be in trouble um so in conclusion if you ignore Alpha if you just use your observational data you'll have high bias and low variants now if you use traditional econometrics you will only use random trades you will throw away all of the organizational data 10 years worth of History doesn't matter you'll only use your two months of uh experimental data and you'll get a bias fee estimator by definition we operate under the graph where there is no negative result but because the sample size is pretty small you'll get a traditional t-stat which is unfortunately small and so in conclusion for realistic experiment sizes you'll end up with a low bias but High variance estimator and already at this point this is interesting Traders care about these kinds of numbers on their own but we'd like to contribute a little bit more than just the simulation of the status quo so we're going to introduce a new method that's going to outperform both of these causal regulations so what is causal regulation as I said before it's a method that came from Amazon so the paper is Junction 2019 and there's a companion paper and um if you're familiar with cross-validation it's a very similar technique you use one data set as your training data and a different data set for your testing data and the idea is to use your bias data as your training data and you only use the unbiased data as a testing data to tune your meta parameter why do you want to do that why don't you just directly use cross-validation on your unbiased data why do you need the biased data the answer is because the international data is so small you're just never going to have enough data to do cross validation you want to use the fact of your observational data is very big and free if the observation observational data is small it's also useless like why even bothering you're going to have a trade-off between injecting more data but also injecting more bias so that's essentially the the key Insight of the paper this is the algorithm at an extremely high level so you assume that you have a large organizational data a small Interventional data you have some model with some parameter you want to fit and you have a meta parameter that you use to shrink them all to regularize them all and your three steps are train the data on the bias data set for Lambda equals zero this is going to give you an overestimate of data and it's going to be the wrong estimate but you're going to tune the the regularization parent or the shrinkage factor to get the correct estimate and you're going to tune it on the unbiased data so for example you could maximize the r squared on the testing data or any other relevant metric and that's going to be the model that you use for causal prediction um and yeah that's the algorithm it's very simple and if you have intuition about cross-validation it carries over to this case um so let me uh show some results so the value here really comes from not throwing away your 10 years worth of History it is biased you need to correct for it you can't just use it directly but this method gets the best of both worlds it gets a low bias low variant estimator you can see that even for a small experiment you get a higher T stat then a medium-sized experiment when you throw away the data and the fact that it's biased free is because you have this regularization this meta parental calibration that's what guarantees that you get a good estimator um why does it work mathematically I'm not going to give you the proof it's not even in our paper we just refer to the yantic paper for the path Michael proof but intuitively if you are familiar with the proof or standard regularization if you've read the papers on lasso on Cross validation on Range regression it's the same proof you just modify slightly the assumptions you do some pattern matching to figure out which parts of the proof carryover and which parts need to be slightly modified but the core idea is the same um in cross-validation you basically fit a bias small in Sample bias in this case um what they call finite data bias and then you use the cost validation to shrink it and remove the bias here you fit the model in Sample there's a bison sample you use the outer sample to remove the bias exactly the same proof so this is the summary slide um I've spoiled it earlier but basically you need five times the little data to get the same kind of confidence and this is very helpful for this price impact estimation problem that I was talking about now I want to give a second application just to show that this is not a kind of one trick pony um so I want to look at the converse problem the problem of alpha researcher at the hedge fund of mutual fund who wants to estimate uh the accuracy of the alpha model and the difficulty is that if they trade using one or multiple brokers they don't have as reliable trading data as if they collected the trade they have themselves um so in the extreme case where they have no trust in the trade data where they're not even given the trading data you you would have to estimate Alpha without knowing the impact um so if you don't know the impact at all then you have the exact same performance before you regress Delta p g and Alpha but because the true model actually has impact you will overestimate Alpha and this is what I the situation I gave in the introduction so we're going to solve this problem I'm going to first start by acknowledging that there is a solution if you do know the trading if you do have reliable trade data so this is this paper by Wellbrook which I strongly recommend so if you do know the impact of your trades you just subtract the impact from the returns and you fit the alcohol against impact addressing returns and that's going to give you an unbiased estimate of the album so this is what I call the trusting case where you have a very good relationship with your broker and you trust their data and there is no translation error whatsoever in practice you might have multiple Brokers they might give you data that is not comparable between Brokers maybe somebody bins it in seconds the other one gives it you and nanoseconds they have different conventions it's a mess you don't know how to use the data they give you impact knowledge that give completely different answers and you're stuck so what can you do um in the spirit of randomized trading um I can do something to get some form of unbiased data so in this case what I'm going to do is randomly not submit trades if I have an alpha signal I choose not to trade on it then I lose money opportunity cost I could have made money I didn't trade I lose money that's bad the good news is on that not submit a trade on that fictitious trade that I didn't submit there's no impact I didn't trade so I can do this very naive regression on that data set of not submitted trades randomly not submitted trades can I get my alcohol and this is great because it's model free so um one of the advantages so the First Advantage is I don't need trading data in this case I don't even have a broker for this trade it's a non-submitted trade so that's great um the second uh great thing about this method is it's model free I don't need to specify a pair a parametrical form for my present back model I don't need a pricing technology at all so that's great there's a huge downside though um if you want any kind of confidence interval on your Alpha you're going to have to not submit a reasonably large fraction of your trades for example 20 30 of your profit you go to your boss and say I want to not make I want to forego 20 to 30 percent of the profits to make sure that I have the right Alpha signal you'll again get into trouble so what you would like to do is do this but with less data and so that's kind of the title of the talk um you can use this machine learning method to get um the same result with less data by not throwing away all of your observational data so in this case you you fit your Alpha mode on biased data on trades that you actually submitted that are biased by a price impact and then you just fix the bias by looking at that those handful of Trades that you didn't trade and um that's pretty much it um in terms of General takeaway of the talk uh causal machine learning is is great um it's already well established in the tech industry but I think it's very applicable to training applications why is it applicable I just gave you one example of a bias prediction bias there are many many others when you trade on dark pool there is a bias there's a long list of trade biases um that you could use these methods for this is why it's applicable to transaction cost analysis price impact and Alpha research and fundamentally the reason why these methods outperform traditional econometrics is because we're in this very specific data regime in trading you have a lot of trading data it is biased but you have a lot of it you have a deep history because you've been trading for a long time and trading experiments are expensive you don't get them for free so you are in the perfect data regime where these methods are likely to outperform traditional econometrics um this is my talk thank you and welcome to questions thank you foreign truth so um the answers without a b testing yeah no you there is a fundamental uncertainty here but if you have a b testing mode you have a perfect reproducibility of why you traded which is the CFM example there is a ground truth to be found you can only find it statistically you can't find it on trade by trade basis uh so on a trade by trade you just observe a return but statistically you can say there is a ground truth and there is a theoretical model to be to be found but again this is because of of the very initial definition um so I can either look at the graph definition where I can look at my with words definition I've defined price impact as the price move caused by trading so this definition is not the eye of the holder this is my definition then if you have randomized training experiments you can attach a statistical number to that definition so there is no ambiguity there's a difficulty but there's no ambiguity but without something to combat prediction bias yesterday then it becomes uh as you said the eye of the beholder so to give you a concrete example A lot of people myself include Fit price impact miles on the public trading take on just NASDAQ data into data and to some degree you're almost assuming that there is no off in the trades like you get average away across all the players but that's an assumption that you make and you um you don't really have a way to to confirm the Assumption if that makes sense also likes him a lot of his papers to distinguish between long-term price impact which he interprets as Alpha and short-term pricing back basically in that world he assumes that people don't have short-term price uh short-term Alpha and conversely that price impact must Decay to zero so if there's something left at the end some permanent impact it must be Alpha again that's an assumption so to answer your question there are scenarios where I would say it is in the eye of the holder but there is also a rigorous definition that's not philosophical in nature and that you can ask mathematical questions and attach statistical estimates but it is a hard question which is what drove Nick and I to to write this paper can we can we actually prove things in fact on public data versus private data are two very different animals yes um and also there's a distinction between being observational data and Interventional data part of the problem is that publicly is by definition purely observational um and then the other thing is you just don't have access to the alpha um so if you have access to the alpha which you have sometimes with proprietary data or if you have access to randomized trading data which also tends to be proprietary in nature then you can do something otherwise your always measuring a mix of the two and you just waving your hands as to um why you measured the right one of the two if that makes sense to be fair though I think that the assumption that there is very little Alpha the public trading tip is quite accurate yeah or at least it's a working assumption okay should you say it is right I kept really CFM I only did a test in 2010 11. it seems like yeah so I I don't know because that's just the one that I decide to publish therefore I know they do it continuously this is just the one that they want to publish in academic paper um I do know that a lot of banks advertise the fact that they do a b testing of some sort um this is just one example I could give you other Brokers that do that um so is a b testing or randomized trading a thing yes how widespread and who does that I can't answer that question I don't have examples in the literature and they're interesting and those are the ones that we basically emulate you know assimilation we just literally take those papers look at what they do and we say can we simulate that and what kind of estimators do we get does that answer your question there's a question in the back so it is it is a common transformation I'd say transforming is a oh sorry let me first uh repeat the question um so the question is is this relationship transfer learning because there seems to be some kind of analogy where you happen to do we train them all on one data set but you really would like it to work on the other data set um so there is a little bit of a causal interpretation here that's missing in transfer learning but the it's the reason the proof Works transfer learning works because cross-validation works and just as the cross validation proof carries over to a transfer learning proof it carries over to this and so you could view this as a special case of transfer learning I do think the causal interpretation though is quite novel even if the mathematics is not um but yes you could view this as just transfer learning I'm learning on the wrong distribution the biased one and I'm but I'm hoping that the bias data still has some information that I can learn from thank you 