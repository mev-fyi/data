all right everybody Welcome to the let's see if I can remember the name of our seminar yes CFM data and analytics um uh seminar series uh basically you know Quant TV let's call it and uh we're delighted today to have Harrison Walden who is from UT Austin and in fact he's uh we share the same uh thesis advisors so um I'm welcoming my little academic brother here so uh so give him a good cheer and I'm really happy about uh to be hearing about this research because we have talked about it a few years ago and Harrison was interested in reinforcement learning and algorithmic trading and uh and it looks like he has a really interesting talk for us uh regarding collusion of algorithms and uh well without further Ado Harrison uh the floor is yours and if if anybody has any questions please put them in the Q a and I'll be sort of um asking them as we go along so thanks for joining us Harrison awesome um thank you so much uh Sasha for the invitation to speak today's seminar um yeah so today I'll be speaking about some recent work with um Alvaro cartea Patrick Chang and Jose penelva on tools for studying algorithmic collusion um so the contents of today's talk will be will primarily be drawn from two working papers um the first two that are listed here but for those interested I should also point you to another working paper that my co-authors of my co-authors which uses a lot of the techniques presented today um to study the impact of price discretization on collusion so I I can send that one as well um okay so um first uh just to speak broadly about uh the prevalence of algorithmic pricing in the financial industry so of course it's you know very well known to this audience um that the vast majority of Trades um in in electronic markets are executed um and driven by algorithms um so the particular forms of the algorithm used in practice are varied but many algorithms used in practice rely on machine learning techniques like deep learning or NLP and one particular class of algorithms which has seen an increased attention is that of reinforcement learning or algorithms that learn trading strategies on their own via trial and error so RL has seen practical success in deep hedging as in the paper of Hans Bueller and co-authors as well as in optimal execution I mean in Academia Aura has been applied to all corners of the algorithmic trading literature including trading Market making and option pricing um so as noted by the AFM so that the Dutch Financial regulator this year trading algorithms which rely on an end-to-end reinforcement learning are not so common in the financial industry um the the firms that they surveyed none of them used the end-to-end RL in their in their um in their in their in their algorithms however many firms indicated that they they want to incorporate RL based trading algorithms in the near future due to rl's well-known successes such as achieving superhuman performance in complex board games like go or fast-paced video games like uh DOTA and Starcraft um so while um while RL has seen empirical success in robotics and Engineering uh Financial Regulators have expressed concerns over the possibility of firms relying on entirely autonomous algorithmic trading um and algorithmic decision making so indeed Regulators around the world envisage uh the risk of independent algorithms enabling task collusion or inflated prices in the market achieved without uh without explicit communication so this concern is due in part to both empirical work which shows that the adoption of algorithmic pricing strategies is often correlated with an increase of prices which can't be explained uh by other economic factors um so this has been noted um kind of outside the financial industry like um this this paper by Assad and co-authors look at looks at the retail gas market gasoline Market um so there's also a wealth of experimental uh evidence which demonstrates that certain reinforcement learning algorithms in certain scenarios can learn complex collusive strategies so Calvano and co-authors listed here for instance shows that RL algorithms can learn to inflate prices in Bear Trend and Corno competition models uh while ramakant and his student show that deep reinforcement learning can learn to inflate spreads in a continuous time market-making Market making setup however so a limitation of the existing work on this subject and the the the work mentioned here is that um the the um study of the algorithms generally lacks a theoretical rigor so they they take a simulation approach um by just simulating the algorithms and seeing what they learn and while the simulations present in the papers mentioned here are are useful for illustrative purposes they're hardly enough to justify say new regulation on the subject because they're they're subject to to a lot of um kind of heuristics you know we don't know if the behavior exhibited by the algorithms are just artifacts of the particular experiments maybe the um the experimenter stopped the the algorithms too soon um we don't know if what they learned is is is expected um so along these same lines uh the AFM has expressed concern over RL explicitly due to it do it due to its lack of explainability so in particular RL algorithms are generally understood in very simple scenarios in which a single agent interacts with a fixed but stochastic environment so a Markov decision process markets however are multi-agent and non-stationary and increasingly driven by the decisions of ml algorithms but very little is known about how these algorithms actually interact with one another so this brings us to the main question uh that draw our main questions that driver work which is how can we understand the behavior of interacting reinforcement learning algorithms in financial settings and second can these algorithms provably learn to collude um so just to get us started with some of the formalism I'll just review really quickly the basics of Ro so RL in RL an agent seeks to optimally interact with an environment to maximize reward so this image here is from Sutton and barto's classic reference on the subject and shows the the sequence of events so an agent observes the state of the environment and picks an action to feed back into the environment this action will cause the environment to move to a subsequent State and the combination of the action and the state will yield a reward um for the agents um formally RL is the optimal control of a Markov decision process which is given by the Tuple the s-a-u-pia Delta where s is the state is the set of states of the environments that the environment can be in a is the set of actions available to the agent um UI denotes the immediate reward received by the agent so which will depend on the present state of the environment and the realized action and P is the transition function of the environmental Dynamics so it denotes the probability that you that the environment will move to a particular State given that it was in some other state and we took some action and Delta denotes the discount factor of the agent so how much they discount future wealth or future payoffs okay so agents will choose actions according to a policy function which may be random so policies are denoted pi and their functions um from the set of states to the set of probability distributions over actions so the goal of the agent is to learn an optimal policy Pi star such that Pi star maximizes expected discounted payoff so this is where the um the uh discount factors coming in um so RL takes an algorithmic approach to this problem so generally speaking agents will parameterize their policies Pi um by a set of by a set of parameters Theta which we say will lie in some set G so they'll pick parameters from some set G um and uh the thetas the parameters are updated successively by an algorithm which in its most general form is just is written here so um let me hear so uh we call F the learning rule so f is a is a function that takes an old parameter values the current state current action and possibly the subsequent state so maybe we have a little bit of a delay in when we when we update our parameters um and it incorporates this information into a new set of parameters the learning rate this gamma n um is either exogenous or possibly endogenous in the case of adaptive regimes and determines how much of the information should be incorporated so the goal is then to set a to find a set of parameters um that um such that the the parameterized policy approximates the um the optimal policy and it's a very general way of presenting RL and a wide class of algorithms um can be understood this way so one particular algorithm we'll focus on is asynchronous tabular cooler so Q learning at its core is very simple but serves as the base the basis for a large number of more complex algorithms so such as the the dqn algorithm which is a a very popular deep uh reinforcement learning algorithm which achieves superhuman performance on learning how to play Atari games um from Pixel data so we're focusing on the the tabular version of this algorithm um which Associates one parameter value so given by the letter Q to each state action pair um and the algorithm updates Q values in an asynchronous fashion so that is the Q values are only updated when a particular State action pair is realized so this is compared with synchronous Q learning which studies uh the updates sorry synchronous Q learning which updates the Q values at every Q value in each step and actually this synchronicity and asynchronicity is is um pretty important in determining collusion and and um some authors have studied synchronous Q learning which actually is more amenable to learning competitive outcomes than collusive outcomes um so with a given set of Q values policies are parametrized by a soft Max distribution uh with inverse exploration rate uh Tau so as Tau gets lower um we Explore More and as Tau gets higher we exploit more um so this algorithm is very popular and underpins many algorithms in RL and Its Behavior is is well understood in certain settings in particular uh when there's just a single agent interacting with an environment whose transition function is fixed then Q learning will learn an optimal policy however as I said markets are multi-agent and non-stationary but still people use Q learning anyway so a lot of the the um the academic work using RL and finance uses Q learn um so the question becomes so how does q learning behave in multi-agent settings um and more specifically how does q-learning behave when it interacts with other Q learners so Harrison yeah yeah ask you a question so um I mean for a video game it's pretty simple what the state would be and what the rewards would be right the state is yes you know other pieces and where they could be on the board and winning is clearly defined or like winning a piece it could win you three points or something like that so I Wonder Isn't maybe in finance the devil is in the details of how do we Define the state or what you know are the actions Buy sell hold or are they uh go long go short or double down I don't know like you could buy a call option or I don't know so I wonder whether you know what what are your thoughts generally on the state and the action space in finance yeah so we in the examples that that all uh cover in this talk the state space um or the the the value of the state process is the vector of prices um posted by other by other um by other agents so basically I observe the current set of prices and then I'm going to take um I'm going to price conditional on that on that Vector of prices um which is a a pretty um yeah this is It's a a simple um a simple way of conceiving of the state space and I think definitely as you um as you make the state space more complicated um you're gonna have to do yeah there's a lot more analysis that needs to needs to occur but one one interesting thing um in our work is that sometimes if you start conditioning on things that actually aren't relevant in the market um you can people can learn I guess uh people can learn to Sun spot trade um uh in in our analysis shows that you can can prove that these algorithms will learn kind of sunspot Trading um yeah okay so um to answer the question of how Q learning behaves when it interacts with other Q Learners uh we'll need to define the multi-agent analog of a Markov decision process which is a stochastic game so as to cast a game is a repeated game uh whose payoffs may change period to period depending on the state process and the state process now the evolution of the state process is controlled by the actions of all players in the game so all agents are going to influence the the the flow of this um of this process so the notation is basically the same although now we'll use bold letters to denote vectors across the number of um agents um so bold s will denote the State of the State process um you know in its most general form this the state process could could write could be very large and incorporate public and private signals um uh actions payoffs and the transition functions are are defined similarly um so now where each quantity depends on the actions of of all agents in the system so the the goal of each agent is then to maximize expected discounted wealth or expected discounted utility um but now the Dynamics of the state process are determined determined by all agents so this expectation is taken with respect to um the the policies of all agents although the policies of the other agents are unknown to um to the H and I so we're instead of seeking a rather solutions to to to games or not necessarily um uh we don't have a sense of optimality but we have a sense of equilibrium so um we'll be we'll be um kind of concerning ourselves with with Nash equilibria this of this um of this game um so right so because because um because agents are going to be learning and adapting their policies through time the the true dynamics of the state process are going to be non-stationary even though the transition function might be fixed um so the main example uh we'll we'll run with throughout this talk is that of the prisoner's dilemma so the the prisoner's dilemma is a classic two-person static game um so just uh one interaction and and nothing repeated um each agent has two possible actions denoted c and d uh standing for cooperate and defect in the um in the classical uh presentation of this um and these actions yield the following symmetric payoff so um if if both agents play C and C then the payoff to each agent will be one um whereas uh if if I uh if agent one plays C but agent two plays D then C will get zero um sorry web agent one will get zero but agent two will get two um so uh we interpret the prisoner's dilemma as a stylized market so definitely very stylized and I'll talk about the limitations although a lot of them are are are are clear um so with two competing liquidity providers so um in our interpretation each liquidity provider must quote a spread and can either quote wide or narrow uh when both liquidity providers quote wide there's Mutual benefit and demerited demand is shared equally among these higher prices um uh so we this corresponds to each LP getting a payoff which we just normalized to one um however quoting a narrow spread will undercut your opponent and take demand so you'll get you'll get more of a payoff you'll get you'll get the payoff of two if you undercut your opponent while they uh quote wide um so you'll so you'll get greater payoffs than than the mutual-wide um uh quoting mutually wide but both liquidity providers have this same incentive to undercut which means that the unique equilibrium is for both liquidity providers to quote narrow so we say that the the equilibrium of mutually narrow spreads corresponds to the competitive quoted spread so in the competitive sense the the spread should be should be narrow so the the president's dilemma is classically a static game so Agents come together take actions and never interact again agents are myopic and they own in that they only cons are they're only concerned with immediate profits um and the notion of equilibrium also assumes that agents are infinitely rational so there's some limitations to the static model um but to uh to make the model a little bit more realistic um we we consider a repeated a repeated prisoner's dilemma so so um agents are going to uh play this game over and over again um and we're interested in algorithms learning to play this game uh while other other players use algorithms as well um so so to frame uh prisoners dilemma as the stochastic game uh we need to give some notion of of state to the game so we'll consider the case as I said um the case of one period perfect monitoring so this is the the state process uh is of a given period the the the state value of a given period is the vector of realized actions from the previous period so one can think of the two competing uh liquidity providers each choosing a pricing policy which is conditional on what the opponent just did in the previous trading period um so yeah just to speak briefly about the limitations of of prisoners dilemma as a model for Market interaction so of course this is definitely a toy model and prices should have an inter-temporal effect on demand um which should be incorporated into the Dynamics of the state process uh here we only consider the state process to be the vector of Prior realized prices um but really uh the the state should probably incorporate more uh more information and uh at a longer time Horizon uh longer time scale than just the previous action profile um as well as you know exogenous um uh shocks to the system um and we just yeah we abstract away from this um and um and also of course time takes uh time places uh plays an important role in one's pricing strategy but we're just we just look at the time at which a trade takes place um so while the repeated prisoners dilemma is is pretty simple we do think it captures an important kernel of the repeated interaction between Market participants which is already much more than a lot uh much more General than a lot of literature on algorithm inclusion which uh a lot of a lot of this literature considers just static games um and also the tools that we present here um are much more General and cover and cover more General scenarios that address that can address these limitations and we just we we're forego a treatment of it um explicitly mainly because of the the difficulty of of visualizing um stochastic games and um and uh as as things get very high dimensional it gets very hard to uh uh visualize them okay so like for each agent we use an algorithm to learn a parameterized policy and our goal is to understand the behavior of these algorithms interacting um with each other and to do so we'll analyze the evolution of policies through the evolution of parameters uh direct analysis of discrete time algorithms is difficult um because you're having to deal with the randomness um of a non-stationary state process um so instead we pass to a continuous time limit of the system so we use the ode method from stochastic approximation which I'll describe momentarily and so what we do is we derive an ode a system of Odes which approximates the evolution of parameters and the system we arrive at is called we call it the algorithmic learning equations um which is in the um our first paper from from last year on this subject okay so okay so the goal is to arrive at an ode um which approximates the evolution of parameters so classical stochastic approximation um would would look at the ode whose Vector field is just the expected update of the learning rule um where you take the expectation of the um of the state process um and if if the state process is simple maybe it's the state process just the realization of IID random variables then this would be easy and and you'd have that the the ode would really just be the expected the expected evolution of your of your algorithm however in our setting s is the evolution of this non-stationary process whose Dynamics are changing according to the learning dynamics of all the agents so what we do is we derive an approximate expectation and so to accomplish this we consider the hypothetical evolution of the game with parameters fixed so this is what we Define here this s Theta so s Theta when you fix Theta um is a true markup chain whose transition Dynamics are given exactly by equation nine so we have the transition function and then we just multiply by we just um multiply by the uh the probability of taking um by taking the uh any set of any any Vector of actions um so if this hypothetical if this hypothetical evolution of a state process s Theta is ergotic then there exists a unique stationary Distribution on states which is given by the long run average frequency of visiting a certain state um so uh this is the the true State process isn't going to be stationary as we said um but for fixed beliefs or when when learning stops okay this hypothetical Evolution uh is um can be ergotic um and so one also has that that this stationary distribution when it exists is Lipschitz continuous in Theta uh when the policies are lipsticks continuous as well and that'll be important in a moment so to arrive at a set of a set of equations uh we first condition the learning rule uh with respect to the stationary distribution and just induced by fixed parameters so this gamma Theta of s and then we take expectations there so we so we take this conditional expectation with respect to the um the stationary distribution and then we uh we we build from there um uh yes so what we end up with um is a system of ode is written here which we call the algorithmic Learning equations and um the the vector field might look foreboding um but it's really just yes conditional expectation stationary distribution and then expectation with respect to policies um yeah a question from the audience um yeah someone's asking do we assume that all agents have the same parameterized policies um so no in in this general form we don't um in um sorry here we'll assume that each agent um parameterizes policies with this soft Max distribution over Q values um and this choice of softmax softmax distribution is is is very common in RL literature which is why we do it so we think that this each agent using a common parametrization is not um too limiting an assumption um okay so okay so under um under the following conditions uh the the evolution of the ode will um will approximate the um the evolution of the algorithms um so just to speak briefly about these conditions condition A1 ensures that the learning rate is non-degenerate so we can't have that the the learning rate decreases to zero um so fast such as the such that the algorithm stops learning just because the the learning rate is zero um or goes to zero too quickly um the proof of approximation also relies on compactness um so uh we'll need that the the um that the parameters uh all stay in in a in a compact convex set um and um I should say the the example that we'll consider of Q learning we show that that Q learning under the softmax um action selection satisfies all of these properties um so okay so we need that that parameters stay in a compact set and in order to find the um the stationary distribution we need that the state process is a nargotic Markov chain uh with a single recurrent class for every um for every possible Vector of parameters Theta so for all this all the parameters Theta in in our set of parameters the the state process needs to be um ergotic um we also need that the um policies are Lipschitz continuous in in parameters and that the learning rule is lips just continuous in parameters as well so under these conditions the evolution of the algorithms can be approximated by the evolution of the ode in the following sense another quick question I think about the previous slide sure um will there will there be a pattern for parameter a action of Agents across different states or will values of parameter a be random basically will the behavior of Agents be predictable um yeah so the the um the ode is a deterministic system um so in that sense we we've killed all the randomness when we when we look at the the system of Odes and and how we approach studying these algorithms is we we look at the um the evolution of the Odes to describe where the the algorithms will tend towards they're still stochasticity in the algorithms but um I guess this I think this next slide will be helpful um because it describes in what sense um the the equations approximate this Evolution so um uh so our theorem one given here is that uh under an appropriate time scaling so if we scale time with TN equaling the sum of learning rates up to time n uh up to period end um then the algorithm will be close to Solutions of the ode for any finite time Horizon capital T with arbitrarily high probability provided that the learning rate is small enough so we assume that the the learning rate was non-increasing so all um values of the learning rate are are bounded by its first its first value um so the probability that the that the algorithm will be close to the solution can be made arbitrarily high if we take the um if we take the learning rate small enough um so next if the learning rate decays fast enough but not too fast so we have um that the we still have that the first moment of the uh learning race is infinite but you know the the one plus Epsilon moment is finite um of of the uh of the learning rates um then the OD actually approximates the asymptotic behavior of the algorithm as well in that the algorithm will converge to locally asymptotically stable Solutions almost surely under some restrictions so we need that the behavior of the this that will have this asymptotic um this asymptotic approximation when the Dynamics of the ode are well behaved um so this first theorem just says that the um says that when the learning rate is small enough we have a finite time approximation um and the Arab two says that if the learning rate will decrease to zero fast enough then we can have an asymptotic approximation as well um yes okay so as a proof of concept to validate these equations uh to say to show that they're indeed good approximations of the behavior of the algorithms let's first consider repeated prisoners dilemma um but agents don't actually condition on the previous action profile so we just um they just learned one policy and they use that same policy every every period they're kind of a dumb algorithm they don't they don't remember anything um and we'll use Q learning um with the same soft Max uh softmax action selection and it can it can be very easily shown that the Q the Q learning status satisfies all the conditions necessary for the um for the ales to to approximate their evolution so they they satisfy condition a which was given before and the the the state process condition the one that says that the estate process is ergotic for any beliefs is satisfied immediately because there's only one state um the state doesn't change um so um so we have here um is uh our our plot of the actual algorithm so recall that the the ales approximate the finite time evolution of the algorithms um but only when the learning rate is small so one question is how small does the learning rate need to be um so we've simulated here trajectories of the learning algorithm with a large learning rate so um the horizontal axis axis uh corresponds to the probability of agent one um quoting wide and the um the vertical axis is the probability of agent two quoting wide so we started the the algorithm here at these at these points and it ran and um in these two scenarios we we ran up to the the top right corner which corresponds to both actions sorry both agents quoting widespreads with probability one or newer one um in the bottom left corner corresponds to the equilibrium spread um so the the Nash equilibrium um oh sorry um so the solid lines here oh sorry and the the um the learning rate here for this algorithm is um somewhat large so I think we took this to be 0.01 um which is uh not not entirely unrealistic for for a uh um an algorithm to be to use a learning rate of 0.01 in practice um the um solid lines here represent trajectories of the learning algorithms with a small learning rate um so we started from the same position just ran the algorithms with smaller learning rate I think this was um you know one uh 10 to the minus four I think um um so that's this these are there again these are again trajectories of the algorithm itself so they're playing something like the prisoner's dilemma but with this uh quote wide or quote tight and basically right exactly exactly yes yes um this experience is super small right it's just two yeah but the the complication is that there's several agents or that they're competing yeah so here we're just considering two agents um and in in the in the numerical examples that that will follow we'll still consider two agents but we'll look at we'll look at when we will enlarge the state space so that each agent will actually be conditioning on the vector of actions um that uh um that were taken in the previous period so the the gray lines here uh represent a field plot of the Odes so you see they are a a pretty excellent approximation of the algorithm with small learning rates and the approximation doesn't break down too poorly um as the as the learning rate grows um so we end up going to the same place so um um so moreover the ales are a good way of of analyzing kind of the probability that one will end up at a collusive outcome um because we can analyze the Basin of Attraction of this absorbing Point here and the basis of Attraction for this um for this scenario is quite large so there are a lot of initial conditions a lot of initial parameters will lead you to the to the um uh the uh Perpetual collusive uh Point um yeah I mean oh more than 50 of initial conditions will lead you there okay so now um will give each agent the ability to condition uh her spreads on the spreads of her opponent from the previous period uh so the setup is similar to before but now we consider State dependent Cuba so this algorithm can also be shown to satisfy the conditions necessary for the uh to be approximated by the AL East the main condition which needs to be shown is that the state process is ergotic for any choice of Q values but this is given because the um the Q values stay bounded um stay in a bounded regime because the rewards are finite um so so with Q values with Q values bounded the um the soft Max will be will be bounded away from zero as well um so that means uh yeah can I ask you a question about the the previous simulation that we have a question from the audience yeah yeah they ask what's the source of noise in these simulations the source of noise here is the so um in the rougher lines here um there's there's Randomness in the policy selection so this is um the um right so so the the there's there's more there's more Randomness in the action selection so there's going to be more Randomness um in in what the in what the agents learn um does that make sense are they is it because they're mixed strategies right so basis yes these agents keep flipping a coin to decide whether they're wide or tight right exactly exactly yes um okay so um because the the state process is given it is the is this Vector of previously realized actions um and the probability of playing any action is bounded away from zero the state process is going to be ergotic because any um any Vector of actions has a problem has a positive probability of being realized in any in any um in any period that that um that that probability might be very very small but it's at least bounded away from zero so uh we do um have that the state process is ergotic um so that the ales that we end up with are are similar to the ones before just with the addition of the scaling Factor um given by the stationary distribution for fixed Q values okay so um we'll show that Q learning learns to collude but uh now that we have a more complicated algorithm where that's going to condition on past prices we can actually be a little bit more precise about what we mean by collusion so in economics collusion um isn't just observing inflated prices because you can you can attain an inflated or super competitive prices without collusion um you know I can just decide one day that I wanna I wanna you know inflate my prices and maybe my opponent wants to do the same um the inflated prices and the collusive regime uh must be enforced by a what's called a reward punishment scheme um so agents reward each other um reward those who stick to the inflated price by continuing to play the inflated price and those who undercut are punished by deviation to the to the competitive price um so to examine the frequency with which a set of policies will induce the collusive outcome so how often do we actually see the collusive outcome uh we'll look at the stationary distribution of the two policies at the mutually at the mutually collusive outcome and then we interpret the components of each agent's policies in terms of um in terms of punishment so um the probability that one is going to quote narrow given that the given that one's opponent quoted narrow can can um give us a sense of the the strength at which uh agent one is going to punish um punish its opponent for deviating and if both agents have have quoted narrow um we can think of this as the length of punishment punishment so how long do I want to continue playing uh quoting narrow given that we're we're in this state of punishment and um right the probability of of playing uh quoting wide given that both agents have quoted wide is the probability of continuing cooperation okay so here we've plotted the algorithmic learning equations for State dependent Q learning for a range of initial conditions until the trajectory is numerically converge so the horizontal axis here I know it's sorry it's very small um I can zoom in a little bit if that helps um the um the horizontal axis is the probability of undercutting given that your opponent has undercut and the vertical axis corresponds to the probability that you continue to play the competitive price given that both agents quote competitively and so the left hand panel shows the initial Alpha collusion probabilist the initial stationary distribution um of the mutually collusive outcome the relative frequency that initial policies will play the collusive spread and in the rightmost panels we plot the collusion probability at convergence and the middle panels just plot the difference between values at convergence and initial values so as we see so for every initial condition in this range um the agents will learn to play the collusive spreads nearly 100 of the time um note two that agents need not start with high collusion probabilities so around here the the relative frequency of of observing uh collusive prices is actually only around 0.3 so you'd only observe collusive prices about thirty percent of the time um the and the red squares here correspond to uh competing uh playing the competitive spread nearly 100 of the time um and um some so the uh Perpetual collusion I should also say that that we've we've assumed a lot of symmetry in these in these plots here um in the previous plots we didn't assume symmetry but here just for ease of visualization we've assumed some symmetry but we have um we have plotted and investigated this when we when we break the symmetry so we don't assume that each agent starts with the same um with the same policy um it's just a bit more difficult to uh to visualize um and but we still get the same uh we still get uh similar Behavior so a very large Basin of Attraction of collusive um of the collusive outcome but collusive uh and competitive outcomes are not the only outcomes um that we observe so um these yellow colored squares here correspond to some kind of unexpected Behavior the agents here actually learn to play the collusive outcome 50 of the time and in fact players will learn to cycle between the mutual mutually collusive and mutually competitive outcome so they'll just flip-flop back and forth from from both wide to both narrow um so as we said before uh for for Behavior to be considered collusive um we call agency to learn and reward punishment mechanism and so we do see that behavior exhibited by Q learners um the the middle and right panels here describe the difference in immediate punishment probability and the difference in length of punishment probability so respectively um between right convergence time and initial time so notice that when Q uh Q learning begins in this region here the difference uh or there's a there's a positive gain in the the strength of punishment and in the length of punishment so so here um we see that the the probability of playing of quoting a narrow spread um given that your opponent has defected gross and same in this case um so in this in the setting we can say that Q learning uh learned collusive behavior um but it is it is quite delicate because because from for many many of these initial conditions um Q learning uh didn't learn to increase the um the the strength of punishment okay so to summarize these results um uh we proposed The algorithmic Learning equations which are a system of Odes that approximate the finite time and asymptotic behavior of learning algorithms and stochastic games um the uh the methodology we use to to arrive at this is is stochastic approximation and we provide minimally restrictive sufficient conditions that allow for the behavior of a wide class of state-dependent reinforcement learning algorithms to be approximated and we implement the a at least numerically for popular reinforcement learning algorithm Q learning and demonstrate this algorithm can learn to collude so some limitations of this work is that there are some numerical difficulties in that in in some of these in these plots um calculating the the stationary distribution is actually quite sensitive to machine Precision um so in a lot of these cases the the stationary distribution we calculate is actually one so the the compute computer spits out one um when theoretically it should be bounded away from one and bounded away from zero but but this is because you know to calculate the stationary distribution you need you need to take a lot of Matrix products um and and you need to be careful about how you do so we also don't have theoretical guarantees so we don't have that um we don't have uh so sorry here um to to guarantee that the asymptotic behavior of the of the equations approximates the asymptotic behavior of the algorithm we need to show that the Dynamics of the ode are well behaved and this um amounts to showing existence of a lyapanov function um and for the key learning ode this is uh this is pretty difficult because of um you know you need to deal with this uh the stationary distribution here so you need to differentiate through it which is which is quite difficult um so I know I'm running a bit short on time um sorry um so to uh to address the the um the issues here uh we do we in our second paper uh we introduce a a new algorithm uh which we call State dependent um smooth fictitious play which is an in a generalization of classical fictitious play which is a more game theoretic algorithm which um learns uh stage game or static game Nash equilibria through repeated uh through repeated interaction um so it's a particular particularly simple algorithm uh fictitious play is uh basically the algorithm assumes that all agents in the system play according to stationary policies and the beliefs of those strategies are formed via the empirical frequency of play so say my opponent plays action a twice and action B twice then in the fifth period I'll assume that he'll play action a with a 50 probability and B with a 50 probability and then with these beliefs I'll myopically pick a best response and under particularly strict conditions fictitious play will converge to an equilibrium Behavior so you can actually theoretically prove that fictitious play will will convert converge to an equilibrium and this is a very old algorithm I think fictitious player was introduced in the in the 50s um there are some issues with it so in the 90s um basically if if um uh if you know because the the learning rule is quite strict and there's no it's it's completely deterministic you can take advantage of your opponent um so you say I know my my opponent is going to learn in this way that means I can play in this other way I can play in a you know in a way that's conscious of that which is going to guide my opponent into a certain direction so to to get around that um Levine um introduced smooth fictitious play which which basically adds some Randomness into the system instead of taking a a true best response they'll take a smooth rest response so basically um as we'll see this is really just uh taking actions according to a soft Max distribution which is what we considered before um so our algorithm um here and I I think yeah I'll just say I'll just describe the algorithm um really quick and then and then we can kind of move on to questions um so our algorithm here we call State dependent smooth fictitious play uh it's an extension of fictitious play two stochastic games um each player is assumed to have full knowledge of their own payoffs but not necessarily the payoffs of other agents each agent assumes that all agents play according to a stationary State dependent strategy and beliefs about the strategies are given by the empirical frequency of play which is State dependent as well so for each state of the game each agent keeps a running frequency of each action plate uh beliefs are updated then by equation 21 here so we just update by the by the empirical frequency but we can we can rewrite uh the algorithm in in a nice iterative form which is here um so play from this algorithm evolves in the following way given a belief um agents compute uh the expected their expected discounted utility um for all states and actions assuming these these uh assuming that all agents will act according to these these believed strategies so they they compute their their expected uh discounted utility um and then sample um sample actions with respect to a softmax distribution over um uh over these action values so we call this the the action value um so we take it we take an action um through the this softmax um this soft Max distribution or logic Choice function and we repeat this um we repeat this process uh infinitely um so the the the main result is that we can we can approximate this algorithm by the like a continuous time system using the algorithmic learning equations and um and we can analyze that system of of equations we can prove existence of a lapinov function and show that that rest points of of uh that system of ordinary differential equations will actually contain collusive strategies so we guarantee that this algorithm smooth fictitious play will converge to rest points of of the continuous time system and rest points of the continuous time system may be collusive so this algorithm has a non-zero probability of truly converging to collusive strategies um and here's a quick plot of the of the of how the Basin of Attraction of the collusive strategy grows as agents become more patient so this this algorithm depends crucially on the discount Factor as the discount Factor grows and as as um as agents are are more and more patient the probability of of of learning um of learning the the the true collusive outcome uh the collusive strategy will be um uh we'll grow um so I think I'll I'll cut it here and if I if people want I can go back um and and talk about things more but the the key takeaway is that completely decentralized algorithms can indeed learn collusive strategies so we saw this numerically with Q learning um and uh well it was rather quick this algorithm smooth fictitious play State dependence move the petitions play can provably learn collusive strategies um as well so um further directions to take uh Take This research um definitely you know we want to consider a more realistic Market Dynamic so considering State uh stochastic games where the state process um is going to track demand and and prices will have an inter temporal effect um on demand um and uh of course we want to we can consider more complex algorithms so we want to we want to study deep RL algorithms um rigorously and theoretically and see which which um which equilibria and which prices this uh these algorithms can learn theoretically um and um the the algorithmic learning equations that we've that I've presented here can be applied in both of these directions so this is definitely something that um that we will be doing um and finally um there's definitely I I've spoken about the possibilities of algorithmic collusion um but um I I haven't said anything about collusion detection which is definitely um a very interesting problem and a very difficult problem um and we're we're looking into ways about how how to actually measure collusion if you're just observing behavior in the market can you determine whether the behavior is collusive or not um yeah so I'll cut it there um thank you all very much for um for attending and listening and I'd love to take your questions okay um maybe I'll start um yeah yeah if other questions pop up feel free to put them in the Q a um I guess you're gonna find you know about your this future work uh if two agents have a behavior that's correlated does that mean they're colluding or does it just mean they are both doing a rational uh you know self-preserving strategy yeah yeah it's it's a really tough uh question because um because the collusive strategy is here this is the these are the particular this is the particular set of collusive strategies that SFP our algorithm can learn which is basically take a set of collusive prices if every agent uh quotes those prices I'll continue to quote those prices but if anyone deviates then I'm just going to go to the competitive price um this strategy these strategies are a true equilibrium of the game so in some sense they are a rational outcome um but our understanding of collusion is generally with respect to this to the static game so this the the equilibrium of the static game is uh is to quote the narrow spreads um but you know the static game is a is a really horror reflection of the reality of of the dynamic interaction between people so I think definitely part of this this work and the and and I think the the legal work if if um you know if this is to be regulated I think there needs to be a a kind of a holistic like um I don't know treatment of what what equilibrium Behavior do we want to consider as collusive because this is rational in some sense um and we see that that rational algorithms can arrive at this Behavior without without communicating with each other yeah that is very interesting um okay well it looks like we don't have any other questions so um Harrison thanks again for great talk uh everybody else we're we're done with the spring um spring semester series we look forward to seeing you in the fall and uh yeah have a great summer everybody 