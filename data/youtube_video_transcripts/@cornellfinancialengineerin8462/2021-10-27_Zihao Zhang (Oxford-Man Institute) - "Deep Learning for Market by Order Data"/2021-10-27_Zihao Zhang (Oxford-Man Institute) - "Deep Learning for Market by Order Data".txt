okay so i guess it's it's happening um all right everybody um i'm really happy to introduce jihao zhang uh who is at oxford and um uh currently coming to us from china so i'm real happy to have this uh global seminar um this semester and we're gonna hear and talk about deep learning uh for market order data so um see how i'm really excited to sort of see how you approach my market microstructure not so much from the perspective of limit orders but by observing the the market orders and the trades in the market and specifically taking this deep learning uh angle which i know many people in our audience are are excited about so welcome to the seminar and uh and uh look forward to your talk thanks sasha uh hi and i'm jihao um thanks uh peter sascha and reserve for inviting me to the seminar and thanks all of your time today um so i'm a postdoc at oxford my institute and also i'm part of the machine learning research group here uh at the moment i'm in china and working at a hedge fund that's doing uh high frequency trading so in this seminar i'm going to present our recent work that applies deep learning model to market by other data and this is a drawing work with brian ling and stefan lauren so here's today's gender so i will start with a very brief introduction so just and describe the marquee microstructure data including the limit of the book and the market by all the data all the mbo data then i will introduce our deep learning model and discuss the network architectures so we design some some specific architecture for this type of data after that i will present our results and show you some interesting results uh i see at the end there'll be a summary so i will conclude the talk and and discuss some interesting future work so here's the uh the overall the agenda and let's go to the introduction so in this paper we study the market by other data the market well the data is essentially just a sequence of messages that describe actions of individual traders because those are high frequency microstructure data so those are kind of instructions given by different trader and it is arguably one of the most granular sources of information um so you so you have more updates so you have more events for the envy of the data compared to the limit of the book data because the limit of the book data are actually derived from such messages so derived from the mbo data but the mbo data is currently kind of largely neglected by the current literature uh so one of our motivations actually to fill in this gap so if you look at the high frequency data if you look at the the paper most of the paper are doing modeling on the limit or the book data yes i think that at at the time of of writing this paper we haven't find any literature on this so that's kind of our motivation to filling this gap and we're applying deep learning model uh like the lstm and the attention mechanism to model the dynamics of monkey by all the data and we use those models for for predictions um and i think to the best of our knowledge this is the first predictive model using the mvo data for forecasting high frequency movement and we see that mbo data provides an orthogonal source of information so can additional information to them or the book which helps to expand the universe of rfa discovery so i think we think there'll be some interesting things so some interesting features that people can derive from the market brown data and those information will be additional to the limit of the book and those information will be helpful for funding on funding or uh be helpful for predicting returns and etc so that's the um that's the overall like the that's the goal of this uh of this work then let's uh take a a closer look at the limb or the book and the market about the data because they're highly uh currently so this animation shows the evolution of the limit order book so in case if you haven't um if you if you don't know the limit of the book so the the limb of the book is simply a record of all outstanding limit orders for an instrument at a given time point uh so if you have seen some like a stock chart or stock price so you see the candlestick chart etc but those are like the low frequency data so you get a high low uh open close price for a specific instrument but if you zoom into that chart um the price of a stock or security is actually not a single value but multivariate time series and that is actually the limit order book so you can see the limit order book of a given instrument this one is actually the lloyds i think this is a large from the last year and you see uh limit autobook is sorted into different price levels and those price levels are actually based on the submit orders uh so because when you submit a limit order you get an option so you can specify the price and the quantity of your interest and those orders will stay in the book until they be matched and you see there's two sides here the ask side and the bid side so the so the ask side shows all the selling orders and the bid side presents all the buying orders um and you see each price level here so each price level each bar here consists of many small orders because those orders are segmented by different b by different traders and overall we have we have those amount of quantity for each price level and all the books get updated when there's a new message coming in so this message can add a new position so add new orders or cancel an existing order or maybe just update the existing order and when when the message comes in we get a new update and such individual messages are actually just remarkable the data and it displays traders action so we can restructure them all the books step by step and this this animation shows how the other book involves by incorporating incoming market by other messages if you uh if you play it again you can see how it gets updated so uh we get an update of the level order book whenever there's a whenever there's a change uh so this can be like an addition of the orders or cancellation or etc and let's take a look at those messages so as i mentioned those are just just individual messages from various traders so if you look at the table here we have the timestamp so you see the timestamp here those are not regular time intervals because because those are the or the moments that that message is being sent out and we have the id so those id identify the the traders but the anonymous so you won't be able to know who's doing this trade and we have the type so the order type says it's if it's a limit order or market order and then we have the site which basically tells if it's uh uh if it's on the ask site or the bid side and action like cancellation uh addition of order or just update of the order and we have the price and the size as well um so so you see we have those type of messages and those messages comes in and all the and the limit order will get updated so the number order actually the derived data and the limit order shows the overall demand and supply relationship for a specific for the specific instrument the raw data the underlying data is the market by other data so it charges the kind of placement and cancellation message and our interest is to see if we can just use those type of messages to make predictions and still and see if those type of messages will provide additional information so that's that's the focus and uh and now let's just look at how the nbo data updates and limit order book i'm gonna present four different scenarios and the first one is the placement so it's a it's a addition of my order and you can see on the left side here we have a limit order we have a snapshot of them all the book at time t um and that's uh as a new message coming in so this new message is essentially marked about the data so from this id from this person on the ask side the type specify the which side of the book uh also the the type specified if it's a limit order or marquee or then we have we have the site specifying it's on the ask site and the action so we're we're adding a order here so you see uh in in at time t there's nothing here but this message says so we're gonna add some orders to the uh to this price level so the 17.04 with this amount of shares of 7580 shares so this is uh this is uh this is a message this is typical message of the of the nbo data and we see we got a new update so you so those time intervals are not not in regular intervals we often refer this as the tick time so it represents the change of the limit order book and similarly we have the cancellation uh again from this person uh we have an existing order standing in the limit order group and we are we're gonna we're gonna cancel the entire position and we have again we have a new update of all the work and we can do also do like partial cancellation or update um so for example in this case we have an order of this amount but we're going to reduce our our size to half of what we had before um and you see the action so the action will be different um and the last case is uh it's an aggressive order so basically um so basically this order says we're gonna we're gonna cross the spread so the spread is the difference between the first ask price and the first bit price so we're gonna cross the spread uh and we're gonna make a trade so uh so those uh this is uh this is the last example so in this case we're gonna buy this amount of share at this price level and you see this this this amount of order being matched so it's gone and so those are the four different scenarios uh now let's move on to the deep learning model um so as i mentioned the current literature focus on the on the limit order book but just to remind you our motivation we like to use those messages from the those message string for market about the data and model them directly and make predictions so the the mbo data is is lower dimensional than the demo order book so we have less features but it's a bit difficult to learn the structure because those are individual message and they do not explicitly show the overall demand supply relationship but uh but uh but the limit order book does that so however only however the results indicate that those type of message provides us with additional information uh compared to the limit order book so on top of the level of the book i mean um and just a little bit recap on what we did before so we used we started with the other book uh i think two years ago so um what we did is we have some where we collect the limit order book data so at a time point at any given time point of t we take past few events like 50 updates or etc we can form that as an image so it's kind of space time picture so on the y-axis we have different features so essentially just different levels of order book and on the x axis we have the time so this form like 2d matrix so this is uh this is uh this is the input at time t and we can give this input to a neural network and we can we can we can predict the market movement so this is the this is the typical structure of the model and this is actually one of our previous work so we use the limit order group data to make predictions so it's just a quick recap showing how to use limit order book to make predictions and later on we're going to compare with uh with the market order data and you see we have an input which is a little autobook image and this can go through a serious layers of convolutional layers or inception module or resonance etc and we ought to put the the resulting features from the convolutional layer to the recurrent neural layers so the recurrent neural layer the lstm here can learn the structure so additional dependencies from the features and and as the last layer we can output output the predictions so in this case it's a simple like classification setup where we have the up neutral and those labels can be decided by uh by using returns with a threshold so if the is uh is uh is above a certain threshold we see the market going to go up uh or on the vice versa of the market going to go down um so this is a this is a typical structure that network architecture that we use for for making predictions from limit order book and in terms of the market about the data what we did is to replace the first two components so we're no longer using the autobook data but we use a message from the mbo data and we're going to replace the convolutional layers because um because the the message is lower dimensional in yeah than an older book less structured data and here's the typical example so we have the input and bio data those are messages from individual traders seeing what they're gonna do like by this amount of shares or sell or maybe just updating this message so it's just a message that have displayed before and we're going to put those message uh to directly to the lstm layer or attention layer uh at the end we're still going to make uh make a classification so i still classification problems we're going to make the predictions so predicting walking movement uh then then i can uh just briefly introduce the attention mechanism as i put here so the lstm is quite straightforward and also we experimented with the attention mechanism just in case you are not familiar with the attention mechanism uh so for given input of these lines of capital t um we use the attention mechanism so the attention mechanism typically have an encoder and decoded structure the encoder can be just lstm and the encoder kind of steps through the input times to extract meaningful features so you can you can think of those x1 x2 xt as the limit order book like at each at each time point uh and the encoder steps through those those time points to extract the features from the order book or from the from the market about the data so you can think each point is a message and we take parts like past few messages and form an input and the encoder will first extract the information from those time points and the encoder will summarize those information to a hidden state and this is a context factor so the content ranks to summarize the information from each hidden state and finally we have the decoder here so the decoder will generate the prediction so so in general the attention mechanism is used for sequence to sequence learning where we have the multi-input and output but here we are just using it for a single point prediction uh in in one of our paper we used for we use the attention for building multi-horizon predictions but in this case it's a single point estimation and this is a just a simple illustration of the attention mechanism and now let's present you with sounds let me ask you a quick question could you back to the previous slide uh just about i'm trying to understand the data as it's coming into the model so if you have a buy uh 8560 at 704 and let's say it's a limit order does the model know what the best bid and the best ask is or is there no memory of that oh yeah it doesn't it won't know specifically but so the through the normalization it will it will kind of so we'll so essentially through the normalization it will know if it's uh oscar order or a bit order because we normalize the price against the mid price so if this price is above the mid price then the normalized price will be be above one so basically i see so this you do to do a little transformation of the raw data into yeah yeah it's relative to mid at this input stage yeah i think if i show you the original message so those the the original message looks like this uh so in the in input we won't have the id uh we don't have the id but we have the we have the side action price and the size uh we we don't have the type as well because uh like over more than 95 percent of the of those messages are limit orders market orders it's very it only only account for for a few percentage of the overall order flow so mainly the aside action and price and the size we do a little bit we'll do the transformation here so mainly the price and the size uh because we we train or train the model with uh with a group of assets so we have to we have to normalize them to a similar scale okay so i i just wanted to make sure that at every time stamp we know the best bit and best task and it's normalized with respect to that yeah yeah yeah so let me go to the bit yeah so uh let me prevent you with some results uh just say just to just to introduce a setup so we have a classifications that have a predictive market movement so three labels um plus one zero minus one so the market is going up staying stationary and minus one and we have uh we have in total we have 12 12 months of data so that's about like 134 million samples we split them into train validation and test site so the first six months are the training site and we select five instruments so the lloyds barclay tesco bt and waterfall the next three months are the validation site and the last of three models are the are they all have sample data the testing site and we make predictions at different prediction horizons so basically 20 50 100 ticks ahead so in clock time that's kind of like 5 10 or 30 seconds ahead so in clock time and uh here's his uh his uh his uh the results for for the prediction horizon at 20 ticks i had and we tested on different models so you see the the lm it is a simple linear model and multi multilin multi-layer perceptions lstm and attention model the the this mbo indicates this model is trained using the mbo data and the law here means the the model is trained with limit order group data uh so overall you can see the model trend result with a limit all the book data are still better the results from mbo data are comparable but not as good as all the book data this is not very surprising because the other group data shows the overall demand supply relationship because those certainly those are aggregated data from from different orders the embryo data are just individual messages so they display individual actions but they don't do not show the overall demand or supply relationship um but what interesting is you can see we still get some decent results from just directly molding those raw messages however the p the pure ambient data uh can now outperform the limit order book uh so if you if we look at the f1 score here the data the data because it's a classification problem the labels are slightly unbalanced so so you need to focus on the f1 score as a mini evaluation matrix but the interesting thing is if we look at the correlation between the predictive signals so you can see the the so you can see the the predictive signals from the ambient data are less correlated with the with uh with the limb or the book of which the model trained from them all the books because if you if you look at the lstmc and that are trained from all the books you see that the circuit signals are highly correlated right but they're less correlated with with the signals from the mbl data so that means if we if we can uh combine those two signals um we can benefit from the diversification and also it means that we actually extract something from the mbo data that is not captured by the order book so a combination of the two signals from those two type of data can can can benefit from the diversification also kind of reduce the signal variance and we'll get we can get better predictive signals and to verify that we just include an ensemble model in our experiments this ensemble model is very simple hsc uh i just equated average of a predictive signal from the ambient data and from the uh from the market value the data and you can see this this ensemble model gives the best performance and this is a result where we combine two signals uh so this to some extent indicates the potential benefits of using the uh using the of incorporating the mbo data into into into predictions because we just directly use the mbo data you can do some some transformation of the mbo data and you can do a little bit of feature engineering as well and we think that's going to be helpful as well uh so this this table shows the results of the uh at the prediction horizon of two of 20 ticks ahead and we have the we have the results for 50 and 100 ticks ahead and you and you see similar similar behaviors here so yeah i have a few implement implementation questions in the chat room uh is would this be a good time to ask to ask them or uh yeah yeah yeah yeah okay yeah so i have um one question from peter de krem is does this normalization work with multiple securities that are potentially co-integrated or do you or do you do this one security at a time or or is it yeah we we train we're training a single model by using all the instruments that's uh that's in general that's in general helpful for generalization because there's kind of a little bit of uh stochastic uh stochastic there um i mean uh so in general we find it helpful to train a big model with all the instruments and you can find here and after that you can find you need you can fine tune the model for each instrument maybe fix the like the convolutional block or fix the fix the first few layers and just fine to the last layer and that's going to give sometimes that's going to give an improvement of the results i see so so you throw everything all the stocks together into the another question i have from david fitzpatrick is is this mbo data bought from a specific exchange yeah we yeah we we we uh we're spent we're sponsored by the main group so we have access to thomas reuter so uh those those uh those data uh uh comes from tom schweitzer uh and those uh uh those data from the london stock exchange okay so it says it's the london it's a single exchange and it's london stock exchange okay yeah yeah yeah and um i also have a question about how you define so david mandel is asking how do you define a tick up or tick down uh is it exactly the mid price moves by more than x or or by and is x one tick or is it just is it multiple things yeah there's uh i think there's a few ways to define the labels here so i mean the first one probably the most before world run is you just take the return uh you just take the return of the future price like the pt plus k minus pt and divided by the pt that says return you can you can you can use that that's that's one way and the so i saw the other other ways of defining returns like you can take the the average of the price uh you can do it's kind of a little bit like a small thing and you take the return of that and also you can specify a path so in this path if the if you find a point that gives let's give you the return higher than the threshold then you can see and you get an update so in this paper we we take the second approach so it's uh um it says average of the of the future price and take that as as a as a price and compare that with uh with uh with the current time point um i just uh that's what we did for this one yeah okay all right i i'm i in order not to interrupt the flow too much i'll ask you this one last one and keep the rest for the end um it's someone is asking uh patrick dote is asking um why focus on nf1 when ultimately pnl is the measure of success and 61 percent could have a better skew than 68. uh yeah i mean you can you can definitely focus on the on the on the piano that's uh it's just i think it's good that's a good point yeah uh uh i think the problem is um i mean sometimes uh uh just the way when you transfer the predictive signal to the conditions you can uh that's i mean just uh various ways of doing that so i mean you can just use a raw signal uh and and and treat that as a prediction that's fine but sometimes people probably gonna define some views like see when the when the probability from the soft max is higher than a certain threshold and you then only only only only at that time point you enter the position that's a little so that's a little bit different yeah but uh but yeah i mean we can check that you know that's that's that's absolutely fine that's a very good point yeah yeah okay i have a few more questions but i'll keep him for for the end of your talk so okay yeah thanks yeah so the i think we're just gonna to uh uh summarize this paper and that would be the end of the talk so i think this uh in this paper we propose to model uh the message from the mbo data directly instead of using the commonly used or the book data since one of the motivation is really to introduce this type of data because it has been neglected by the literature and we tested on on deep learning models including the lstm retention mechanism and we expect the those those architectures kind of extract some features from the multiple other data and use those types of features for predictions so the results indicate that the mbo data gives just some results but you cannot outperform the lemon order book data but a combination of both second of four signals give the best results so so in the future we're thinking we can probably down some future engineering things a little bit research on that side and see if we can direct more information from the from the market about that data and and that's all from me and thanks very much and i'm happy to answer any question on this one all right so yeah i i do have a few um more questions in the chat so please please uh you know for the audience if you have some some questions that would be a great time to ask them um so i've got uh peter degram asking what was the auto correlation between market moves did you find that did you measure that or or is that something no we haven't measured that but we can have a look after that okay so um the philip perumal is asking if we assume that some trades are from so-called noise traders can we filter out these trades and build a model only from those traits that convey more information and alpha thanks um i mean yeah i mean that's probably that's probably i think that's possible because when you refer to the noise trade i i i assume you mean the the orders have been added and cancelled and uh and and in general i think the orders uh in the deep side in the deep level all the books have been constantly cancelled i mean a large percentage of them have just been cancelled so if you uh so after the normalization you see the distance from the price you submitted against the mid price and if the price is above certain levels and we can probably treat that so i can always trade and pencil them yeah yeah yeah yeah i think in the in the literature often noise trader just means an uninformed you know essentially bad trader who doesn't know who has no predictability so i think the question is trying to see can you filter does your model maybe filter out uh in any way these traits but it doesn't seem like um like it's explicitly done i don't think i will really do that okay now we have uh someone who seems to uh karen lin who seems to have some experience with neural networks who asks what's the motivation for first making a limit order picture you using cnn layer and not directly putting time series data into lstm uh yeah it's the it's the it's due to the the property of the limit or the book so the limit order book are sorted into different levels um uh i mean you can put the limit order book directly into the lstm layers but but you can you can treat them as a spatial spatial but there's kind of spatial structure uh that can be explored by the same layers and that's what we experimented before so because you essentially have different layers each layer has a price and level and then they're ordered and if you use the same and and you can first extract the information from the price and because essentially essentially the cnn is it's kind of like a filter it's kind of like a little bit like a small zinc filter you can first use the same layer to extract the information from each price level summarizing information from the price and the volume and you can move that the window at at each price level then you can use several uh considering layers uh and and combine the information to another layer of filtering and at the end we find it just put it it would be helpful for predictions and if you're interested in this you know why we are using the same for a lot of book data and i think probably it's easier if you look at our our previous previous paper the default part we do we do a detailed explanation of why we are doing that yeah okay thanks um we have maybe um so we have a question by chen benar who said who asks any insights on why you used the learning network architecture you described so maybe we could look at the slide where you where you show the architecture and um yeah uh so i guess chen benar is asking a little bit about insights or of why this specific structure is is uh is useful or is it uh yeah so so i think it's kind of it's kind of related to the the type of of the data so those type of message so in general uh um i mean that's uh i mean that's a that's a that's uh so we can use a multi-multi-linear perception multi-layer perceptions but the problem of the multi-linear perception is we need to flatten the input and that would distort the time the time flows a little bit because you treat you essentially treat each feature independently and the lstm is is better because uh lstm lstm layer shares parameters to some extent and and the lstm kind of steps to information from the past events and kind of summarize that to a hidden state and propagate the information from the past to the to the current time point so the lstm the focus and the reason of having this attention mechanism is because the lstm layer only takes information from the last data and state to make to make predictions but we want to see uh if by using information at at each hidden state and we just want to see if the past events see whether if the wiser the participants can contribute to the prediction so that's why we use uh we also explore television with attention mechanism um but i think the benefits from attention mechanism is limited just because the um it's due to the property of the financial time series so that to some extent the most the most recent time points summarize the most information for the time for the time series so the benefits of using attention mechanism is not that significant uh compared to a toasting for an stm layer and i guess because of the normalization you're doing the you always know where you are relative to the mid price so that yeah i think yeah in a way is a form of attention um yeah yeah all right so now we have domenico spoto who asks how many hyper parameters are you tuning in the cnn and lstm models oh yeah that's a good question i can't really remember it has been a while but that's not a um that's not um uh i think i think in a paper we specify all the hyper parameters uh that's uh that's the field but not that but not that many i think in the paper we put a detailed description of what of what kind of hyper parameters we cure the following for the model yeah i just i just can't remember at the moment very well but as a in terms of an order of magnitude i guess if you're taking 20 inputs it's in the hun maybe a hundred or or two but not a thousand or two is that would that be right uh i think the um i think we only tuned i think we tune the number of layers and number of neurons at each layer and and the learning rate and the best size and i think that's i think that's that's that's eight yeah that's um we didn't we didn't think a lot of happened with family for this one yeah so yeah i mean that's often a criticism of the of neural network methods is the amount of parameters seems to be yeah quite large um um yeah but you think they're not that large uh i i think probably just we we we we often use this type of model so [Music] we can't know what kind of combination of problems they're going to work on or not if it doesn't work cause than april this means you're not suitable for this type of data so we didn't show a lot of uh i would do we did a training a lot of model in previous works by the english one not but not that much but the network often have this problem yeah you have a large uh space what you need to have around here okay um i guess so now we have a question kim who asks how do you define the success case does it include bid ask spread cost i.e success even when taking liquidity into account so i guess that in the question of your response you take the change in the sorry the returns and you you define these up down you know middle states but do you have you ever or have you considered using the bid ask spread as a criterion in there uh no we haven't that's um that's a good point i mean you can you can look at that i think it's just because of because of our motivation is to to demonstrate how you how can we how can we use the the uh mbo messages so we haven't looked into those details and all those type of metrics but but i think it's it's always good to to look at that yeah but we haven't done that yet yeah i mean i mean i think that there's there's something maybe i'll i'll put in my question here is that um you've you've done this for multiple horizons right and different thresholds so i wonder do you have some insight on what application you have in mind you know in terms of are we talking about uh h high frequency trading or ex order execution or you know do you have some kind of insight into for what application is this the most uh useful [Music] yeah i think as you said treat execution one of them and try some market making type of strategies uh or you can you can down something the data a little bit to the uh query to like uh three second intervals or even one second intervals and and you can you can you can build some some some strategy and being a liquidity taker and then you can uh because for the rolling all the book data it just we got too much we got too many updates and it's really difficult to cross the spur to make a trade so you you you so it's really really for the market making your trade execution so you can assume you're you're doing trade on one side of the book and if you want to cross the spread and you probably need to down some pull the data a little bit okay um so we have a question by reiner hoft and the question is when modeling the elo the limit order book do you include information about the individual order sizes or do you just aggregate the total size at each price level which i guess it's related to you know mob versus lob like when you talk about the elo limit order book are we aggregating or are we just um price level there yeah for the for the limit of the book it's the aggregated uh it's aggregate for the price and the volume uh aggregates the volume yeah so for for for the other book it's uh it's overall uh it's over order size for for each price not not individual orders yeah okay but for the ambient data uh those are the individual positions individual orders here yeah so that that's probably where you get some of the additional alpha i guess from the um you know i guess one order of size 10 000 is different than 10 orders of size 1000. yeah so actually if you do some so you can actually because we have the id those are anonymous ids but those ids won't change during a date so from mbo data you can actually you can trace back and look at the behavior of each individual order see what how they respond like if they cancel their positions or if they're adding position to the to their existing order so that's something you can get from the other book from the mbo data but those type of information are not available from the demo auto group data so there's some additional information there but um uh so we think you were thinking with the proper feature engineering things we can we can get something out from there okay i have a question from umu chaitin who asks in contrast to semi cancellations uh does your model include updates in the price of a particular limit order keeping the size unchanged so i guess an order that's at the best bit that sort of modified to move up with the price for example uh yeah yeah uh again that's will be that would be just that we treat them as a new update yeah so you have new message that size won't move just the price can be updated yeah that that that that happens okay uh well i think that's it in terms of questions on our end uh zihao thank you very much for being sort of concise but at the same time uh really presenting some uh interesting material uh you know that combines microstructure and neural networks in a in a way that's insightful so um so thank you very much um [Music] and uh to to our audience we'll uh thank you for the great questions and uh i look forward to seeing you uh next month for for our next seminar so um thank you very much and uh and good morning thanks thanks you 