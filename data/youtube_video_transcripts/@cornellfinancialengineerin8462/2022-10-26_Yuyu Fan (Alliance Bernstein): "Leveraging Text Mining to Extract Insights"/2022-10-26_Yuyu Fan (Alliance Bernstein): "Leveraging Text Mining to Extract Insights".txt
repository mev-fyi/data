all right everybody uh thanks for coming to the second uh financial data science seminar of the semester we've got a pretty full room here uh at the Cornell and I guess our Zoom room is is filling up as well uh I'm really happy to have uu fan speak to uh our seminar about a topic we're all really interested in which is NLP uh in finance so for the purpose of uh predicting Financial outcomes of companies this uh you know at ifem we've done several projects in this area and it's interesting how it these projects are either so simplistic like counting words and you know you know comparing numbers that word counts to something that resembles like a futuristic AI you know uh concept and so there's a very broad range of things and uh to be honest it's a bit overwhelming so I'm really happy to have you you sort of decipher a lot of these topics for us and uh um I look forward to the talk so welcome yeah yeah hi everyone I'm very happy to be here to share my research on using NLP to analyze earnings called transcripts and it feels so good being here because I haven't been on campus for a few years right you know we walk away even so it's happy to see all the young visits and also share our research with you um basically you know companies use earnings calls to share their financial and business information to the investment Community an analyst typically don't end already through the earnings earnings called transcripts to decipher the information that may impact the company's performance actually their stock prices but if an analysts need to cover a few hundred companies or even a large Universe then it wouldn't be efficient to do it in a manual way that's where we can Leverage The Machine learning AI techniques to automate some of the work to ease the human labor right and to to increase the efficiency and basically the natural language processing techniques in op has been proved very efficient in analyzed financial documents there's literatures both in Industry as well as in Academia has proven that distract those information to help formulate efficient trading strategies so our research also confirms and also get the same conclusion and today I want to show you a few things we expanded first is typically if you read papers they tested on your U.S large cap but we expand our research on different universities cap there's also U.S small cap that the world index as well as Emerging Markets and the second point is that um you know transcripts are usually considered as a whole but when we do the analysis we actually to our signal generation on different sections the individual sections as well as the combined sections which I will show you more details later so um and in the slides I will also show you as Sasha introduced you know there's a simple back or words approach there's also nowadays the state of our Transformer approach so I will show you a systematic compulsion about the results the context driven outperforms the background words naive approach so basically uh you may already know that uh what the earnings call is so that's by regulated the public components need to disclose their financial information to the public after an earning call ends they typically get the transcript from our vendor within half an hour and later they will have they typically call this spell check the copy may contain some of the errors like you know spelling errors or some other tagging issues later they will release edited copy which is more accurate and even later maybe in a few weeks they will release audited copies based on their sample checking as well as feedback from their clients when they receive the transcripts whatever transcripts we receive we will put them in our NLP Pipeline and generate the NLP features then save those features back into our database so the quantum researchers all the fundamental analysts can use these signals and their offer generation so here I give you uh some basic information about the data set so here it shows the number of components across regions you see that uh the data are from whatever we examine or do the back testing from from 2010 to 2021 but they actually have transfers back to 26. but those earlier years the transcripts have much lower coverage especially for U.S small cap that's why we start from 2010 and for the United States and Canada the number of components covered are around like a fossil then and it was pretty steady from 2010 to 2021 but if you look at other regions like Asia Latin America and the Caribbean even Europe so it increased gradually so there's more confidence captured in recent years than earlier years and as I mentioned for us it is regulated they need to release those information quarterly so you'll see Iran for southern companies or 5000 companies there's a four course each year right so quarterly so that's why you see around 15K calls happen for the United States and Canada but also the U.S especially for Asia if it is not regulated it's much more sparse the data so recently we are doing the similar analysis on Chinese earnings called transcripts and we see Chinese companies typically whole course um you know twice a year or even once a year so this is the total number of transcripts we have as I mentioned before for the same call there's different copies depending on the quality and how timely the information can be released so it's typically three or four copies for each call that's why you see the total number of transcripts uh are up to like 60k for the US and Canada and in total our database till today we have around 1 million transcripts uh we processed and we've seen all of the information into the database so this is a basic structure of the earnings call or earnings cost transcripts because an early score is composed of two sections the presentation section and the Q and A section the presentation section is solely just by the participated by the company Executives they share information about you know the business or the financial or the operational uh related issues about this component in the Q and A section the company Executives interact with analysts to answer their questions to give immediate answers to address some of the concerns so then we can form five in the video sections the CEO presentation the other executive presentation the CEO answer other executive answer all the analyst questions they carry different kind of information later I I'll show you it turns out the analyst questions typically to be more objective and you can easily you know uh this is more very intuitive to understand because company Executives maybe try to convince better information about their company right so we generated NLP features on each of these individual sections as well as the combined sections yeah yeah in your opinion I see uh for this specific research we only do the English transcripts which are original English and it doesn't involve in any translation so feel free to ask any questions in the middle you know do you have any questions so we also generate the feature on the combined sections means especially by the uh different type of speakers like the executives or the all Executives or only the CEOs or only the analyst part so here's the three categories of NLP features regenerated um basically they fall into three broad categories the first one is sentiment because sentiment the features are very widely used in financial text analysis and the uh and it is divided into two categories one is the dictionary Beast the other is the context driven and for others uh it has the accounts uh for example the number of words the number of sentences and we have some Co characters and also readability scores so here I'll briefly introduce uh those three categories later I will give you example of at least one feature for each of these categories so here I show you our back testing method this is a very simple back testing method we use to screen all the features we generated because there's more than 200 features generated and we use these to find the features that's robust and also good perform could perform which can be used as investment signals and um they not only like do a we don't want to do in a purely data mining way like you know because you could randomly get a signal that works well but don't really mean anything so when we screen the signal we also uh consider some of the fundamental analysts and to economic intuitions as well as some priors we get from papers back to this back testing method basically and each at the beginning of each month we will look at all the features for all the components in the University test and then rank all the components into quantiles by the feature values for example if it is a sentiment features the components that is the highest sentiment features will go to q1 and the components with the lowest sentiment features will go to Q5 and then we will track their monthly returns for each quantile at the month's end then this back testing is from 2010 to 2021 then we will calculate the basic Matrix to evaluate the performance like the annualized return annualized wall the irsr those kind of metrics so that's basically uh a way we do the initial screening to see whether there are some meaningful signals in terms of all the more than 200 features we generally so here I will introduce you about the uh feature and their performance in each of the categories and the first category it is the simple word count this is the most most basic background words approach basically you know how long is each section how many sentences there are and here you see for this specific this is just a a part of the uh Speech from Tim Cook in one of his uh earnings course 2019 and there's a 60 words in need and three sentences so it's very easy you can use a lot of you know even even you don't use open source packages you can use Python to write the code simply and it turns out for this workout and uh here is I show you one of the features the analyst question word count so the prior for us is that the more questions that analysts asked the probably more interested in the component and it carries more investment signals so basically we would expect q1 have higher return because q1 is the and these questions with the longest number of words right and Q5 are components for the analyst questions that has the least number of words so it turns out it's generally uh consistent with our prior q1 Q2 generally have high performance and a Q3 qq4 Q5 the performance is lower for the US large cap U.S small cap and World X us but it turns out em is um exception because yeah you see that more questions were uh worst performance and uh uh less questions better performance so we don't have a good interpretation about this uh it could be this signal is noisy it doesn't really robust across different Universe it could also be for em we have much lower coverage for the U.S large cap the courage uh are really good we have around 90 percent 95 percent of the confidence covered but for em it is only around 20 to 30 percent and especially recent years earlier years is even only around 10 percent so it could be we don't have enough components captured and it also could be em have different cultures because it's you know Asia or Latin America compared to us Europe it's the you know emerging markets and the developer markets they have different kind of behaviors foreign for each quantile and you'll see it is consistent with the previous findings for the annualized returns and you could also see the five lines are not very differentiable from each other right it means this signal is just okay not that good so the next one is an example from the readability scores this is just for you to this is just a simple example to illustrate what a readability score is and from the name you you can easily guess readability measures how difficult it is to read through the text to understand it and a short simple sentences definitely has low readability score which is easier to understand and very complex long sentence have a higher readability score which is difficult to understand there's different metrics yeah I mean will you showing this between the model you need to label this as to be how to calculate the readability scores right yeah uh that's what I'm going to see there's different metrics you can directly use this is released and they typically have two focuses one is the number of difficulty words some of the metrics they are based on a list of typical words and you know only higher educated person may be exposed or often use that words so and another is the sentence lens like how many sentences or how many syllables it also captures how difficult we understand the text so there's a package called a um text stats uh it's a open source package and Python and you you can easily you know uh clone it in GitHub or just people in store so you can directly calculate around several of them and we have some customization of it based on our usage but basically we follow that logic so here's a readability score on CEO comments so uh the CEO speech as well as the CEO answer so the entire CEO section and our prior is that if it is easier the CEO speech is easier to understand so it probably the information delivered by the company senior executives are easily to incorporate into the market price and they are transparent and they lead to good performance but if it is very difficult to understand the CEO comments you've probably been they try to hide some of the bad news or want to spill kid the audience in order to hide something about their operations so and the back testing results are also generally consistent with our priors you see for uh U.S large cap your small cap Vortex us as well as em uh q1 usually have the lowest performance yeah do you move over the entire set of companies or relative to some other benchmarks thank you for your question yeah I think I need to clarify that relative return is just the annualized absolute return minus the annualized The Benchmark return for example this is each Universe U.S large cap it actually matches to the Russell once all the large cap components and and then we just extract for the it to track the excess returns yeah before uh higher readability scores means difficult you can think about readability score as the number of years education needed if the higher the score it means more difficult to understand yeah uh so it's it's kind of the patent but you see there's definitely not linear and you see for us large cap Q3 has much higher annualized relative returns than other quantiles and for U.S small cap it's not very differentiable the five quantiles right uh we have a question online as to why do you do this method of different talents what's the intuition why do you like a screening they typically use because we have too many features we want to evaluate so we want to come up with some metrics and this is a very basic method once we have some recommendations about the really good perform the features and also consistent with the economic priors we would recommend to the investment teams and they will use more complicated like back testing strategy as well as simulations to to to to to do more testing instead of just this quantile screening so uh here's a dollar growth of the five coin tiles based on these three scores on CEO comments and you see that for the U.S small cap the five quintiles are very close to each other not very differentiable right so here is the sentiment scores yep how do you decide when to read balance since companies report their earnings at different yeah that's a good question uh we do it monthly monthly rebalance and if we use transcripts that happens within the past six months we use sorry the earnings course that happens we see in the past six months we also tried a different the key strategies uh it turns out like uh you know it's generally noisy some features with sticky is better some features it's original is better so we just here is a simple results just Service uh past the six months cut off of the events used okay for a company the relatively any kind of normalization that you do uh we don't this is uh this is just by Universe all the companies ranked and uh these so I sector neutral like you said it's sort of normalization by each sector the results will become bigger in general yeah especially I think for the U.S large cap the past decade a lot of large cap components that works well uh Tech components and the year they have very high sediment if you do sector neutral the selection of them would becomes less and then it turns a little bigger results so here's the cinnamon scores and if you remember we have two categories this is the most basic the background words approach the dictionary based and uh there's different dictionaries you can use and how what I before is very generic dictionary that originated from the psychology research so it is not Spanish specific and the reader is more used on social media as well as short reviews because it has a very special dictionary they even have emojis and name probably you've already heard of it it's kind of a very popular dictionary used in financial research the two professors you read it the dictionary at least are words from different categories like positive negative and a Superfluous and litigious and so on from the filings the annual fightings of 10K and then after that it's the original research was released in 2011 and after that in a few years they updated the dictionary gradually I think the latest version now probably been the 2020 2021 and we also here's a very illustrative and simple example about how you calculate sentiment using the dictionary based approach basically you will count the percentage of positive words and then you count the percentage of negative words then you just subtract subtract the two to get the sentiment scores so we also have the proprietary dictionaries we collaborated with our research analyst to customize some of the specific words these things that can convince some of the sentiment information so we've implemented in our calculation and here is the CEO sentiment so this is the sentiment score based on the LM dictionary and uh uh calculated on all CEO speech both the presentation and the answer part and it turns out uh it carries more investment signal than the previous two examples I showed you especially on U.S small cap for U.S large cap there's a little like a noisy because q1 turns on not being the highest but Q2 is and for us small cap is very differentiable about the five quintiles there's some noise in uh the word X us for em also but generally the pattern is consistent with our prayers that higher sentiment leads to better performance so here's the dollar groups of the five quantiles across the four Universe for these specific features used sentiment based on LM dictionary and you'll see compared to the previous results those five lines are more differentiable and if you look at the absolute like dollars especially for U.S small cap it's much higher than before right so and based on evaluation of the different type of speakers we also found that um they they have different patterns this is just a simple mean sentiment across or U.S large cap and uh based on the LM dictionary and you see the blue line is the co sentiment and the yellow line is the analysis sentiment CEO sentiment is much more positive than the analysis sentiment and you see the big deep between it's it's in 2020 June that's you know the components begin to release their performance in 2020 q1 which the um typically where performed during the first few months of Kobe so you lead us to think that analyst sentiment may be more reliable indicator than the CEO sentiment because CEOs may try to convince the good news to the public than all the news and also they may use specifically now you know people are all aware of those machine analyzes of their speech so they may purposely select some of the words based on those dictionaries are available and then uh distort the analysis results towards their speech so here's the dollar growth of the uh CEO and Analysis sentiment based on the LM dictionary it turns out in the earlier years it's pretty similar but in later years the analyst outperforms the CEO's sentiment so now we've covered the dictionary based approach now we are going to check the context driven sentiment so we specifically use one of the Transformer models called Birch so it's a birth represents bi-directional encoder representations from Transformers so if you use Transformer models you know that Transformer models are composed of the decoder and encoder and there's different type of Transformer models we specifically use bird because we focus on natural language understanding instead of natural language generation and this bi-directional basically refers to when they pretending the birth model the the surrounding information on the left and the right are used for predicting a specific word because in birth when they pre-train the model they actually based on two tasks to create the loss function one is called mask language modeling LL MLM and the other is called next sentence prediction and MLM has the pan over standard for pretending the Transformer models uh basically what it means is for the whole Corpus you will selectly mask some of the words and then use a strong information to predict it the Assumption behind that is the meaning of a words can be inferred based on the context around it right so if you look at this example I am going to give her so this is a fill in blanks questions if I don't give I only give you the information before the blank I'm going to you have a lot of options to put here you can see I'm going to work I'm going to uh home right and uh so you even don't know it is a verb or it is a nine right but if I give you the information behind it the paper you you definitely know it should be a worm I'm going to read the paper I'm going to cut the paper so that's why the probability to get the correct must work predicted as well as infer its meaning is higher if you post on based on both directions information and the second component is called the encoder representation so Transformers are composed of two parts the encoder and the decoder encoder basically takes the original means for for NLP it is definitely a sequence of words it could be a sentence or a passage then you tokenize it then through going through the encoder layer you will get the hidden layer as output which is the which is usually referred to as the embeddings that's the numerical representation of the linguistic characteristics both semantically and synthetically about the original sequence the sentence you put in right and the decoder is generally they will take the embedding the offers from the encoder as the inputs to the decoder decipher it then you get your target output for example in motion translation you need to translate a sentence from English to Chinese then your encoder will encode the information from the English sentences and your decoder will translate it into the Chinese sentence but here we only focus on lateral language understanding understand what the original input means so we only use the encoder part in birth after that you would do fine tuning means you would have labeled sentences you already have numerical representations of all the original language and the English language you use then you use some you will add a downstream task which is a classification model so that needs some number of Labor sentences with the sentiment labels it's a you know positive negative or neutral then you can create or fine-tuning a sentiment classification model so you get the model output You can predict it all the sentences on the text you have here the earnings cost transcripts that's basically how the bird model Works basically um the embeddings you get from the burn model it can represent the information from the entire sentence and um you know you the patrolling takes some time and needs some computational power but nowadays there's a lot of open source and already good quality pre-trained model you can directly leverage you just do the fine tuning part which is much easier right yep I just want to call you the label the sentiment actually have three one of it is publicly available it's called Financial freeze bank and there's uh they have five thousand sentences and they have 16 annotators to to label them as uh positive negative neutral and we only use around 2200 which are all 100 consistent on the labels then the other team we self labeled a little more than a southern sentences from earnings transcripts they also add some other type of sentences like writing sentences to specifically Target those neutrals from the earnings cost because there's a lot of greeting sentences learning cause transcripts yeah that's basically how we do it at the end we have Arana 3300 sentences for fine tuning so here is a uh also illustrative example about the sentiment on the sentence level uh when the comparison between the the dictionary based sentiment and the context driven the bird bird-based sentiment so if you look at the first sentence uh by the way the sentiment score ranges from minus one to one with minus one means it's very negative one means it's very positive and zero means it is neutral so the first sentence thanks for taking the questions question is a lucky word in the LM dictionary because uh um in the LM dictionary could read it from the fightings and fightings if you see something question or questionable it is problematic means so and there's no positive words in this sentence that's why you get minus 0.20 in LM dictionary as its sentiment but if you look at the sentiment from the first model it's 0.07 so it's already captured it is a very neutral sentence and the next sentence nice quarter because it is in the earnings cost there's questions part so analysts could be used very speaking language and uh nice or quarter doesn't carry any of the sentence sentiment information from the LM dictionary but the bird-based model captures it is a very positive sentence it's a part of Visa sentiment as point 72. sentence and outputs a number but that number is not sentiment if you use that number that the birth model outputs to train us on classified so there's a yeah generally like that uh this per model is embedding which is a vector to represent the meaning of each sentence you could think that Vector as a features for this sentence then you have the classification label the result and sentences and classification label then you can create a classification layer which is at the end becomes a classification model yeah and it all puts uh sorry yeah go ahead uh I think you just mentioned you use around 3 300 sentences for the uh the the fine tuning part so how much difference do you see uh by using these uh these examples to to fine-tune the birds compared to uh just using the bird without the fine tuning the the original Patron birth model it doesn't have the classification layer so uh it only gives you the final embeddings to represent the original sentence so you can sync the original pre-tran birth model as a feature engineering layer so now you have features you need to have some labels in order to have a classification model okay yeah but yeah sorry go ahead before but if you use the classification layer on top of it does that mean that can only understand what puts similar or like the one called classification like yeah that's a good question uh the pre-trained bird model it was trend on very generic copas it uses all Wikipedia English text as a as well as a a large book Corpus text so means the aim of it is you know in now nowadays especially nowadays the state of our Transformer models they try to build a universified model that can do any type of task means the model is more like human beings we we have the linguistic knowledge we can read English but it is in finance and sociology or in biology right so that's a blueprint model aims to do but it turns out generally generic model wouldn't capture some of the very specific domain like sentences about so you can use the label sentences or financial text to fine-tuning the original model to make it better you read it for financial sentences that means only the words that were indeed and the in the yeah they call it in tokens it's a little different from words but in the pre-trained model they have around if I record it correctly they have 35 t uh tokens means they can they even can capture they call it World PCS so some combinations of the tokens Can generate a words that you use so it is a large coverage of the vocabulary in English yeah how does it perform on sentences with both positive and negative words yeah um so uh that's interesting um I haven't thought about that but I do see uh I do see some examples like uh yesterday I I'm reading a transcript it says you know uh they suffer I've got the original one but the basically the sentence I seen in the first half uh we suffered from the supply chain issues nowadays but if our performance or our Revenue still increase like you know 10 compared to last quarter it turns out it is a positive sentence yeah I think you know as human beings especially analysts who are very experienced at reading a specific sector or understanding it they may have a different view that's possible because 10 maybe compared to their expectation it is lower right but from the sentence itself I think I uh it makes sense even if by a classify it as positive despite tuning of this birth model in this classifications that do you typically get a high accuracy or has quite scores or what do you decide how do you decide it's doing an okay job yeah yeah yeah that's a good question it's difficult to have a a hard threshold like you see you know a 90 compared to uh 92 percent accuracy or F and score what do you think it's see the Publications in Academia it is a big difference they think it is very different it is it is worth a new method right to be published but and practically when you do the classification on the large number of the earnings called transcripts may not make much difference because especially we have the sentence level sentiment when you aggregated to the section level use a mean or standard deviation to capture the dispersion this 90 percent accuracy and 92 accuracy may just end up very similar performance so it turns out our model is actually around 90 accuracy on all sentences or whatever for the SQL time we wouldn't race through the following two sentences so here I'll show you the context driven the verb based CEO sentiment currently you'll see the LM dictionary business sentiment here is the third context driven CEO sentiment and a very clear difference you'll see is still on the U.S small cap you see the difference between the q1 and Q5 is also much larger than before right much differentiable and it turns out uh the sentiment signal really performs very well on the U.S small cap so they even on different sections and the different combinations they generally are very robust this is also a signal we recommended the investment teams to use so here is the dollar growth of the uh based on the CEO sentiment for the dictionary based approach and the context driven approach you see the context driven approach which is the yellow line outperforms the online and this is just the difference on the performance of q1 minus Q5 so here's the NLP features for the information ratio for all NLP features we have across the four universes so basically there's a 280 in total of NLP features and this is the IRS of q1 minus Q5 the Longshore strategy so it is marking neutral and if you see uh on U.S small cap it really performs very well around at least 30 percent of them have the IR above 0.5 so 0.5 is the generally reference number when we look at the IR and uh on U.S large cap similarly is there's around 20 percent and a four or X us there's around 15 and the lowest would be on em it is around 10 percent foreign the content of the research I want to share with you and we actually published the paper which has more details about the comparison especially on the results part and we think those NLP features are very promising for uh to be used as Quant signals to form efficient trading strategies and uh um you know companies also adapting their semantic speech to adjust uh to the machine analysis on there reports or documents so it will be uh there will we are doing more research in order to adjust that and also uh we are doing some data augmentation as as you ask Siri Southern and through 100 sentences are kind of okay to be used but there's definitely room to improve so we are working on data augmentation which we tried a few methods uh which I don't have time and so I don't include here but if you're interested we can discuss that part after this any more questions yeah if there are any questions here maybe we start with that I have a few questions on the laptop if you need to think about it for a second um yeah we've got I was wondering how are the features you created correlated with the traditional earnings data from financial statement and how do you test whether it contests more information thank you uh I don't really giving the question because um on the video we don't hear it so the question is how the NLP teachers correlate with the traditional Quant features uh such as the fundamental features as well as the Chrome features uh so it turns out in general the correlation are low especially for readability and accounts for sentiment it has some medium correlation around 0.54 us large cap with momentum which makes sense you know okay I had a question about how you measure readability you mentioned some packages I don't know if if you can the package name is taxed stats they have different metrics you know uh the canyon frog or or the command Leo there's a lot of Iran they have around seven or eight published they also have their customer customization of overall readability and we changed some of them in order for our usage because based on their scores some are very extreme so we just did some normalization yeah so I have a few questions about the return side of the equation the not the quintiles but how you do the returns so are we talking about um a few days before the earnings or is it after or you know how long you know can you describe the beginning and the ending period of the return it is a monthly rebound so we are tracking one month's returns and it could be a few days or just you know around two days after the call and it could be uh as long as you know three months after the call because the earliest uh we've always used the latest call the latest information available before the rebalance day okay but never before the call it's always after yeah always after that racing you don't hold the company until you either see positive or negative side of it and then you go long short depending on that sorry could you repeat your question I'm trying to understand the implementation are you saying that you wouldn't hold a company unless they have an earnings call positive it's making you go short yeah and for for us large cap they typically have called quarterly so most of the companies are included but as you mentioned especially for emerging market it has a low coverage so it would only be the cost that has the latest information available included in those uh quintiles yeah so I have a question online about the potential for CO2 Arbitrage the words that he's using or or the hair so like could you explain a little bit if you've thought about this or if it's um something that you think um it is from the chart if you see the Blue Line it goes from around one to about 1.5 before the covid so there's a trend based on the LM dictionary the CEO sentiment becomes gradually more positive the next element yeah okay so there's evidence that it's happening okay um uh I have a question about Finn Bert versus Bert have you tested both or did you choose which one did you end up using yeah I need to clarify that so Bert is a generic model pre-trained by Google um uh Wikipedia text and book helpers so the fem bird it's it's called it's sort of a generic term it's basically you know the uh kind of bird usage in finance but there's different versions so the in our paper there's this then we come up with the method we actually refer to a paper the author's name is RAC and it published in 2019. so he used the birth model and using the financial freezebank sentences to fine-tune you need then call it member let me come up with our version we call it in Denver and there's a few professors in Hong Kong technology University they actually returned the birth model on financial Corpus specifically you know filings earnings as well as news all related in finance they also name it as member so you you can think of as the financial usage of the bird model now but there's definitely uh need to be I think one need to be differentiated is the pre-trained version please find tuning or not well the the more you answer questions the more new questions appear so I guess people are very excited um there's one about um it asks how do you convert bird fine-tuned sentiment classification outputs to scores so I guess classification would just be a binary good or bad sentence and then I see does your classification model basically it could be binary it could be multi-class right so when you when they fine tune in the model the labels we have are three classes uh positive negative neutral so it will output probabilities and it will add up to one like a positive 0.60 negative is 120 and neutral point 20. then it will be uh you can do a like a voting or the the like Max class yeah so you're asking about the DT right uh maybe for this it depends it depends on the signal actually we did some uh examination on the Decay some of the signals it could last around four months like yeah some of signals like one or two months it's gone and when we do event analysis based on the transcripts uh release date or the earnings score release date it's typically two weeks but that's for more frequent like rebalance or inventory yeah [Music] that's a good question so the question is whether there's more we can dive in about the Alice questions right the different persons different components there's a paper found that when there's more buy-side analysts that and ask questions it carries more signals means it performs better but they don't have the data to differentiate from the cell side analyst and the buy side analysts so we just treat them all as analysts okay so you mentioned the 90 percent accuracy of um I guess classification of the sentiment with Bert uh have you looked at differences from sector to sector or are there sectors where this works better than others that's a very good question uh definitely the keys the for some of the easily understand or generically like uh you know Layman person can understand sentences and you already have very high accuracy for example retails or even Tech components is good but if you look at some specific sectors like communication or Healthcare it usually a little lower and we also do some topic modeling and extract very topics specific sentences and we found that this the those the accuracies on those sentences will be much lower only around 60 to 70 percent that's why we want to try data augmentation and working with our analysts to specifically increase those topics specific performance the the sentiment yeah yeah okay one more question there yes thank you uh I have a question regarding the data collection so I think the original data should be some species should be audio so you need to you need kind of need to convert it into transcripts so how do you do that you are you asking someone to take a downward you're using some algorithms to do it automatically yeah that's a good question and I have a short solution we use vendors so we use smt and it turns out or it's a pretty high quality like a vendor data and about transcription they do try we do have transcription like uh Services uh our team provided to our collaboration like sbos and uh there's uh there's vendor Solutions and now we are experimenting these uh package or a model called whisper they released by open AI so you know openly are very good out training those large scale Transformer models it turns out their audio qualities are pretty good when it turns to be transcripts and they even do multilingual transcription means you'll have your original video mix of language and they can output the English for you English transcripts thank you okay well I I think uh we'll call it a day because we could spend another day answering questions and thank you again [Applause] 