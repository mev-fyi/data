okay hi everybody um i'm happy to welcome you to our first cornell city seminar of the year um today i'm very excited to have charles albert as well as sylvia ruiz giving a flash or lightning talk towards the end of the hour and so i put a few informations in the few links for you in the chat um in the chat room one the first one is we have a youtube channel where um this seminar and our past seminars reside and our future seminars so as they same youtube parlance uh go there and smash on the subscribe button and uh and uh get in touch with us um and um and this seminar this year i'm really excited because we're trying this new format we have a channel albert who pretty much doesn't need any introductions who's uh really uh an expert in quant finance in almost every arena i i'm become a friend of his in the last few years uh because uh he's a great scientist but also a great engineer and really cares about uh science that works and and has applications so he's a bit uh you know a bit like me has one foot in academia and one foot in uh in finance in the practice and uh i would have i would like also to say that his two feet are very firmly in each camp so um so anyways every year he publishes this really interesting quant finance uh calendar um where uh he talks brushes each month over the certain topics and i noticed that this year's calendar had a lot of uh topics that had to do with nlp and uh and i i'm really excited to to hear what he has to say to us on nlp and so he'll gonna he's gonna give us a 40-minute presentation afterwards we're going to have a five-minute q a so please put all your questions in um the q a box not the chat box if you'd like to just say hi and uh you know share your thoughts on your on nlp or your contacts that then you should use the chat but if you have a specific question please put it in the q a and after five minutes of q a we're going to start with sylvia's talk which um sylvia has done and just graduated from cornell financial engineering last year and was involved in an fe project with rebellion research and this project was had to do with scraping a lot of uh language data and applying nlp techniques to it and uh she has a github repository that you can also find in the chat room um and i'm hoping that this will help a lot of our community get started you know one of these nlp is a very very exciting topic but it can be feel intimidating to get started because you need to scrape things you need to apply some packages and and we hope that after today's uh uh talk you'll have a lot of tools to do that so without further ado uh q a if you have a one sentence section uh question for our speakers and chat if you do really just want to chat introduce yourself and and meet other people who care about nlp so without further ado okay thank you for the nice introduction sasha and for the invitation so uh so i will talk today about nlp it's not something i'm very familiar with even if i started my first nlp project uh i think 30 years ago now but it was far more i mean simple than what can be done today and uh and in fact in finance it is the first time i'm i'm discussing that publicly of course at cfm uh my team is using nlp for more than one year now also we we have in production some predictors and some strategies related i mean using nlp and but here uh so after a short introduction about the context uh i will talk about a giant work by uh mengedar who is here with us and and so it's an ongoing work so do not hesitate to ask questions to comment to send me in the chat some links or any papers that i not aware about or you think i should have more used in this work please do it so yes why trying to use nlp in financial markets first of course when you do systematic uh strategies for investment uh maybe you started 30 years ago using technical indicators or moving averages and on and you were capturing things like cycles inertia of governance of companies things like this and but in fact what you try to capture is information about the companies so but what you've got this information what is important is to be able to bet on that and so you are going on markets and then the magic operates i mean you you push the price somehow because you want to buy you push the price up because it's a dual auction game it's something that channels very well and uh and so via this kind of market impact the price will change and so you've got this information this is a link very specific between the information and returns and now with nlp you can try to immediately access to information as it is not to numbers like balance sheets of companies that maybe 20 years ago and the network you started to use then you use i know supply chain data that are always descriptive i mean there are variables that are describing some economic values for companies uh you are trading here is something that is a little bit different because it's somehow pure information it's text it's speech that is converted transcripted in text and so this information how can you use it i think that it's important to try to understand what you are modeling because usually especially nowadays with nlp you've got so many parameters that you've got a high risk of overfeeding so what can you hope first you can hope to understand very quickly an information maybe a little quicker than other people and so you've got a speed advantage and you will start to buy before other people are buying and so before the price move up and so the speed advantage you need to be very accurate and very quick on something that is quite specific and i would say a very uh localized in time and you've got the second aspect that is you can be roughly correct but on a lot of text containing a lot of companies so here you will place a cross-section of this information so you don't need to be that accurate you just need to be sure that when you run companies according to what you captured the ranking is correct so uh the companies that you put on the top of the list are going will will go i mean more up i mean or will have better returns than the companies that you ranked at the bottom of your list and this is probably here where you've got a lot of things to do with an lp because you've got a lot of possible texts uh sylvia will talk about edgar feelings but you've got it's a lot of texts you've got financial news you've got of course all the social media and you've got all the speech of the earning announcements for instance that are transcripted so you've got a huge corpus of text and even if you had people humans experts to read that they will never be able to come back early enough to you with the information so that you can take decisions using so many documents on so many companies so this cross section is a very nice way to try to use nlp and uh and so just to sum up a little bit with that so information we know for long that information is k and here is key and here we are we are trying to address the immediately the information directly or information not figures and numbers who are describing something something that comes maybe after or before these numbers but i readjusting what you what you could think about them and uh and so i mean this is a hope that we can have in using uh nlp on financial markets and of course a lot of people are starting to do that so you've got a lot of academic papers and and for instance here of course most academic papers you've got curves that are going up okay then i'm not saying that they are not good i mean most often all these papers are very properly done and but the way is okay you take a corpus you try to identify words lexicon either via supervised learning like in this paper so here you try is they try to uh to make a i mean a correlation between some terms appearing in texts and the fact that the return of the companies are going up so you you supervised your system on the returns of the companies and then you design a strategy a portfolio and you've got numbers that are going up here again another way to try to use this time a lexicon to identify texts that are positive or negative and to build portfolios then there is a lexicon that is now very well known the longhand mcdonald and i will investigate on that in few slides and you go to also all the paper you see that are using this lexicon to try to uh to infer positive sentiment on on stocks and negative sentiment on stocks and you are long of the positive ones and short on the negative ones and you hope on the cross section to make money to have something that is predictive out of that okay and when you zoom off what you are on what you are doing uh so in some papers people are really trying to comment and with i mean human analysts to try to understand the structure of the sentence to identify that the the words that are positive or negative okay this is something that is like a really lexicon based but of course and we will talk about that in a few slides you've got all the embedding word embedding based methods that are more i would say self-supervised and that start to be used also to try to uh to build predictors on financial markets of course all the big providers of data like bloomberg so here you can recognize gary and jiden who are trying so on at bloomberg who are trying to uh to link information texts especially when you work at bloomberg and or reuters for instance you've got a lot of text and you try to link the text with events on on the prices or on volatility or on volume traded on stocks and you've got companies that are really i mean specialized at traven park i remember the first time i've heard and i discussed with people from raven park i think it was 12 years ago and and so they are building for a long nlp system to try to uh to give a package sentiments i would say so it's something that is highly uh used in finance and now that you've got all the buzz on ese you just type esg and natural language processing on google and you've got a lot of pictures so a lot of people are using these methods either they are lexicon based either they are because of course you've got a lot of attention on bert and this kind of algorithms either you try to use these kind of algorithms and but uh then the point is uh what can you capture and i think that what what i try to do and when manda mangda joined me to try to do this exploration is we will try to understand what can be really understood what can be done with data from from a scientific viewpoint so i will not show you any p l curve i will not show you any i don't know sharp ratio it's more how to combine lexicon based systems and embedding systems on a financial corpus and what are the what can you hope for what is possible why is it somehow important it is that on on the one hand you've got all these lexicon based systems on the other hand you've got embeddings and what you would like to do is to combine them and so because uh these lexicon based they are most often built by humans who have ridden i mean it's like an analyst who annotated a lot of text and then you record the annotation and you end up with something that uh saying that is saying that when this word is in a sentence it's a good news when it is in another when another one is in some sense it's a bad news and on the other hand you've got embedding so embeddings it's more like a modeling of the probabilistic context of what i will say i will say that with a more mathematical mathematically in few slides and you are just capturing this kind of uh probabilistic conditioning of words once by the others and how can this approach work together and uh and so here uh what we will try to do is to try to explore how it can go together and uh and why why doing that so here is more or less or planner so we try to understand what by nature world embedding can capture when you look at the loss function when you try to understand what kind of structure they are able to learn you will see and it's a concept that will be very important for this talk that they are trying to capture somehow synonyms and and these synonyms i mean it could be good if you can capture synonym because you reduce the complexity of a text instead of saying uh this is great good wonderful you just have one token that have the same the same sense and so you can use it in an algorithm but you will see that the way embeddings are capturing synonyms and structure of a language is not that semantic driven meaning that they can easily not be able to make the difference between synonyms and antonyms i mean words that are exactly opposite but uh there will be a it will be very difficult for a system based on embeddings to make the difference between them and so now if you think about mixing that with lexicon it's a big question because if you want to inject your lexicon in a system that cannot make the difference between antonyms so you want to understand the polarization of the sentiment good news bad news but your unbendings are not be able and not able to make the difference between good and bad so how what hope can you have and it is what we are trying to explore here okay and uh and doing that uh we want to do like a synthetic experiment that is something that i didn't saw that much in nlp so uh that four we are trying to build a generative model of a text and then we can control the structure of the text the structure of the cementing and see if we recover what we put in the text and i mean you will be surprised it's not that easy to recover with this kind of standard techniques what you put as a structure in the language okay so um so as i said it's a joint work with mengda who is here with us and it's motivated by more than one year of work at cfm by the team i am managing the data analytics team with more more or less our data science team at cfm and so uh people like uh silvan from puenoes and charlie granto elise and augustine i mean discussed a lot with us and we discussed a lot together about how to do something with data for cfm that is an hedge fund okay so the my plan here is to uh to go through some theoretical aspects and uh it's something i wanted to do uh for long and thanks to mangda i succeeded into uh going forward in that then this idea is about uh trying to understand uh if it's easy or not to recover something that you put in a text so thanks to this small theoretical aspect we can build language synthetic language with given structures and so we can see if we recover the structure or not how difficult it is and it is just a question of identifiability a statistical question of inventive ability and then we will use a big large corpus of news financial news and try to apply these medals to see how it works can we recover different polarities of four sentiments using embeddings okay so a little bit of theory so if you so we will mainly in in our work we are using uh the word 2x keygram method but you will see that according to me at last is quite generic because it's really a significant of a lot of embedding driven systems but first just describe a skip gram so you probably know already this picture i don't like the picture for neural net for long i prefer formulas so in fact it's quite simple okay you've got a word i mean you've got a v word in your vocabulary and so if you want to uh to question your your what to work with a word you put it as input so just you you take in fact uh one uh the corresponding element of this matrix that is a matrix of embedding w and it maps your word to a row that is uh in fact the soft max of transpose of x times w times w prime so this is a row of a stochastic matrix okay so this is a row that zooms to one and the elements of this row are simply the probability to see another word one of i mean so it's a row of size v the size of the vocabulary and it at each coordinate you've got the probability to see this word in the neighborhood of the word you you questioned okay so uh so if you want to know the likelihood of the appearance of x j i mean the the word number g j around the word number i just compute this soft max okay and uh and so it is done very simply i mean it's a low rank decomposition of the mat of a laurenc the composition of a matrix that usually uh nobody knows and this matrix is not really written in usual papers about about embedding so it's quite simple you can rewrite that as a neural net with just what in the layer and a soft max output but it's simple you've got a v times n matrix you enter with one word so then you recover one embedding this embedding you multiply it by the context matrix you end up with one row you apply a soft max so that it zooms to one and each coordinate is a probability that this word number 210 is usually in the neighborhood of the the word you uh you put as input okay and in if you use the softmax keepgram in fact what is important is that what is a neighborhood is just that this word is inside the next capital c word after after the word you started with the input okay it can be a little more complex but let it keep simple and put it it's like that okay so now i will claim that it's not it's it's quite generic because what what can you do more than that okay you can put more than one word as input okay but in fact putting more than one word as input is just increasing the dimension of the input space instead of uh having a one word you've got a pairs of words i mean tubes of words i mean sequences it's not that different it's just the dimension is higher then you can try and it's very important to localize those two matrices okay the w and w prime saying that in fact i will take another version of this matrix another version of the embedding another version of the context if i am in a given surrounding in a given context you know given like uh i saw these sentences around me and it is in fact what attention heads are doing what bert is doing okay just being more local and instead of having one large global embedding you've got a lot of local embeddings that are addressing context and you do that just being more non-linear in fact those two items are the same if you are more non-linear it means that instead of having one big embeddings that is flat one big context that is flat you've got a non-linear one and so when you combine two words it depends of the context the outcome depends at the end i continue to be convinced that in fact if you could do i mean it has a sense to say that there is a taylor expansion of birth in the space of embedding and probably if you if you would compute the taylor expansion of a bet in the space of embedding you will see a world-to-week nothing more okay and so what we are talking about here is simple but in fact very generic in the kind of understanding we can hope to have if we understand the unbeatings so now uh just to uh put it clear in terms of formula so the loss function when you see a sequence of words now with a word to vector skip gram from you start with a x k capital x are the words uh occurring in the corpus so x capital x one is the first word of the long document and so it's a word number k of the long document and then the sequence of other words and so what you want to have is an estimate of the probability so this estimate is parameterized by w and w prime of seeing after capital x k being the word number i of your vocabulary so you see this sequence okay and this is just a soft max so you multiply the numbers because uh for the word to wake it is an iid setup and so you just have this expression to understand the likelihood of of this neighborhood and it is the loss function that you will try to uh to to optimize a concept that is quite important uh at least we will use a lot this this concept is what we call a reference model okay a reference model is just the big hidden matrix that you never see okay it is a v times v matrix and the entries are the the words of the vocabulary and is simply the sequence of probability to see all the other words in the surrounding in the neighborhood okay so this is this big big matrix the advantage of considering this matrix is that you don't have any softmax on this matrix because it's already a stochastic matrix okay it is when you try to reduce this matrix via this kind of florence reduction that now you end up with a with a with a raw vector here that is not zooming to one so you've got to put a softmax okay and you've got a lot of different reference models okay there is a reference model that corresponds to a given word to back so you learned a model according to an embedding now if you multiply w by w prime and you do line by line soft marks you obtain this big matrix and it is the view by the word to reg of this true hidden matrix of the joint distribution of all the words but of course this joint distribution exists as soon as you've got a corpus okay you you could compute it manually it's just uh quite computer i mean cpu intensive and more intensive but it's fuzzy and more than that and that is the most important reference model it is the hidden one the one that generated the text okay if you see a text what is interesting if you want to model the text to build a language model to build an embedding it is not really this corpus by itself it is always structured so it is a real probability distribution of a joint probability distribution over the world so you've got the reference model corresponding to the hidden probability law of the language you are analyzing there is one realization that you can statistically compute empirically an equivalent that is somehow these lines that you will obtain are statistical estimates of the true one that is that is not visible and then if you compress you've got the equivalent but after compression so how does these three reference models speak together so there is something that is quite simple if you start with a true reference model a big matrix and now you want to have embedding so a compression and compress just one dimension if by chance you've got two lines two rows that are exactly similar if you look at the loss function what will happen is that it will just map the two words the two input words on the same output because those two it means if they've got if they are exactly the same rows it means that those two words have exactly the same uh distribution of words once you see them around them so in fact you cannot make the distinction between them so the best way to compress is just to map them on the same embedding and to reduce by one line this big w prime zero okay so this is trivial okay so what does it mean this triviality is just means that if you've got the compression the projection the association of words inside embeddings this uh word embedding they will try to put together words that have the same surroundings so the same neighborhoods the same words around them indoor corpus so it's something that you you would like to have and uh but it's not always wonderful okay if in your purpose each time you've got a sentence like look at this bad guy you've got also a sentence look at this good guy and if at each time you've got this is bad english you've got a sentence saying this is good english in fact it is you are not able to distinguish between those two words bad and good so a good i mean a good an efficient uh what to wreck it will put bad and good exactly in the same position they will have the same embedding okay so and it is what we can call frequentist synonyms so they are antonyms from a semantic viewpoint they are not synonyms but because if you see them in sentences in the corpus they are always in the same position they can be uh they can be substitutable they will uh your model your embeddings the medics you want as sophisticated as you want they will not be able to distinguish them okay so that's important to understand that and uh of course uh not all words so this is i mean if you think about that it is related to all these papers about ethics of nlp when you want exactly so you want to obtain that you want that the context of women and men are exactly the same and you you don't want any bias in their embeddings okay but here what we want is our embeddings to be biased for positive and negative words especially if you want to draw conclusions for financial investment okay another example and you will see that it's not that trivial because it's not because the sentence seems similar that you can have exactly the same embeddings so the colors for instance red and yellow okay you can have uh exactly the same sentence with red and yellow interchangeable but not always okay i don't know how many yellow apples you've got probably lower i mean not that frequent as red apples but for sure you do not have any uh red bananas okay so it means that uh inside the context an embedding that can use a context he will understand that yellow is not red just because if you are if you've got a corpus speaking about cooking you will it will be able to see the difference if it's a corpus that just speak about cars it will be far more difficult so there is a context context a very important room for the context and in fact we will see that for instance financial headlines is very difficult for them to make the difference between autonomous because these financial headlines they are like punchy lines and in fact they are like placeholders with good and bags exactly at the same places okay so i will go a little faster on that but let's say that it is possible for what to wear uh to generate corpus using a markov chain okay so you start with a markov chain you change it a little bit and then you obtain a generative model and when you structure the markov chain you structure the big w uh zero prime and so you can put structure in the text and uh you can uh you can really uh put it exactly at you want so you can also make some computations uh if um on the on the on on what to veg to understand what is the loss function and in fact you will understand quite quickly that the loss function of a word to egg when the text goes to infinity is in fact a cross entropy between the two distributions this big reference matrix model that we never observe and the one that is compressed you're estimated by uh by the uh the word to rank so this is really the kind of of metrics that you are optimizing and we know a lot of things about uh cross-entropy it's nice when you've got i mean small uh small dictionaries but with big dictionaries cross-country speed are difficult to estimate anyway let's go to the first experiment quite quickly so the first experiment we put either structure either on the structure so sat we we design synthetic languages with synonyms and we try to see if it's is this possible to recover that the synonyms are simply blocks you know big matrix and in fact so uh we see different so i will maybe first we go with that here we generate a long document on the vocabulary of 1000 words and we've got here a few blocks okay 10 blocks of 80 synonyms and here more blocks of less synonyms it means that the intrinsic dimension is lower here than there and effectively we see that the unbeatings are able so this is a learning rate okay during the epochs okay you see that the learning act is doing that is going down for an embedding of size 200 that is more or less compatible with uh with what you can expect to uh to have with this uh with this kind of matrix what is surprising which is what is true excuse me what is surprising is that uh their embeddings are very poor in identifiability so here just for the for the record we generated a long document over a vocabulary of 50 watts only so very poor uh long wager and when you've got a large embedding it is not able to recover this low dimensional space okay it's very difficult for it okay it's probably coming from all these random metric theory aspects that says that uh in fact when you are highly symmetric when you start it's very difficult to find a spike somewhere in high dimension and so here we we have more or less uh the information that you we wanted to understand i mean the compression capability of embedding send one and here what we do is that we compute the cosine similarities between the embeddings of synonyms so it's decently high and with other words it is close to zero otherwise of the once it's learned but it's not that high when you are in a high dimension okay here the cosine is in fact very low but significantly different than zero and this is a cosine of synonyms okay so it explained the fact that in fact when you when you once you trained on bedding you don't have to look at the at the intensity of the cosine similarity but just that it is larger i mean it is closer according to the cosine to this word rather than to this other word but the amplitude by itself is not really really meaningful so now i'm going fast on the on the on the real corpus okay so how to recover this kind of structure we use the lung hand micro lexicon that is widely used in finance so you've got to uh so a lot of force that have been identified by experts so here you've got an example of the first words first positive and first negative words of this lexicon and we will focus on positive and negative only and we will look if we take a corpus how the embeddings are able to make the difference between negative and positive words okay you can do that with embeddexler.wikipedia or on financial news headlines or on financial use body so longer text okay so we've got plenty of text okay 12 years of news a lot of news so we can afford to train each year a model and also to address the stationarity of all that the first point is that as expected somehow if you compare how headlines of financial news are able to make the difference between positive and negative words so politic negative is there is far less than what you can do with wikipedia what you expect because its cosine similarity is that negative and negative are very close positive and positive is very close and positive and negative are i mean very far away and here you see that on headlines they are in the same rounds whereas uh at least on wikipedia positive and positive are close together and negative and negative and negative are close together also okay now you can try to see the stationary d so it's here by here you've got this curve that r so for the body and for the headlines the cosine similar it is between groups okay and you see that headlines are not that stationary and difficult to make the difference between a positive positive negative negative etcetera for the body it's more stable and it goes in the direction you expect because positive and positive are there negative and negative are there and they've got a higher cosine than a positive versus negative okay here uh to finish with that the stationarity issues so here we took two banks i mean jp morgan and barclays here we put facebook and google and you and we look how these words so the names of these companies are close to positive words or negative words okay and so uh we we we should we will we will see that uh banks are closer to negative during the crisis it's it's not the case but they are in fact quite orthogonal to most polarities okay maybe here or no not that much and google and facebook you see maybe more uncertainty around them after a while so there is this aspect of the positioning of vocabulary inside uh i mean inside the embeddings of the learned the learned model okay if i try to wrap up just on this aspect of the financial news and the london mcdonald lexicon so you need to understand that in fact uh uh this kind of model i mean the warden bendings are not made to make the difference between what we call frequency synonyms so words that are always the same surrounding they will be at the same position and it's especially true in headlines so it's very difficult to learn uh an embeddings on headlines and then to try to make the difference between positive and negative words so you will have to do something else and and well i mean words that are meant to be neutral like company names they have some proximity with some polarities probably because they've got some surroundings on instantanes and moreover we saw that the embeddings are not that stationary and just the last slide to conclude so uh so if i had few take away for work so embeddings they are maximizing the console trophy between the models so the compressed the compressed probability distribution and the real diversity of the language they focus on priority distribution of neighborhood of words and so they cannot make the difference between frequencies synonyms and the more structure you've got the easier it is to learn so the more compressed the more you can compress the the unbeatings and and financial headlines are not that good when you've got punchy lines with placeholders to make the difference if you want to go further probably doing multitask learning so training simultaneously the embeddings with a task that is supervised by polarized world i mean a lexicon can be a good idea then answer the non-saturity it is another story and the fact that company names are polarized it may be a good idea because maybe it's a simple way to try to uh to understand if a company has a good reputation or a bad reputation because it's substitutable with a good word or a bad word and so i mean you can use that if you want okay i'm on time i think yeah uh thank you thanks xiao so um so i guess now we're gonna go through our first uh five-minute session of uh q a and uh so i'll sort of ask you some of these questions sort of back to back and feel free to you know say i don't totally understand that question next because we have a bunch of questions so let me go with the first one to what extent can companies try and cheat nlp algorithms to avoid being portrayed negatively and shorted in the short term i think that uh okay you can dream about that but i think that the first concern is not companies who are trying to cheat is more like you've got more and more news that are written by computers that are generated by nlp and this is probably far more complex and far more difficult to under because they are generated using this kind of generative models so what would you find so they can be fine-tuned engineered and then you can generate news talking about things but uh with a structure that is not the one that you can expect and it's totally controlled so i i do not think that uh that companies will do that i do not maybe uh i mean for for for long i mean central banks they are paying a lot of attention to what they are saying when they are speaking is it really different i don't i don't think so but it's more this kind of news that are generated by computers that are more difficult because if you mix them with with real news you don't know what you will capture if you want to compress that okay then we have a question by marshall guan he says in addition to getting buy or sell decisions based on sentiment classification could nlp algorithms suggest a target price volume or holding period so we're going into sci-fi here but you know how far away from this no i i think that um i think that you can use nlp to try to uh to deduce some analyst estimates for instance and not because you are trying to build numbers with text it's just that inside text time to time you've got an analyst saying okay my estimate of the pe i don't know is that and so you can fill some blanks and you can capture part of the picture and filling some numbers it's not like you are you are really deducing a price is more that you are extracting so it's more extraction of information that are numbers and you can qualify them understand them put a label on them and put them at the new value of an estimate that is usually updated only once a quarter okay we have one question that i know and maybe even sylvia has some thoughts on this because i know she's looked at this uh why does the luck luck run mcdonald lexic lexicon seem from what this person has seen to be so imbalanced in terms of negative versus positive words i don't know why but it's true that you've got far more negative words than positive words and uh it has been done by humans so i mean you've got to know and sylvia maybe you will elaborate on that that it has been set up on the the edgar i mean the 10k i think on lq i don't remember so maybe in this they are highly legal and structured document by lawyers so maybe that's why you've got so many negative words because lawyers are very protective you know so sylvia feel free to chime in or maybe keep that we can keep that for after your your presentation if you prefer um okay so um an anonymous uh question charles did a lot of work on transaction cost modeling has he found a way to apply nlp on transaction cost modeling no no no that's great i even i even did a suit about doing it so okay and now we're starting to get into funny questions like how is irony processed by nlp there are in fact in fact there is a large literature on that because one of the playground of nlp is imdb commands and there is a lot of irony on in the tweets also but they are very short text i think that currently the standard of nlp are made for very short text and they are quite efficient the issue or the specificity of financial text is that they are very long and so in fact you need to localize the top what what is what is the topic of this sentence of this group sentence or this paragraph before applying a sentiment and it is why mixing embeddings and lexicon is important this is why we were working on that it's on the one hand you want to use embeddings to identify the topic of a paragraph and then you want to apply a lexicon to to know if it's good news or bad news about this topic and so irony it's more for small texts that it's useful and in fact it's probably not that difficult and there are academic papers on that okay well thanks charles so i think that now i would like to um to go to sylvia's uh lightning talk part of the presentation so as i mentioned sylvia ruiz is a recent graduate from the cornell financial engineering program and she was in a team of six students working on nlp um applications and um she has a github repository which uh we have a link to that actually if um i believe it it should be in the chat room um and um but so and we'll leave another five minute um slot for questions after her talk so if you still have some questions or if you feel that your question was unanswered and maybe you want to rephrase it put it in the q a so um all right sylvia your turn hello my name is sylvia and as the professor said i'm a recent cfm graduate and i will be presenting some of the findings or our project which was called can we predict stock prices using nlp techniques and what we were trying to do is to look at the impact that corporates filings such as 10k or 10q have on the stock price of a company so here for example we have a 10k report of twitter and i'm sure all of you know but the objective of these reports is to keep investors informed about the present financial condition and the future outlook of the company and in our case our group was focusing on 50 companies of the s p 500 of five sectors and in total our dataset consisted of 1095 reports that we scraped from the edgar website and i want to say that we spent a good part of the project scraping these documents because we were just extracting the management discussion section because that's where we believe that the company gets to tell the the story in its own words and it's also where we can perceive the sentiment so from these 50 companies we use the reports from 2013 to 2019 as our trained data and the remaining assert s data and then well as as professor lee hall was saying we started as anybody else does with the dictionary based models but we found two main problems so the first one was that a very few words got classified by lauren mcdonald dictionary in fact in average only one percent of the words in a report were classified as positive and about five percent as negative so that was one of the first problems and the second one is that using a dictionary based approach is blind to context so a for example words that could be classified as negative such as liability might not be negative if depending the context for example a decreasing liability so that's where we decided to use the two nlp techniques the word to back model we used to expand the lm dictionary in finberg we use to understand the context so i think a professor lee hall explained very well all the skipgram model and that's exactly what we did so what we did is that we took all the classified words as positive and negative that were found in the report by lauren mcdonald and then we train a skip grant model on each report and then we expand if oops sorry we find the most similar words and add them just to the dictionary if the probability of being the most similar one is greater than 95 percent and if that word is not present in the negative words and we did the same for the negative words in that way we were able to expand a little bit the lm dictionary and at the end we ended up having like eight percent of positive words in each report and seven percent of the negative words so that was also more balanced and when trying to to see the words it actually made sense and then we used the finn berg in order to try to account for for context so the way that we use a well i don't know if you're aware but finbirth is a model it's a bird model introduced by google that is a in it understands a contextual relations and is strained from left to right and right to the left so um what we did with each one of the report is that each one of the sentence was classified as either positive negative or neutral so those were and or like feature those were like the first variables that we had and then we tried to build some features to try to measure sentiment and word complexity so some examples of sentiment measures that we created was the polarity for the word classification the polarity for the sentence classification and then we also try to measure a word complexity so we were taking a look at how many sentences are in the management discussion how long are these sentences and that was something that varied a lot sector from sector and then after we created this uh we had about like 13 features we run an xg boost model oh and our or y variable or prediction was whether the stock price will go up or down and we accounted for market returns and the way we did that is like we said um time interval let's say five days so we take the stock price one day before the the file releasing then the report is released and five days after so that's like the five day return and if that is a greater than the return of the market that's a one and if it's less that's a minus one and actually our accuracy was quite bad for the long short term we have a 61 percent accuracy and for the long term of 53 percent and then we try to see if we could at least do some kind of a strategy from this so what we did is that we use our predictions as the signal so if the prediction was one then we lost long the stock if the prediction was minus one then we short the stock and then we compare to a portfolio that is all long and equally weighted and on both short term and long term we could find a that our strategy was better than they equally waited so i think there is still something that can be done out of this but uh it shouldn't it needs more more research on this first of all because we need more companies and i think it will be worth dividing it a sector-wise because each sector looks very different so for example financials has a very long report then technology is very succinct and yeah i mean here is my contact information and the link to the github i am happy to answer any questions you might have okay sylvia thank you very much um i'm not sure oh yeah charles is still there so let me sort of uh you know we have another five maybe 10 minutes uh for q a so i'm gonna start kind of reading them out and it seems that may maybe not everyone has access to the chat i guess the chat has been a little bit uh not very chatty let's say so uh so i'll focus everything sort of uh my my attention on the q a so um so we have one question uh let's see is it possible to compare the prediction model to expectations 10k can be positive but still below market expectations okay so so in a way what we're doing here are we reading just an absolute um feeling or is it possible to see this nlp has come being compared to uh pre-announced pre pre-10k expectations in in our project uh well this is a question for both of you sort of you know um maybe it was not not something that either of you have have tackled but um but i suppose that it's an interesting uh point which is what if there's great news but the great news was expected you know is that something that nlp would be able to um help or would it be a building block yes i think that when you use machine i mean we use data science in general for financial markets you make i mean my opinion is that it's good to make the difference between compressing information so you've got a highly unstructured data and you want to transfer to transform them in low dimensional time series that are indicators of something usually on the health of the well-being of companies governments countries sectors i don't know what so it is what you are trying to do i mean data set per data set and maybe jointly using several data sets and then you want to infer returns of that but i think that there are two different tasks if you want to if you start to mix to try to simultaneously understand the text and then say oh but in fact maybe the guy is talking about something different because another guy talk about something else or because numbers are different it will be too difficult and especially i mean all these machine learning tech methods we know that they are prone to overfitting so my advice would be try just to use them to compress information and to build indicators that are informative about what is exactly in the data set and then you can use your good reasoning that is the one that the the guy answering the question say okay but uh there are this surprise we know in finance the surprise effect very well so now if you've got something coming from nlp and estimates coming from a database of estimates you can do the surprise indicator by yourself as a surprise predictor but trying to mix all in a big black box i think it's quite difficult or for me it's very difficult yeah and i remember sylvia when your your team was working on this we definitely found that certain companies were overall more rosy tinted you know tech companies uh everything's going great all the time uh and maybe these banks that are a bit more you know morose and have more negative words uh essentially so each each company has a vocabulary and maybe these surprises might be compared to that company's own corpus maybe exactly i think that's a good a a good idea to improve maybe your model so we have a question for you sylvia how did you choose the 50 companies out of the snp 500 by market cap or was there another criteria and so we decided to to choose 10 from each sector so i think we had financials and healthcare and then um i think actually we just took them at random yeah okay so but with a spread of of uh industries so and by the way if anybody does want to dig deeper i encourage you to look at sylvia's github repository so we have a question for mr lawal relative to the work he presented is there work being done on adversarial training of these models to boost robustness to poor performance on synonyms antonyms where the generation is not done according to preset grammars but instead through an agent trying to confuse the discriminatory model yes i i i think that uh it is something that we looked carefully in the team and in fact there is a lot of work but it's uh for nlp exactly on this topic but more oriented to gender neutrality okay uh when you when you want to learn something and in fact you do not want to have a biased order i mean men are i don't know medical guy i mean the medical industry um women are more nurse and and and and men are more doctors so it's not something that you want if you want to take decisions about that etc so there are techniques that are adversely learning to try to replace words by others i mean you've got two kind of techniques either you you use a part of speech tagging and so you got placeholders and you put synonyms or antonyms where you want to have naturality either after that once you've got the unbeating you identify i mean algebraically the subspace that is spanning by this kind of man vs woman dimension and you orthogonalize your embedding to this dimension and now your embeddings are neutral okay but here what we want to do is exactly the reverse we want to break the neutrality between positive and negative words so for some i mean for for at the last one the last slide when i um i've shown this uh this uh embeddings of company names you can i mean it could be a good idea or it could be an idea try to neutralize the company names for instance but i'm not sure i mean i think it's an open question because imagine so it's something that we didn't test it but imagine that okay companies are of a color i mean when you learn during one year on weddings some companies have a positive or a negative color okay say that now you are building a portfolio on that okay a factor what would it be in fact it may be simply like a trend following something because it means that if during one year the company was very close to positive words and other companies very close to negative words one is going well and the other is not going well if you build a factor out of that it means that you believe that it will stay and so somehow it can be quite correlated to a trend a trend following strategy so again it is a difference between exploiting unstructured data to make indicators and building trading strategies and the two aspects are in fact different i think it's good to uh to build a factory to to generate a lot of informative features in low dimension and then to have people trying to combine this data low dimensional data to build predictors and there are two different tasks okay thanks ciao um i have a question i think this is um for sylvia that seems to be from so it's from da jang zhu who it sounds like he's tried his luck at nlp before uh he's asking any thoughts on extracting a section from 10k besides just trying to locate the section title sometime case have weird titles or combined sections yes so that was a very difficult task for us because so when we were extracting say i think it was item seven management discussion so companies have been updating their format so sometimes they will say item 7 dot management discussion and then a when we scrape it doesn't find the the section but it's there and then each company calls it a little bit different so we had to be kind of like we had a list of all the possibilities that we were doing it and like they could write it and then we will do it company-wise and make sure that we were in each report doing it correctly because the first time we scraped the whole smp 500 and then we realized that many of them were empty because we couldn't find so then we had to do it like before automatizing really making sure we were covering all the possibilities and even then trying to do like one company each time and making sure we had every all the information correct okay thank you um so we have one more um i think it's it's to both of you but um i know that uh challenge mentioned this uh bloomberg sells in so this is philippe perumal says bloomberg sells a new sentiment index did you compare your own embeddings and the index of bloomberg many things so i didn't and it was not the the purpose of this study i i think that again to go back to this idea that uh you've got features uh that you can build on text and then you've got credit or bloomberg predictors are probably trying to build predictors more than indexes and and so then when you build a predictor it's highly mixed it's mixing a lot of things so if it depends how you want to use that i mean if you never used and you do not plan to have a your own text embeddings and polarity indicators and on it's good to try this kind of the shelf solutions but if you want to make an effort yourself it's difficult to benchmark yourself against that because my discipline would be not to try to supervise my indicators with returns at first with really i mean nlp indicators and so how i mean why competing with indicators that have been built via supervised learning on returns it is they are not living in the same space okay um so we have uh i'll take maybe one or two more questions and we'll call it a night uh because i guess in paris it's about midnight right so yeah uh let's we need to go to bed soon so um we have nikolaus rapanos who says who asks uh how would you see the use of nlp to construct empirical asset pricing factors jointly from asset returns and text data okay so there are papers about that i mean the first paper i show i mean i i i in my slide is done by brian kelly and they are doing more or less that so uh so of course in all these factors literature people are doing that for i would say the last five years for sure i mean the way we are talking about embedding sanduan and uh and yes i mean it's natural to build uh but you've got so many nlp factors i would say because sylvia can build one using edgar feelings and only one section i mean the disclose of risk for instance another in management for seeing or i don't remember the name so you can have a lot of nlp factors because lp is a technique and in fact what is important is the information contained in the corpus so you should have like a 10k factor or a 10k a risk section factor you could have like a analyst reports factor earning course factor and so nlp is just a technique the corpus is the most important in that because the information is the corpus so the factor will be related to the information contained in the corpus okay well um i think i'm gonna leave it at that this was really a very exciting uh talk and i i liked a lot of things about both your your presentations these generative models are uh quite fun and i think they give a really like good grounding to the whole thing sometimes i play on my on my uh imessage autocomplete and just com autocomplete autocomplete and i get a text that sort of sounds like sasha but isn't sasha and uh and i think that's your idea is somewhat similar you know trained on you know twitter's uh you know management sections you can do a financial mumbo jumbo out of twitter and that that i think is a an interesting uh sort of way to think about nlp and um i hope that uh the attendees will dig into the github that uh sylvia shared with us because you know this is a a hard topic to get into and but i'm sure that uh that people will be able to use that and uh and uh thank you both of you and uh have a wonderful evening thank you thank you bye 