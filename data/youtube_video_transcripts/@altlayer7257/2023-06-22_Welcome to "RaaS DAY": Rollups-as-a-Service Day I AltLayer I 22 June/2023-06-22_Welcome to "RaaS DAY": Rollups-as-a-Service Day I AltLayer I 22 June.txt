[Music] thank you [Music] foreign [Music] foreign foreign [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] thank you foreign [Music] [Music] [Music] thank you foreign [Music] [Music] thank you foreign [Music] [Music] foreign [Music] [Music] foreign [Music] thank you foreign [Music] [Music] [Music] thank you foreign [Music] [Music] hello hello yeah hello and welcome to all players raste short for Roll-Ups or service day whereas is a virtual event with the goal to bring together experts in the roll-up and modular space and to talk about the present and future of Roll-Ups but from a Roll-Ups as a service perspective that is a world where we have many application tailored Roll-Ups you have an exciting lineup of talks but let me kick start this event by giving a talk on why build rollups or service platform in the first place and how off layers is contributing to the space in general foreign why build rollups in particular why build role of the Service Solutions you know such as now if you look at the space today right uh we it's quite as well established by now that modular design philosophy is the only one that can come and scale applications so that's capable to provide you know applications the right kind of Da bandwidth the right kind of you know block space bandwidth and so chains that are monolithic in nature in particular for example ethereum they have started to move kind of adopt this model a stack and basically focus on settlement and da needs for applications um while the main component which is the settlement layer I mean main or one of the components which is the Saturn clear of the model stack has started to move out of it here right and we have started to see all sorts of execution layers also known as Roll-Ups these days uh to pop up and in fact if you look around um you could see in the space today different types of Roll-Ups depending on uh that have different features for example you could have a Roll-Ups that brought based on ziki or based uh you know so for example ability proof based uh Roll-Ups or you could have for example optimistic execution model based rollups you could have these Roll-Ups that run different runtimes for example you could have evm based Roll-Ups or awesome based rollups so Lana IBM based rollups and so on and some of these rules have different purposes as well so you could have a general purpose roll up for example orbitrum optimism ZK sync and others you could have application specific rollups that you could build using let's see op stack or arbitrum orbit and so on and they could be deployed in different contexts for example it could be a layer two for example you could attach them to base change for example ethereum for de needs or it could be implemented as a layer 3 on top of an existing layer 2. but basically the idea is that because execution has moved out of the base chain now people have all sorts of flexibility because they are no longer limited by the constraints that a patient provides or restricts you and so people are building different types of execution layers that have different characteristics so you can mix and match different features of your execution layer or that best suits your application business needs and so this is those all sorts of x action layers or Roll-Ups out there in the market today you know ranging from uh you know persistent or application General general purpose Roll-Ups like you know zika sync optimism but also very specific application specific rollups like op stack based Roll-Ups and so on and so what has happened is because of the different execution environments or the different execution needs so different execution characteristics that have come up we are seeing a fragmentation of the entire execution space today and as a result if you are a new application developer uh and want to build your own application uh specific infrastructure so your own application specific execution line you will have have a hard time in some way because you have to deal with all the heterogeneity that comes from this fragment execution space there's a lack of for example a single development framework that could use that you could use to build your own application dedicated infrastructure and um even though there are some sdks that I do that are offered sometimes they are very low level so it requires still quite a bit of understanding of uh how Roll-Ups work which might not be uh the expertise of some of the application Builders out there and so the goal was to kind of build something that could unify the entire space in some way right when we started we were thinking about okay how could we build a universal execution framework for all web 3 execution leads and so it's it's a question that you know we kind of know the answer to right because if you look at this space in the web2 world you have this giant Amazon web services and so if you're a developer and you're building anything right from a startup to you know even a medium or big or large scale uh you know venture you Outsource your compute or storage capacity to Amazon web services because it's much more convenient it's it's cost efficient and also it kind of takes away all the hassle of maintaining that that infrastructure yourself and so the idea was to kind of learn from uh what AWS has done for the web to space and kind of take it a little bit forward and move it to the web series piece so can we do this and the answer is yes we can do it let's see let's assume that you you were able to build an AWS of some sort right then what would happen right so one thing is it could reduce cost for all developers so right now if you're going to build an application dedicated infrastructure today uh it will take you millions of dollars to go and build it right if you're not using something like let's say uh you know an SDK that is very tailored to application specific infrastructure it requires a lot of Dev resources right this is a good example of this is for example um dydx right d y d x um was decided to build an application dedicated infrastructure they announced it sometime mid last year it's almost about a year now and it's still not out there yet right because it simply it takes time to go and build application specific or application tailored infrastructure in the blocks in the in the blockchain world and so if you had something like AWS then you could wrap it rapidly innovate so you could very easily go and deploy a fully customized infrastructure and you could host your application of that infrastructure and you know when you feel like uh you know uh you know this is something that you know you really need you can customize in all sorts of ways the second thing you can scale the structure can scale depending on uh you know based on applications that you have and so uh this leads to uh sorry and so um the code was to kind of build something like AWS but in the web web free space and so the idea was to kind of build something what we call today as roll up as Service uh solution so roll up a service didn't exist a couple of months uh sorry uh when we started back in uh 2021 and now it has become quite a few uh quite a popular uh theme outs and a lot of projects are interesting projects that building in the space around around roll up as service or Solutions so what does it offer you so uh allows anyone to go and deploy a fully customized application dedicated roller within minutes and it should be a YouTuber by any non-technical team member so imagine for example you're a business person or a marketing person in a team and you would like to have or at least experience or even deploy or innovate using an opportunity infrastructure you should be able to do that and shouldn't take you you know months or years to be able to deploy that so the goal was to make it easier as easy as possible for anyone to go and deploy uh these application tailored products and so that's that's what the goal of sorry my slides are jumping um so the goal was to so if you had something like authorized roll up a service what could you get from this so one is because I said it's it's designed as a no code framework so anyone with a zero coding experience should be able to go and launch an application dedicated infrastructure you know by themselves and so it it makes it much easier for anyone really and so the second thing is once you have it when it becomes easier it also means that it becomes low cost and it's it's also fast to deploy right and um what's really interesting and I think it's it's one of the core component is that when you have infrastructure like this to to deploy a rollout for yourself then if you are an application builder then you can go and focus on your code expertise so imagine for example your game developer then you can go and focus on game development rather than worrying about oh how do I go and manage my infrastructure and how do I go about uh you know maintaining uh nodes that I have that I have been drawing uh for my for my role up here and so all of that goes away because you are basically Outsourcing your other component to roll up a service provider it basically gives you a managed service of that kind and so um where do we stand uh in terms of uh roll up a service provider so uh we are the only ones water is the only one that offers a unique kind of roll-up that we call a female Roll-Ups and I'll tell you a little bit more about it uh in a minute and so it's a rule of that is basically disposable in nature and uh so you can spin up this rule up you can use it for as long as you want I mean you feel like you're done with it you can take it down one of the only wraps provided today that runs with these plus sequencing so this is something that you know has been uh a point of debate uh in the last couple of weeks or even months where you want to make sure that um you don't run with a single sequencer because it can create uh liveness issues can create issues where you know if your sequencer goes down you will have to find ways to get your transactions uh processed the second and third thing is we are the only ones that are working for proofs on the tested at the moment and it hopefully will go over them individual so um our dashboard which allows people to deploy these robots already live so you want to play around with it go to a dashboard dot all the technology uh there are several uh I would say tiers uh depending on uh these are pricing tiers but there's a free version that you can go and try uh this allows you to deploy a roll up that will last for about two hours when you feel like you're done with it you can take it down the two um kinds of Roll-Ups that we offer at the moment on one that we call a female Roll-Ups or Flash layers so as I mentioned these are Road ups that are disposed by Nature the second one is what we call persistent roll up so these are roll-ups that are basically something like arbitrum optimism but um you know for dedicated application so you know it doesn't go down for you know immediately it stays there forever and the last part is um roll ups that so by the way these two roll ups The Flash layers and the processing multi chain roll ups these are both builts using our own Tech stack but we're also in the process of integrating external sdks so for example arbitrum orbit which is uh which allows people to go and deploy arbitrum based l3s and the idea is to kind of offer people a framework you know a dashboard from where they can go and deploy arbitrary orbit-based l3s so we are adding support for third-party orola sdks as well and so let's come back to these flash layers or female roll up so these other side these are Roll-Ups that are disposable in nature and they're ideal for applications that drive a piece of number of transaction volume but in a very short span of time so imagine something like a mint event uh which could be extremely popular and hyped and you want to make sure that you know it doesn't create a gas power-like situation if you want to host it on the general purpose chain so take for example UK Labs right for example so uh if you go to Google Apps and you want to do a mint event you know that you know your project or your event is going to be super hyped and you want to make sure that you know that doesn't create a gas or like situation so you would spin up this rule up that would be disposable in nature so you do your mint event so once you have spun up your roll up you do your mint event on the roll up and once you feel like you have sold out all your nfts and there's nothing left to you have to do and at that point you can basically take down this rule up and all the assets that were minted on the rollout will go back to the base chain and at that point you can basically take down the infrastructure and all the assets uh move back to base chain for example ethereum and any node infrastructure for example RPC endpoint and those sort of things will go away it will be taken down and so it's a very resource optimized uh version of rollups the second and as I said it's the benefits of you know multiple one as I said you know if you are minting uh nfts for example if you're using for a very short short span of time it allows you to kind of have a dedicated block space but in a very kind of resource optimized manner people would be familiar with user experience will have familiar use experience in the sense that you know if you are uh your users are familiarity VM you could have evm runtime on develop side as well and it's very uh you know your assets go back to the base chain so for example if your nft project or it's very important for you to have liquidity and so you want you asked us to go back on ethereum so that you can trade your users can go and trade on openc and enjoy all the liquidy benefits that ethereum and the patient provides you and so the second part so this was the flash class so these are Roll-Ups that are disposable and this is now I'm talking about the persistent roll ups these are Roll-Ups that stay there forever so they don't go down they don't get taken down they stay there forever and these are ideal for anything that has a longer term kind of perspective so imagine for example a D5 project or a game five projects a social five projects so these projects would be ideally uh should be on a persistent roll up because they they prefer they'll prefer to have a longer term infrastructure only has been there for quite a while now uh so we have been experimenting or Class Tech or in a variety of settings in particular mint events uh and uh gaming so um for example we hosted uh twice already and I think we're hosting one more event of Dark Forest uh soon and uh so dark Forest uh every time we did host the Dark Forest and we received about 200k transactions a day from about 300 to operation replaced so uh our Tech has already been is already been used for different activities ranging from games to social fight to uh nfts there are some other usage metrics more on the tech side so we ran a campaign a few months ago on a flashlight so we allowed people to go and deploy our flash layers and we asked them to go and do transactions on those flash players and during that campaign we received about 100 million uh transactions on those flashlights um we'll come back to a minute but we also have a network called Beacon layer which basically acts as a decentralized sequencing layer and yaochi will give a talk about this uh you know later and that has already been be live on testnet and that has been doing about has done about quarter million transactions uh already you also have a multi-sequencer test net so there's a test net that has multiple sequences they run a local consist protocol among themselves and that has also been live uh and it has done uh close to about a million a million and a half transactions already so um as I said this is a roll up a service day our idea was uh to bring together experts in the roll up and the model of space to discuss and share insights on the latest technical developments and economic points of view uh we have an exciting uh lineup of speakers and talks uh this will range from shared sequencing to rule of restaking it's a straight arm from eigen layer will be coming up next after the talk uh we have a talks on Mev uh PBS designs and economic designs for overlaps so um this will be mostly talks uh to start with we have a panel uh towards the end uh with Stephanie from asari uh Patrick McCory from arbitrum and maida from variant uh and uh really at the end we'll have a q a session with myself and yaochi if you have any questions around Roll-Ups in general service platforms but also uh you know volte in particular so thank you very much for your time and attention and enjoy the talks foreign good morning everybody welcome to this talk on eigen GA which is um a data availability mechanism for Roll-Ups uh in this talk I'll try to motivate why we think this is an interesting mechanism the talk is subtitled hyperscale open Innovation for roll-ups and you know Amrit gave a great talk just before this on alt layer and first thanks to Amrit and yauchi for inviting me to this uh great day the roll up as a service day here um so what I'm going to talk about is how do you design a data availability layer that supports this kind of an aspiration that all player presented which is you want to support a large amount of throughput you want to bring um an insane amount of activity into the uh crypto ecosystem so that's why we call it hyperscale open Innovation for Roll-Ups um firstly I want to thank several of my team members who've contributed to this it's always a joint effort and thanks to all of them um okay let me jump into the applications so we want to start this with you know we want we're talking about hyperscale open Innovation and the question is why would somebody be interested in this and I want to start with a motivating thing you if you want to try to measure applications in the crypto ecosystem you can kind of do this on two axes one access is what throughput do they consume and the other axis is how much value per bit is transacted what is value per bit like per bit of information that you're sending through the system how much uh you know is a single transaction worth hundreds of dollars or it's worth like a penny and okay so why is this important if you take this um the wavier operating in blockchains today very clearly we are in the uh top left quadrant here in the figure which is there is we're transacting systems mostly uh financial and nfts which have high value prepared these objects that you're trading or sending or worth a lot but it's not a lot of throughput so that's the access on which blockchains today are operating but where we want to be so we want to enable new and interesting applications like gaming social high throughput D5 uh not only we want to go to the high throughput regime but also we want to get to the low value per bit like you know a tweet how much is a tweet worth maybe it's worth a send maybe it's worth nothing so you want to go to that regime you are talking about a very different kind of system architecture which would get you there but even if the value per bit is low because the throughput is high the product of these two is what the total value transacted through the system is so if it's valuable bit times bits per second right and so that's value per second so that's this curve right okay so we want to get to this regime where you have very high throughput but low value per bit but the fundamental thing is therefore the transaction cost per bit needs to be um small okay so if you look at transaction cost per bit there you can look at what are the fundamental contributing factors to this I think there are three basic contributions to the transaction uh cost number one is the capital cost the capital cost is basically you need staking if you want security you need staking and if you have staking then there's a certain opportunity cost of capital so that's the first contributor to the capital cost the second contributor to the capital cost is the second contributor to the cost is operating cost you actually run the system you download the data you have to do all these things and then the third contributor to cost is actually congestion cost um and essentially what we've done is to think about like what these how do we reduce these three axes if you look at the third one the congestion cost the condition cost is basically just um the uh if you look at the congestion cost it's fundamentally coming from everybody who's transacting creates an externality on all the others and you if you're creating congestion you need to pay for it so right now pretty much all the blockchains fundamentally price for congestion so you're basically running at congestion cost so let's look at all these three cars and how we think about a eigen layer of reducing these costs for the capital cost if you want a certain amount of stake really and you know if you have one billion dollars taked and you have to pay off the opportunity cost of the capital you may have to pay a 10 APR to actually compensate that a 10 APR of a one billion dollar be talking about 100 million dollar annually in fees that needs to be paid for it so one way we can reduce the capital cost of staking is by having shared security you have the same pool of security supply not only to one application but to many many applications same thing that same economy of scale that really powered general purpose proof of stake blockchains can actually power other things as well so that's the capital cost if you have Shad security the principle the eigen layer Builds on is called restaking which is the idea that if you stake in ethereum you still have um that Capital that you can actually make additional credible commitments with for example you download and run and validate a new system you can actually promise that you're running that system correctly okay so we have Capital costs we have operating costs if you have you know everyone needs to download and store the data that actually adds up quite a bit over the entire system today actually the operating cost is non-dominant like even for a pretty expensive system like Solana you have like thousands of nodes each node maybe annually costs like ten thousand dollars so you're talking about the annual operating cost is 10 million dollars it is uh high but it is not as high as the capital cost because you have several you know five or ten billion worth of stake that means you're paying off like one billion dollar in like the staking APR okay so you have the capital cost you the operating cost which may not be dominant but as you increase the throughput it'll start to become dominant and finally you have the congestion cost and the way to reduce congestion cost is actually by uh enabling shared uh throughput you actually build high performance systems so that there is very little congestion to begin with and you also build an economics where you don't have this unpredictable pricing of oh I don't know when the price is going to go uh up or down and this unpredictable pricing actually leads to a large number of problems uh Amrit in his talk earlier pointed out how uh these blockchains can actually reduce these blockchains can actually learn from existing Cloud systems like AWS and I think we want to do something like that here where for example in AWS you can actually reserve a compute for a long enough time and you know that there is no congestion because you have reserved instances there's also another concept called a spot instance where you can go and buy an instance at the moment but if there is congestion that spot instance may be very highly priced so okay so that's the core core thing we want to think about the fundamental cost basis of the system and make sure that we are actually designing a system which is very very low in the fundamental cost per bit analysis okay so we you just saw the role of architecture earlier so I'm not going to go over this but the core idea is that right now roll up sequencer takes the data and then posts it on a data availability layer why do we need to do this you need to publish for the computation to be transferred and you need to publish the inputs to the computation because if you have the inputs to the computation anybody can replicate the computation or continue the computation onward so you know there are ways to slice and dice this so you could either publish the inputs to the computation or the outputs to the computation depending on whether you're running a um optimistic or a zero knowledge rule or a succinct validity roll up but you do need to publish the inputs or the outputs of the computation on a data availability layer so that's what does the data availability layer do it enables anybody else who wants to know the inputs or the outputs to this computation to come and acquire it Okay so what properties do we need from a DA layer okay actually I'm going to just swap to uh different uh presentation so here we go I think the animations probably work better on this okay what properties do we need from a DA layer we need the dla to be hyperscale okay what do we mean by hyperscale we want the bandwidth of the system to grow linearly with the number of nodes the more the nodes in the system the more the throughput that it should acquire and what do I mean by nodes in the system the nodes which are serving for data availability the more the nodes the better the system should be in fact it should be you know the most ambitious goal would be exactly linear you have n nodes each of the NFC bandwidth you multiply the two n times C should be your system performance that's the most ambitious right you're aggregating all the bandwidth fluidly but there's at least this Factor two loss why because you know we want to operate the system even when like a majority as long as the majority of the nodes are honest so you do take a you know Factor two loss but not that much more like that's what hyperscale is is basically exact linear scaling for a dla okay so that's the scaling uh what about the cost right you know if you compare to a system like AWS which is a single node downloading and storing data maybe there's like a couple of copies or five copies which is storing it and you want to get to that kind of a cost basis even though you want your system to have thousands or maybe hundreds of thousands or even millions of nodes the cost basis should not be as low every node needs to download and store the data so just to uh to pick on the first point here on hyperscale for example what we are expecting is as the number of nodes increases in the system today most of the systems don't have linear scaling they do they actually have no scaling the more the nodes in the system the capacitor the system Remains the Same but what we're looking for is the more the nodes in the system the system bandwidth scales linearly but the cost does not scale at all so that would be an insane system right like you have more performance as you get more nodes the cost doesn't increase and the security increases so you you just like slicing the trade off perfectly in your favor what would be the latency limit you know if you're looking at an AWS system you send a a request there's you know the request goes to the server and the server says yes I stored the data so that would be like a round trip latency and you'd want a system which operates at that kind of like a performance so that your roll-up nodes are not stalling on some data availability layer waiting for you know 12 seconds or you know sometimes you know even 12 minutes for finality should be verifiable nodes anybody who wants to verify that the nodes in the da system are doing their thing you should be able to verify it whether I have to download all the data so this is the property we call verifiability which is a simple light node can download and verify all the data and finally you want to have it be customizable customizable basically means like somebody wants a different Quorum somebody wants to use their own token somebody wants a different kinds of properties we want to be able to allow for all of these different kinds of properties so this would be an ideal dla in our view hyperscale bandwidth's not a limit low cost low latency verifiable and customizable and so the we're building a system called eigen da which is built on top of eigen layer eigen layer is this uh core platform which is based on restaking so the idea of eigenator is the stake in ethereum you set the withdrawal credentials to the eigenlayer contracts eigen layer is a set of smart contracts on ethereum so it's taking ethereum you set the withdrawal address to the eigenlayer contracts and in the eigen layer contracts you set your own wallet address as the withdrawal address so what you're adding is really like a step in the withdrawal flow and what this does is it really allows the system to enforce um a system of like um checks and balances where like if you're not operating for the services that you've opted in correctly then the system can enforce a penalty on you um so that's the um basic structure of eigen layer basically we call it restaking which is your stake in ethereum and then you stay you set the withdrawal credential to the eigen layer contracts adding a step in the withdrawal flow but then enabling trust transfer to all kinds of other systems including like a data availability system okay how is Inga achieving this hyperscale property the core idea is using Erasure codes to distribute the information so that no one knows you need to download all the information so you encode it and send it to all these nodes and each node only downloads a little bit but they also come with cryptographic proofs that they've been encoded correctly so you don't need to worry about it so we use kcg polynomial cryptography this whole architecture of the cryptography underlying eigenda is basically built off of the ethereum roadmap you know in the ethereum roadmap we have this thing called dunk sharding which is a mechanism to actually scale the data availability bandwidth of ethereum eigen Da is basically like an opt-in system of dark shouting that we've built and because it's out of protocol there's a lot of degrees of freedom that we have which actually lets us optimize even more than dog starting so the Don shotting throughput is roughly one megabytes per second 1.3 megabytes per second we are already at 10 megabytes per second with uh eigen ta and whereas the node bandwidth requirement to run an Inda node is actually much smaller than you know all the other systems okay so really this is you know one of our vision for eigenlayer is that anybody can come and build these new features on top of ethereum security and or borrowing at least aspects of ethereum security and India is basically and and what we want to do is to enable people to experiment with new kinds of distributed system technologies that can then be built on top and the the best ideas can then be internalized back into ethereum over a longer time scale so I can use our first kind of uh product on top of eigen layer which basically tries to achieve this awesome so let me uh skip ahead to some of these other properties I've mentioned it's low cost it's low latency uh but I think one of the most important Dimensions that we need to consider when building the you know things like Roll-Ups is how does it interact with the economics and if you look at the role of Economics today there is a bunch of uh problems with the role of economics and the first one is the Roll-Ups dominant cost is Da and d a cost is and the ba cost is high you know basically you have a problem like you cannot make it cheaper than the da cost so that puts a floor on your cost basis and the second problem that we find is the d a cost is uncertain so you don't know because the da cost changes based on congestion also you have same site externalities you know Amrit and stop was mentioning about yoga labs for example if there's a yoga Labs roll up and you have another you know you're checking along doing your own like other roll up and suddenly there's a yoga Labs mint and the dla is congested and then now like your da cost becomes uncertain maybe it blows up by 10x and you cannot satisfy your users and that's really problematic so the da cost being uncertain is actually like not very good and and finally roll ups you know unlike an L1 an L1 can fix a certain amount of inflation as the rewards for data availability inside their own system if you're an L1 but if you're a roll-up you can't pay in your own token for data availability you even if you allocate a treasury from your own token still take exchange rate risk of oh I need to exchange my token for eth and maybe the eat price goes up and you need to kind of eat it on your roll up okay so I roll up to tag India can completely transform all these economics by uh getting the best benefits of chat security while retaining a lot of degrees of freedom first thing is the with eigen d the da cost is low why because the cost basis underneath eigenda is low and really by optimizing on all three axes of the capital costs because of restaking the uh operational cost because of the better like distributed systems architecture and then finally for congestion cause because you know throughput is high there is not congestion so you actually get the underlying cost basis is low not only that there is no uncertainty in the cost basis you can actually go to eigen Da like you reserve a spot you know a reserved instance on AWS you can actually Reserve hey I want 10 kilobytes per second bandwidth for the next one year and you can go and Reserve that on eigenth year because again has plentiful data bandwidth we are able to actually do this and many many many systems can many many Roll-Ups can come and Reserve this kind of bandwidth and what you get is actually positive same-set externalities the more Roll-Ups come the capital cost of the system is split among many more Roll-Ups and the cost per byte actually goes down other than going up finally we allow Roll-Ups when they're doing long-term deer reservation to actually pay fees in their own native token what this means is a roll-up may come and say Hey I want to bootstrap my system and I want to give like the ethereum stakers you know five percent or whatever of my token economy to as as a DA fees for the first year but also that helped me bootstrap the da and bootstrap the roll up this is something that you can actually do on eigen okay so that concludes my talk basically you the core idea here is by optimizing the underlying cost basis of um how you build a DA you can actually achieve all the favorable properties hyperscale low cost low latency right fiability and customizability on a common data availability layer which is built on top of chat security from ethereum hi everyone hi it's uh this is Thomas Sanchez from flashbacks so I'll be talking today about uh cross domain PBS interfaces for uh survival thank you so much for invitation thank you everyone uh who's listening uh or will it be listening the recorded version uh let's go uh so maybe first introduction if uh what's my story with flashboats during flashbirds in 2021 as of mate which means there's like contribute to individual contributor full-time uh worked on The Late Mev Gap specs so really just specifying which was built by the by the Creator the engineering design team from uh from flashbirds then working on the leme Boost design so the design of the PBS for the proof of stake ethereum the major focused on the product management of the early flashbots relay in the time of the uh recap and then then the flashcards blog Builder later took over by others then worked on the some of the swap architecture components so think of suave as like many many modules like es components working together towards the the single unified auction uh for body expression and I work on some of them and some of the research I'll present today it's based on the recent thoughts that I uh when I have been exploring the decentralization shared sequencers the centralizations on the of the sequencers on l2s mostly and the cross chain navs is something that uh photoshops have been interested for a long time we've published some of the research and and kind of introduction to the cross cross domain Mev in the past so so check out what flash plus posted on the research basis in the past um and uh maybe as a disclaimer so this part of the building the open uh I'll flashback so it may it may present some ideas that are at the at least research stage uh they may not be fully reviewed by all the strategy teams that flashboats and uh this means that in some areas that may not be yet fully aligned with the flashbot's values uh or even the quality of design of the research might be lucky uh nonetheless I think that's uh more of the uh to present it as an invitation to to collaboration on the and the ideas presented uh and also I like to to work together towards the full alignment of of how he wants to present all the products at flashbots and uh how they want to achieve full alignment is always eliminating democratizing Distributing MV uh in a permissionless transparent way for for ethereum manager chains so when you think about elimination it's we bring transparency about the past so explaining what happened on the Chain explaining through various tools and and write-ups explaining what happens now by by the real-time dashboards monitoring tools Solutions and explaining also what will happen in the future by educating researching and providing fundamental research for a blockchain Community uh democratizing the permissionless access for for our Solace takers for solo Searchers uh when you think about the minigap in the past this was all about enabling several Searchers to access the market without any big OTC Arrangements nowadays through the Mev boost that's democratizing for allowing solar stakers to access that market without having to access to Joint polls uh and distributing so for the solutions I can maybe share nowadays but also for through Suave or like ultimate goal uh Distributing Mev uh to the to where it originates from to the users or also uh together in collaboration with Solutions uh from EF like MV burn and distributing it across all day all the ethereum holders and also like the holders like the online Builders of other chains or whoever the users are so all this free uh free ideas also behind uh what I'm calling here as a Mev Garden so uh we say let's move away from the Dark Forest of Mev to to a movie Garden so we want to start introducing solutions that are all continuing introducing solutions that are very uh clean clearly divided into components uh this modular way they create a space that is adapting to to all the different market conditions and uh different stage of growth of various uh Builders participants of that market and obviously designed for the pleasure and utility of users so maybe one aspect of it the bright place that defines transparency the transparency of rules and the arrangement for sure but not the transparency of what users want to hide so privacy and all of this for sure so some some space for privacy in this Garden is very important for the users to decide what they disclose about trades and what they do not within the Suave ecosystem all right so with our design goals to reveal some of these ideas and to introduce some new so we want to support well first of all what we are building with a movie Garden we are building a solution for connecting block proposers with block Builders but in the cross domain market so the same as you say PBS in ethereum connects the ethereum proposals ethereum validators vegetarian blood Builders we want to do that for l2s for different domains for multiple and Watts working together potentially enable block building for multiple chains at once so that the builders to be able to make proposals main beats about synchronized blocks together now uh what we want to achieve while designing the solution we want to support multiple Solutions on sequencers so however however the design is whatever the design is coming from ltus from little ones about how to sequence whether to share sequencers whether to provide the native sequencers only whether they are ready to decentralize or not yet all of that should be ideally connected to the solution to as well I want to make sure that it's all permissionless and transparent in design so any any shared sequencer should be able to join the system without any white listing without any um any permissions from from trusted operators so maybe unlike the nowadays solution of uh of PBS or to some extent you think that the relays are whitelisted by participants they the design goal here is to allow anyone to join be in line with Victorian protocol Direction so um we do we do origin from ethereum space and we do think that ethereum is defining much of the Innovation and defining how Mev is built into protocols so we want to make sure that we don't design something that will be at odds with the designs coming from the from the ethereum foundation and from General ethereum Community I want to respect the market space in various players and uh this is very similar to the first goal of supernica heterogeneous sequencer space But Here we say that not only we want to support everyone to say that we build for other builders to to connect to uh to code design the solution and again this is an invitation to to research together for sure uh they should be stepped towards Suave so even as we talk here about something that we uh call uh Mev Garden it's not really like that's important this name because it should be simply a component or Suave and and a step towards swap so it shouldn't be alternative to so it should be inconsistent to a swap anything that we design here should be part of this great vision of fastball status this unifying called the expression of value within the context of me VA and we'll say permissionless uh Fair financial markets on and off chain and the last part which is particularly important because uh I would say it's not fully resolved in this design it's ensuring equal rights of Solace takers and large operators equal rights and more like equal opportunities because rights are the same but the opportunities are there for the large operators to uh to have a bit of advantage and hence and the solution might be centralizing not very much you'll feel that this is not a big concern maybe not comparable to the one that we've seen in the in the PBS space before PBS was introduced but nonetheless the one that is necessarily uh necessary has to be solved before we say that the solution is consistent with the value so of how flashbots designs products and a bit of a glossary maybe uh the basics so we do separate the roles so the sequencers proposal Builders and relays and particularly I'm pointing out at the sequencers and Builder separation so uh sequencers are those uh here in this context sequencers are those who pick the transactions in order that Builders are the ones who really take what sequencers provide prefer and uh and wrap it up with the metadata and construct blocks out of it so maybe further filter the data uh there's the rules not necessarily have to be separated in the in the actual designs but they might be uh so we'll be separating those and proposers are the ones who pick the blocks from the best blocks provided by Builders and proposing to the uh to the market right so announce them to the chain as it is nowadays with validators on ethereum or in other situations and domains is this concept that was introduced by the early cross-domain flashbots research so domains are either blockchains or centralized exchanges or any space which provides the opportunities for MVD creation for the for the state that is less and more discrete and changes over time so when we talk about cross domain we can imagine that might be between the centralized and decentralized exchange not necessarily to decentralize exchanges and not necessarily to blockchains and the most important innovation here uh not the complex I hope is the idea of global time or maybe not the complex to understand but but possibly complex to implement so the global time and synchronized proposals we want to introduce the the notion of looking at all the blockchains all the domains together and ordering the events there and and having a consensus on how how those events were ordered across this non-unified time of different chains and when we think about global time the the order of events we show that we want to sequence events from multiple domains it can be as in this example ethereum Stark net polygon blocks happening one after another what we what we want to achieve is to say that the HTML block 100 happened before block 23 on starknet and we want all the participants of this Market to agree on this ordering of events uh so so you have to find some solution to that so we can propose something but this not necessarily is easy called coinb in the top and actually for discussion uh so one of the examples that could be a set of chain of servers kind of oracles or simply notes that are trying to achieve a consensus uh that look at the look at different chains that are forced to observe chains and put the information about the blocks appearing as soon as they appear so for the observation of the exchange of messages we can we can do some kind of attestations to what we observing the same way as the ethereum laser testing to the to the blocks appearing and being served in the P2P Network the same way we can we can build something here obviously observation of multiple chains chains with various speeds think about Solana versus ethereum and so on this might be quite challenging but we do want to have a simple chain of events like this so there should be no processing no access to vbm operations we just want the ordering of events here you can imagine the construction like this built on uh on some modern mapple solutions uh with like the the basic normal plus ball shark uh solution for example so you're saying yeah we want to have thousands or or more events every second but not necessarily any calculations based on that it's just a just a blockchain of records of what happened when uh why do we need that we'll explain so we need the ability to to prove something for sloshing and related to the announcements of the shared proposers or show it uh so I want to Define also something that I call here synchronized proposals so when we were when we were designing some of the Cross area codes the main Solutions at watch what we were saying that we can't do the beads on the on the success of transactional cross domain execution so we can say that as a server I promise to pay if something happens transactionally and then someone can claim that bid claim that payment so as a Searcher I say if you manage to execute transactionally on ethereum and start net then I'll pay you X if those transactions London chain and I don't know how you do that uh you have to judge if you can do that and if you do that then actually the chain as well will allow you to to claim the payment now um the thing is what I wanted to introduce with my baby garden is also the possibility to make some kind of credible commitment or promise about the synchronized proposal so if you're an Executor on multiple chains how can you promise to me that you actually can do the transactionally so I feel a bit more confident about bidding and how can I potentially slash you if you if you don't deliver on that promise uh so I'm proposing here just like three different definition of that synchronized proposals not because I think there are only three or exactly free or that one of them is perfect or just that in the in the context of our discussions in research we realized that this is not just like very very clear definition this is what uh synchronized means so I'm proposing three examples but uh watch really is the final definition of synchronized proposal is this is a bit open so here I'm saying the universal strict would be like we have a set of domains that we're looking at here in the example it's ethereum polygon start that there are three different domains and I'm saying that the two blocks are synchronized only if only if they are like generally happening uh in a sequence after another but there's no other block to belonging to any domain in between so in this example ethereum X is synchronized with polygon y these are two blocks that are sequenced one after another and the same for the the last two blocks but I already say that uh ethereum X is not synchronized with Darkness Z because there's some polygon Block in between now very often when we talk about cross domain Mev we and we think about two domains we don't care about what happens on other domains so here we are very very strict universally across all the domains that we could take ever into consideration in our Global time assessment so it might be really too strict and we said if there's any other event from any chain in between then it's no longer synchronized like some someone might have executed some kind of me the extraction that changes the state of the Cross domain execution in a way that I am no longer too confident that the transactional executional material must start net will be will be good enough because something in polygon might have changed the opportunities um as I say it might be too strict but we presented the domain strict are probably closer to how I would think about it the it says that everything is synchronized as long as there's uh there are no other no other blocks uh belonging to the same domain in between so I'm saying here that uh for example ethereum x with start and z are actually synchronized even as there is blood polygon Y in in between because I don't care about the domains outside of the domains that I'm uh that I'm considering in my cross-domain proposal synchronized proposal promise but uh I would say that the polygon Y is not synchronized with ethereum X plus 2 here why because there is some other ethereum block so I want to say that synchronized box are only the ones that within the same domain are after another and there should be anything in between uh and even less strict uh synchronized proposal definition is the one that says well uh there might be a multiple blocks belonging to the same domain uh so like ethereum X is synchronized here with stagnet Y plus one even as there is a starting at Y block in between but what I'm blocking like what I'm removing from from the definition of synchronized is that there shouldn't be any other like pair of synchronized blocks in between so ethereum X here is not synchronized we start net y plus two because there's some other pair of starknet ethereum blocks synchronized in between so I'm saying that well I feel that I can extract something transactionally as long as there is no other opportunity to extract something transactional in the same demands domains which would totally lock me out of uh of the ability to execute Mev extraction in a predictable way uh as I say this is not super strict definition you can think of your own how you would like to say that something is synchronized why do we need this synchronized proposals we'll we'll start talking about the messages being exchanged between Builders and proposers so in ethereum PBS we have the builders we have also the proposal so the validators and they talk to each other through the relay and in the current design the relay is the trusted part in between um the epbs so in short PBS and ethereum assumes that the regulate disappears that actually solution doesn't change much because you still have the builders and the proposers uh collaborating but instead of using the relay there is some set of attestations and staged at the station and P2P communication that Alice Alice agreement on what the best beats between the builders are and how to how to keep the proposals to their promise and pro potentially slash slash them on non-execution on delivering the promise so we remove the relay you remove the trusted component from uh in between and we remember that these angle from the beginning of the garden which says well you should be in line with ethereum protocol Direction so now we say that whatever communication between the proposal and Builders about the about the synchronized proposals I would like to have the messaging between those two parties between the cross domain Builders and cross domain proposers the messaging should be not necessary through the relay the design should allow for the P2P based communication and I think we should intuitively see that uh yeah that should be requirements for the the relay there might be slightly more difficult because you don't have the same level of the stations but with the global time uh with the global time that we mentioned uh it should be possible now this cross chain PBS is uh as we introduced Let's uh cross domain proposal Builder separation we adding communication from proposal to builders not only from Builders to proposers like nowadays so nowadays we have Builders announcing the reads announcing the best blocks Landing goes to the relays or through the P2P communication goes to that proposers proposal select the best block and announce it and here we say that not only in this direction but also proposers communicate with Builders back they announced that the about to propose something in a synchronized way so we have this synchronized proposal announcements and they should be ideally uh delivered and announced and included on the global time and why on global type chain because you need to potentially be able to slash The Proposal on failure of delivery a failure on the promise right so it's failing to deliver the promise and what's uh why do we use this communication why we use the global type chain just to make sure that it can be permissionless and transparent so the proposers can join and announce whatever they want and they should be slashable so there should be some some deposit you can imagine that it might be in the form of the eigen layer uh deposit so the the risk taking and uh and anyone can join if you can verify that they actually made a deposit it's transparent it's included on that chain uh this is an example of a proposal where I say I'm announcing a proposal and there is a specific slot uh like some sub times some some time that I sequence and I have a set of domain proposals that Define the domain and the main slot 30 so some number on chain uh and when I estimated my my expression of global time or something so sequence or date time when it would happen and I say that maybe I'm proposing according to the some rules of synchronization so I'm saying that there is like universally strict announcement that I will propose on ethereum and polygon for example at the same time and I and I say that maybe a minute ahead uh so now the the builders have this announcement they can check that it exists on the global time and I may start preparing the beads and they know that I'll be sloshed if I don't deliver so they start to be a bit more confident to set the blocks so if I if me as a proposal if I accept the blocks uh then I have to deliver those and the same as I I include my announcements in global time also the beads with the headers can be and also double time and my commitment to the headers can be announced in the global time so you can have the same properties as a of the commitments as in ethereum PBS where I say I'm about to publish that blog so I signed that header so you give me the body of that log the details of the blog the transactions so I can actually announce it that if I don't announce it I get slashed um what can be the types of proposals so as I mentioned we want to respect the all the players in the market so whether these are shared sequencers native sequencers they can all participate in those announcements shared sequencers should be much more confident about uh doing any like uh shared shirt proposals they may be absolutely sure like 100 probability that they will whatever they will announce they will not be slashed uh in some cases they might be randomly being chosen on different chains and sharing that with other sequencers that are competing with them uh in those cases they will maybe judge whether they want to risk slashing or not like was their level of confidence in particular in particular unit of time that they'll produce the blocks to be synchronized that will be next to each other sequence in the global time uh and again like all the whether these are centralized or decentralized sequencers they can all make these announcements nobody is whitelisting them nobody's blocking them uh they should be allowed so here you have an example of This Global time when actually I'm indexing events and uh somebody announces the deal about the to to propose synchronized ethereum and start net n plus one that they approve some beat for for these two blocks with some hash and since they approved and announced both of those events can be slashable against their deposit uh drawing and layer or some other solution and uh and if it's slashable then the the Beats can be uh can be protected the same way as they are nowadays protected with with the sloshing condition on ethereum so this design goal it should be step towards Suave so uh as I mentioned on swath we have designs for these beads to be payable on successful execution but we didn't have maybe the design on how to uh how to slash on a promise in the executor telling that there is some specific probability of execution so me as a Searcher as a cross domain Searcher or Builder I can feel a bit more confident so it adds a piece of swab like on the side of proposers making similar commitments like on the size on the side of beat makers on the Searchers so now we have both sides actually trying to to communicate of how much how much they can achieve and what what kind of promises they make uh and that last call that I mentioned that it's not resolved and notice it here so if I'm about to make announcements about the sequenced synchronized proposals then the larger my stake is on two different chains the more likely I am to have synchronized proposals so the more often I'll be benefiting from the extra bits extra payments to me from the from the Searchers if I'm a solo sticker then I cannot do that on multiple chains because I'm not probably not take care of multiple chains they would have to team up with someone or just give up and join the large operator a large pool that can actually synchronize this across many chains that they are they are operating so there should be a solution when me as a source taker can dynamically exchange messages with some other Solace takers and together make commitments to the uh to the synchronized proposals so what is required first of all we need to Define this communication between proposers something that is reliable and again it would have some promises but then the question is about the splitting the the payment from the from the cross domain block Builder across domain sequencer uh about how how exactly you're paying to to the proposal on chain Y and chain B uh so this should be doable there should be easy part uh maybe slightly harder is the sloshing Target so if I don't deliver my promise who is really slashed if I'm if I'm teaming up with as a proposer on ethereum if I'm teaming up with a proposal on polygon uh the who will be slashed if we don't deliver that promise uh that's a good question uh and well so that's why this has to be answered so within the roadmap think about this collaboration of uh of the multiple multiple proposers has to be resolved and uh well and also making sure that this design really answers the questions from the shared sequencers and whoever build sequencers so it all has to work together to satisfy the goals of how flash what builds the products at very early it's building in the open it's just a research potentially component would be dropped never delivered or maybe something that would be fundamental component of sort of thank you so much foreign foreign thank you all right sorry just getting up the slides here okay can everyone hear me all right okay uh great so I'll be talking about espresso and the role of a decentralized sequencing layer for Roll-Ups uh and I am Ben Fish I'm a I'm the CEO and co-founder at espresso systems and uh I'm also an assistant professor at Yale so first of all um one way to think about what roll-ups are able to provide for the ecosystem is horizontal scaling and so we can think of this as every application That Couldn't exist on a layer 1 blockchain can actually run its own rollup you can also have applications that support other applications on top of them like VMS or ZK EVMS being run by companies like Zig async or optimism or starkware Etc but we can think of this as as essentially a way of sharding computation across applications so that the layer 1 nodes only need to verify these lightweight do this lightweight verification of either fraud or ZK proofs but the actual execution of transactions for a given application is done by the application itself okay and it's a slightly different way of thinking about the role of approver you can just think of the provers being associated with the application approvers of optimism are associated with with with optimism which really is an application that represents a VM that hosts other applications the other key thing about why Roll-Ups actually help scale blockchains is they leverage heterogeneity right a Roll-Ups wouldn't help as much if all nodes in the system were of equal computational power they're leveraging the fact that they're actually especially in the case of ZK Roll-Ups increasing even the work of some nodes the powerful nodes that are doing computation and producing the proofs while making it easier for the weakest nodes of the system to verify the results so the layer 1 nodes can be it can be very weak if we don't get bottlenecked on the weak nodes whereas at the application layer the nodes that are actually executing transactions for a given application can be very powerful and we don't need to have as many of them they don't need to be as decentralized in fact the problem is that today because roll up servers are playing such a monolithic role they end up controlling too much right so the application layer is not just executing transactions but it centralizes the entire process of deciding which transactions to include and in what order and this loses the whole uh set of properties that blockchains were achieving in the first place by virtue of their decentralization so it can lead to censorship lack of neutrality it can lead to monopolistic pricing um on uh by the by by the servers that are carrying out this entire centralized process essentially uh the way that Roll-Ups look today with centralized uh servers that are doing both the proving and the entire process of deciding which transactions to include uh the L1 is just now auditing the Integrity of what are essentially web 2 applications right so we've removed the major role of the decentralized layer one to decide um and manage the process of uh which transactions get included in what order and they're now just verifying and following along what each of these web 2 applications are doing and that's that's not a good situation to be in so the solution is to separate ordering from execution and uh the simplest way to think about this is that the the the architecture that would save us from the situation is if users would submit transactions directly to a decentralized L1 the L1 would only order and make available the transaction data it wouldn't actually do any of the um it would not execute right um and then the application layer would just read from the layer one and every application specific server would uh would execute the transactions for that given application and prove the result and state update to to the layer one so this has also been called base roll up which is a relatively new term but in in many ways this is sort of the purest of ideas for rollups where the layer one is still doing most of the job it's just that the Roll Up is only doing the execution the users are submitting transactions directly to a consensus Network that in a decentralized way is ordering them and making them available roll up servers just read from this do the computation and produced an update now there could be a reason to separate out this transaction ordering and availability layer into what we might call a layer one and a half which would sit between the layer 1 nodes and the application layer servers that are executing transactions and what might be some reasons to do that um well first of all protocol modularity um and it just as protocol modularity for data availability makes sense right the the protocol that we designed to optimize for ordering and availability data may not need to be the same that is running the L1 and handling the smart contracts and the bridges and and verifying the proofs um I'll talk about protocol modularity on the next slide too when we can and in light of uh of things like eigenlayer we may not even need to think about this separate layer as a separate physical layer it may just need to be a separate logical layer and second is there's an opportunity to make different design trade-offs from the L1 so for example we could design this ordering and availability layer to have higher throughput and lower latency but sacrifice on things like Dynamic availability which in the case of ethereum is one of the main properties that ethereum's consensus protocol achieves at the sacrifice of other properties so as I mentioned um we can think of this layer one and a half that's doing transaction ordering and availability as not necessarily even a separate physical layer it could be run through services like eigenlayer that that uh that that enable this by the same set of physical nodes that are running the layer one or at least you could subsidize the the participation of uh the the physical nodes that are running the layer one but logically it is separate and it could be a separate protocol that operates differently from the layer one's main consensus protocol a second problem of the layer two because the roll-up ecosystem which has which which has less to do with the the lack of decentralization and uh more to do with the um sort of a fundamental trade-off that happens when you Shard computation is that the liquidity or interoperability becomes fragmented too so applications on different Roll-Ups are now isolated from each other uh whereas when they were all running on ethereum they were not isolated from each other and this makes various things like atomicity flash loans Etc very very complex to Impossible so bridging becomes complex certain forms of interoperability are now lost liquidity is fragmented so we can ask to what degree does sharing a sequencing layer and when I say sequencing since this is an overloaded term that some other people in the industry may use differently but when we talk about sequencing is just the layer that is in charge of determining and finalizing the ordering and availability of data does sharing that layer help with interoperability right so I will discuss three advantages of doing so the first has to do with partially simplifying cross-roll-up Bridging anatomicity the second has to do with mitigating systemic security risks of bridges for the role of ecosystem overall and the third has to do with cross-roll of building so let's talk first about partial simplification so in general Bridges require two Atomic legs you have an asset that is locked on some roll-up a and then a representative would be minted on some roll of B so let's consider scenario one where role of a and Roll appear utilizing differencing different sequencing layers okay in other words a different consensus protocols for finalizing their transactions in that case in order to implement a bridge roll-up B would need to verify both the inclusion of the lock on rule of a and also its validity and verifying inclusion means that roll up B needs you will need to implement in roll-up B in the bridge contract and roll a b some way of verifying the consensus of roll up a in scenario two with a shared sequencing layer part of this goes away right so now roll up B only needs to verify the validity uh it needs to verify the validity and that could be not by executing it the transactions itself but more likely by verifying a proof from roll-up a whether it's um whether if it's an optimistic roll-up it would be receiving it and waiting for the um the challenge period and then possibly verifying a fraud proof um but if not then it would be if if it's a ZK roll up then it could be verifying the ZK role proof directly uh that still needs to happen but it no longer needs to implement anything to verify the consensus of roll up a because they're sharing a consensus and uh essentially the lock and Min transactions are are just in the same transaction bundle that was processed by this common shared sequencing layer so the second thing I want to talk about are security risks of bridging uh in general we can ask how can what's what actually goes wrong when consensus Protocols are compromised right how can an attacker profit by attacking a consensus protocol and reversing the finality of transactions of course that's an inconvenience overall but is there a way an attacker can profit which this thus gives it an incentive to attack the protocol often the way in which attackers profit is through some form of taking advantage of a bridge that exists um so Bridges offer the clearest example of profitable attacks an attacker can lock an asset on chain a and then mint on chain B reverse the finality of the lock on chain a and now it has doubled its Holdings uh the the common example of you could send an asset to incentives exchange withdraw Fiat and then reverse descend is actually an example of a bridge it's a bridge between a blockchain and an external system which is part of the centralized world but it's still an external system right sending money to Amazon to purchase a good and then reversing the transaction on the blockchain is another example of taking advantage of a bridge because it's a bridge between Amazon's the system and the blockchain if everything stays within the blockchain it's harder to uh to profit from reversing the finality so uh of course there are always going to be opportunities for profiting by reversing finality but if you have many many Roll-Ups that all have Bridges between each other then it massively increases the profit opportunities for the uh for for for the attackers of consensus protocols that are controlling the uh transaction orderings in these different Roll-Ups and when Roll-Ups share an ordering finally layer the attacker cannot profit in this way reversing the lock on one side of a bridge will also reverse the mint on the other and finally uh I want to talk about the advantages of cross uh to cross-roll a building so when Roll-Ups use different sequencing layers then a builder who's trying to build blocks simultaneously for multiple Roll-Ups faces complications high risk and generally slim chances of of success for example it would be difficult for a roll-up to make some kind of economic commitment to its user that it will get slashed if it doesn't includes both legs of some kind of Arbitrage transaction because it's always possible that one of those uh one of the blocks that it proposes gets accepted and the other one does not because it's dealing with two different consensus protocols now with a shared sequencing layer because the consensus simultaneously proposes and finalizes without executing but proposes and finalizes a super block for all the Roll-Ups running on it and through proposer Builder separation a leader in consensus May accept from a builder simultaneous blocks for one or more roll-ups and therefore a cross-roller builder can guarantee any user desired atomicity from flash loans to Arbitrage and it can post bonds that you know which which would get slashed if it violated its promise of some form of atomicity to the user and shared sequencing enables an honest Builder to be certain that if it behaves correctly it won't get slashed so finally I want to conclude with some open questions and challenges of shared roll-up sequencing one challenge that arises is around Revenue sharing so how is revenue shared among Roll-Ups that share the same sequencing layer it's very straightforward for basic fees uh all the fees that our users are paying on transactions destined for a particular roll-up can be attributed to that role if it's very easy to attribute what the marginal contribution of a roll-up is to the profit generated from basic fees and in fact the fees can even just be directly encoded and paid directly in each roll up itself it doesn't even need to go necessarily through the sequencing layer um and the Roll-Ups can pay a commission to the sequencing layer there's many different ways that that could be handled that's not a hard problem the much harder problem is figuring out how Mev gets shared this may be less of a concern for rollups that wants to mitigate Mev and I'm going to talk about Mev mitigation on the next slide but setting that aside let's say that Roll-Ups do want to profit in some way from Mev or get a share of the profit from Mev uh well the marginal contribution of each roll up to Mev is not transparent in fact Mev there is no real public deterministic function that you can just run on a given ordering to determine what the Mev is rather Mev is typically based on private information available to various actors in the system and is only discovered by running auctions and figuring out what different actors will bid it's very hard to you might think we could simulate the auction with each roll up independently but it's very hard to simulate an auction truthfully if it's not being run for real so this is a hard problem and uh I and leaving it as an open question that needs to be solved the second open question that I think is very important and interesting is whether shared sequencing layers can be any mechanism independent so there are many different approaches to addressing Mev from uh and it can be a matter of philosophy right from optimizing it and democratizing access to Mev to preventing it entirely whether you believe that's possible or not so there are many examples of approaches from auctions to first come first serve ordering protocols threshold encryption protocols time delay permutations Etc some involve various assumptions such as honest majority assumptions so the question here is whether a sequencing layer that is shared by multiple Roll-Ups can be agnostic to the Mev mechanism favored by each Roloff uh it could be quite simple I think for threshold encryption because the rollup could introduce its own separate threshold encryption set and all the inputs that come to the shared sequencing layer are threshold encrypted but it may get more complicated when we start considering other types of Mev mitigation approaches so I leave it as an open question whether shared sequencing layers can truly be Mev mechanism agnostic in any case this these are all problems that we are working on in espresso systems espresso is developing a shared sequencing layer for the roll-up ecosystem specifically focusing on ethereum today so thank you very much that concludes my talk yes foreign I'm Ed Felton I'm co-founder and chief scientist at off-chain Labs the company that developed arbitrine and I want to talk to you today about arbitrim orbit the next step in scaling ethereum orbit allows you to launch your own arbitrum chain and to customize it to meet your needs so that you can have the freedom and the scalability that comes from supporting your own chain so let me start by talking about Roll-Ups going back to the beginning of ethereum scaling Roll-Ups of which arbitrum 1 of course is a famous example are the leading approach to scaling ethereum and Roll-Ups have a bunch of advantages they have low-cost transactions by moving most of the cost of a transaction off of ethereum onto the roll-up itself you can reduce transaction costs by a lot when done right a roll-up can be fully trustless with security rooted in ethereum and interactive fraud proofs the interactive fraud proofs are very important because they're the guarantee of security and the arbitrim stack is the only optimistic roll-up stack that has working interactive fraud proofs and then of course a roll-up properly done is also a drop in compatible for ethereum with full support for evm equivalents arbitrim currently offers two chains that are managed by the arbitrum Dow arbitrum 1 and arbitrum Nova and there's and although they rely on the same core technology they have slightly different um slightly different technology and security models and therefore different costs arbitrum one is a general purpose optimistic roll up it runs on the nitro tech stack which is fully evm compatible has been live on mainnet for about a year and a half now uh for I'm sorry has been for about six months now call data in arbitrum one the call data of your transactions is posted directly onto the ethereum L1 which guarantees that it's available for everyone gas savings well depending on what you're doing it's often 20 to 50 times cheaper than L1 ethereum your transaction is is uh is result is provided to you in less than one second and it's used widely for applications like defy and nfts arbitrim Nova on the other hand is a gen chain using what we call the any trust technology also based on the core nitro tech stack full evm compatibility the difference from arbitrum one is that with Nova instead of posting the call data on ethereum it's instead sent to a special data availability committee the result is a big savings in cost even compared to arbitrum one depending again on use case 30 to 150 x cheaper than L1 ethereum uh the average transaction response time is still less than one second and it's often used for applications like gaming and social so let me talk a little bit more about arbitrim1 and again I'm deep diving into these Technologies to show you what's possible with the arbitrim technology because you as a user of arbitrum orbit will have the ability to launch the same technology that's powering arbitrim1 and Arboretum Nova and have your own version of these sorts of Technologies so arbitrum one arbitrum one really was the initial arbitrum chain developed initially by off-chain labs and it's in alignment with vitalik's vision of a roll-up Centric ethereum roadmap basically arbitrim1 as I said your transaction call data is posted to ethereum um L1 to L2 bridging is done via a bridge contract that can self-enforce the validity of transactions the security mechanism uses fraud proof so that if malicious behavior takes place security is still guaranteed by the ethereum L1 and it's very it provides a very strong guarantee it's not subject to the kind of 51 attack or even 34 attack that some side chains have arbitrum one by its nature is able to withstand uh the corruption of a large number of validators in fact you need only one validator that is correct and behaving honestly and is available in order for arbitrum one to guarantee security that's the key advantage of optimistic roll-up technology if you have fraud proofs as arbitrim1 does the arbitrum one ecosystem is very large more than 2 000 dap Integrations you can see a lot of logos here lots of different dapps especially in the D5 and nft space currently over a million eth reside in the arbitrum bridge that is more than the 10 than 10 other major chains combined you can see on the right hand side and uh an illustration of the dominance of arbitrim1 and the eth flows over on the left um and I'm showing you this not just to show you that arbitrum one has a huge market share although it does but just to show you that that the community of L2 application developers have voted with their feet for arbitrum one over Alternatives because the arbitrim technology provides a level of performance and reliability that they really like so if you look in terms of measures of value locked arbitrum one has depending which day you check 5.8 billion dollars worth of value locked and roughly 65 percent of the roll-up market share arbitrim1 in Nova arbitrum Nova because it offers because it uses external data availability service it's able to offer much lower cost per transaction even than arbitrum one uh and so it's it's been very attractive for for games and you see some of the games that are built um in in arbitrim here let me talk shift gears talk a little bit of our about arbitrim Nova arbitrum Nova as I said uses the any trust technology which is also available to you as an arbitrum orbit developer arbitrum the arbitrum any trust technology indeed is very attractive if you want to run your own chain because of its low cost so the way arbitrum novel works is uh the transaction life cycle is a little bit different from arbitrum one first the arbitrum sequencer will batch and compress the incoming transactions in order to be able to encode them as efficiently as possible the sequencer then submits that data batch whereas arbitrum one would have put the call data after batching and compressing it onto the L1 ethereum chain arbitrum Nova submits it to the data availability committee I'll call that the DAC and then requests the data availability certificate from the committee the certificate proves that the committee has collectively promised to store that data for you and if you have a valid data availability certificate that means that if even two of the committee members are honest that you know you can get your data back once it has the data availability certificate the sequencer posts that data availability certificate to ethereum thereby proving to ethereum him that that data that backs the next set of transactions on the Nova chain will be available through the data availability committee now if for some reason it's not possible to generate a data availability certificate because who knows maybe some of the data availability committee members are down or not cooperating then the chain will smoothly and transparently fall back to roll up Post Its data on ethereum and continue to make progress so you get the advantage of having a very efficient data availability committee when it's available which on arbitrum Nova has been always in the history of the chain but if for some reason that's not available you don't give up the safety of being able to fall back to roll up okay so let me talk about reddit reddit has adopted arbitrim Nova arbitrum one Reddit scaling competition in 2021 this was fiercely contested among a bunch of different L2 scaling Technologies Reddit decided to go with arbitrim for this so Reddit Community points are migrated on chain on arbitrim Nova with a wallet slash Vault that's built into the Reddit app when Reddit stood this up that they migrated about a quarter of a million Reddit accounts onto arbitrum Nova with their state at a total cost of about Seventeen hundred dollars Community points are live in two subreddits and the Reddit Community is quite large 400 million users per month and one of the reasons why Reddit chose arbitrim Nova is the ability of arbitrum Nola to Nova to scale up in order to support the level that is the ability of Arboretum Nova in order to scale up to support the level of um of of scaling that Reddit needed in order to in order to know that they could bring all of those users into the arbitrim Nova ecosystem so the arbitrum Nova data availability committee has a set of well-known parties including Reddit Google Cloud consensus openc and of course uh we at off-chain Labs ourselves and again you need only two of these parties to behave honestly and provide data uh when when they promise to in order for the Nova data availability system to guarantee access to your data okay so what's new in arbitrum the biggest thing that's new in arbitrim is the evm plus concept and the stylus technology that implements it so the way we look at it is evm equivalents the ability to run any program that could run on ethereum any contract or application is crucial uh for for arbitrum and we've supported that from the beginning now but that's just a starting point the stylus technology which is uh which is currently in advanced development expands arbitrum 1 and Nova and can expand your your orbit chain to run smart contracts that are not written in evm but are written in other languages it's compatible Stylus is compatible with rust C C plus plus for writing your contracts in fact any language any tool chain that can compile to wasm web assembly you can use that tool chain to write a smart contract that will run on an arbitrim chain using stylus so that opens up a vast set of languages and just as importantly tools for writing smart contracts now I want to emphasize this is not a separate chain this is not a separate Silo within an arbitrim chain this is fully interoperable fully composable with smart contracts that are developed using the classic evm and solidity development stack so what you get is that full interoperability users can interact with these stylus these stylus supported contracts they can call back and forth with evm solidity contracts indeed if you develop with stylus your users don't even need to know that you are using stylus they'll just get the benefit of your use of an advanced software development stack also significantly we expect contracts that use stylus to get a significant performance gain compared to evm and the reason for that is Sim simply that wasm is designed for efficient execution and it's possible to execute wasm much faster than you can execute evm so a significant performance gain again with full compatibilities so this is additive to the evm ecosystem hence the name evm Plus okay now let me talk about arbitrum orbit having talked about the main arbitrim chains and the technology let me talk about orbit arbitrim1 and Nova currently serve many thousands of applications millions of users they secure billions of dollars of assets yet some applications perhaps yours want to go a step further orbit lets you launch your own chain which you can customize however you like while retaining the time-tested security and scalability everything that people love you and others love about arbitrim you get to keep but you get to have your own chain and uh and customize it orbit is permissively licensed so that you can customize and launch your chain on top of arbitrim1 or arbitrim Nova without needing to ask permission from anybody so what does Orbit give you well it gives you Nitro the arbitrum Nitro stack which is time tested and extensibility not only will you benefit from updates that arbitrim1 and arbitrum Nova deploy as they become available but you can also Implement updates that the arbitrum Dao for whatever reason has not adopted for arbitrum 1 or Nova so you get not only the advantage of the current arbitrum note Nitro stack you get the advantage of further developments in the stack whether they're done by off-chain Labs or by others in the community so this gives you more flexibility than the public chains uh you get increased gas price reliability many types of dapps rely on having predictable transaction costs because the orbit chains are somewhat isolated from a gas cost point of view from the arbitrum L2 and ethereum L1 traffic using an orbit L3 chain means that you have more reliable gas prices which means that your dapp's users can rely on more reliable transaction costs you get account abstraction account abstraction as as popularly implemented these days on ethereum Via erc4337 that is supported on arbitrim that's available to you right away so you can use that predictable gas prices help make alternative cost models uh more feasible such as subsidizing fees or indeed replacing transaction fees entirely by say some kind of subscription model or whatever it is that you want to do for your application this helps you further abstract away the technical complexity of decentralization helps you deliver more familiar experiences to non-technical audiences all the flexibility of account abstraction is there it's on arbitrum one it's on Arboretum Nova and it'll be on your orbit chain there's also the ability to customize the protocol logic now many people won't need this but if you do need it or want it you might want to modify the logic of your chain settlement its execution or its governance protocols in either in order to meet whatever your specific requirements are and you're allowed to do that the license allows it and the software is flexible so that it so that you can make the customizations that you need orbit cell 3 chains let you do that while still deriving security from ethereum L1 through the arbitrim Dows governed L2 chains arbitrum 1 and Nova so you get the ability to customize and also the security Foundation of arbitrum under you there are a lot of decentralization options here arbitrim roll up derives at security from ethereum you can choose to use arbitrum any trust with the assumption that you can trust the data availability committee orbit L3 chains let you use either of those models you can also introduce additional trust assumptions if you want in order to to meet specific needs or you can go with that simply take the default off the shelf trust assumptions this again is up to you it's customizable and of course there's low upfront setup costs orbit L3 any trust and roll-up chains can be created quickly and cheaply because they're on top of arbitrum one or arbitrum Nova which have much lower cost than uh than than launching on top of an L1 you're going to benefit already from the cost Savings of arbitrum 1 and arbitrum Nova your chain although almost everything that happens on your chain will be within your own chains uh resources and economics you do rely to a lesser extent on the Chain underline chain that you're built on and by building on arbitrum one or arbitrim Nova you get lower cost and more flexibility and you benefit from that as well security obviously orbit chains can be configured out of the box as either any trust or roll-up chains they can be further customized as well but you get the benefits of arbitrim security mechanism you get working fraud proofs not only working today but a fraud proof technology that has been live for a long time and has serviced many millions of transactions on arbitrum um if you want you can permission access to your chain you have the option to enforce transaction submission or contract deployment restrictions whatever the goals of your chain or your business model is you have the option of doing that now I have to say there are some drawbacks to running your own chain these are drawbacks of running your own chain no matter what technology you use and I want to acknowledge them because um although orbit chains or running your own chain is attractive for many use cases it's not for everyone first of all there is some friction at the at the chain boundary you'll need to connect to a bridge in order to leverage existing defy infrastructure instead of natively having all of those teams build on your chain right so um on the layer 2 arbitrum one and arbitrum Nova there are many users there are many applications already there for synchronous composability if you build your own chain that won't be there and you'll have to get those people on board or else use cross-chain Technologies which are which are good and improving but still that will not be without friction there's also more infrastructure complexity if you're running your own chain if you're using one of the uh one of the existing large chains then all that's provided for you but if you have your own chain you need to think about RPC providers Bridges block explorers exchanges uh Fiat on-ramps and so on all that's already taken care of if you're using existing public chains if you do it yourself of course you're going to need that although of course the whole idea of roll up as a service is to provide that for you as a service and to reduce this drawback okay so is arbitrum orbit right for you well many apps are better served by public chains due to the widespread infrastructure support as well as the synchronous composability with other applications but if you want the scale the flexibility the customizability the ability to control economics and governance that come from having your own chain orbit will do the job for you and will do it well now the the emergence of rollup as a service infrastructure is really important here because the rise of Ras infrastructure support is going to shift this trade-off to make it more attractive to launch your own chain via orbit for more use cases so we're really excited about arbitrim orbit we think it provides huge opportunities by bringing together the time-tested arbitrum technology and the ability of the arbitrum community to deliver reliable and efficient software um making but making this available to people who it wasn't available to before we're excited about orbit we're excited about the growth of Ras infrastructure uh and the RAS economy and we'd love to see you be a part of it if you want to learn more about orbit you could go to this URL here arbitrim.foundation orbit there's plenty of documentation there including instructions for launching your own devnet chain to test it out yourself on top of the arbitrim gorley test net with instructions in the future for launching on uh live on mainnet you're going to hear more exciting announcements about companies and teams using orbit we would love to have you join that community so please reach out if you're interested thank you very much thanks for your time and enjoy the conference hi everyone this is almaya here the header Investments at matalabs and today we'll be talking about the economic considerations of modular roll-up services foreign ly I'd Begin by trying to Define what a roll-up is but given what we've seen on Twitter recently it turns out it's a little bit more difficult uh than what it seems at face value you know all one has to remember is that a roll-up tries to move parts of the execution from the base layer from the L1 off chain while saying something about the guarantees of the data availability and then It ultimately ends up posting a smaller version of the data that is required to reconstruct the state or at least say something about the validity of the state back to the L1 so the reason that overall up really is to guarantee the the correctness of off-train execution as well as say something about the data availability behind that execution and a few other things to remember the roll-up nodes actually depend on the host chain in order to be able to update their state because the host chain defines the state transition function and as a place to nominally store their history the history of the actual roller we know that the actual layer 1 its consensus and then therefore it's security does not in any way rely on the roll up because it's not waiting on a block by block basis uh to determine what happens on the rollup instead every once in a while the roll up will post its state which has to be consistent with the rules defined by the L1 and the the bridge between them if if you were to use the word bridge is the ability for the layer 1 to check the validity of the state transitions that are proposed by the L2 every once in a while now usually people talk about this idea that they roll up in a sense partitions the host chains consensus are really it's it's state by allowing it to apply a subset of rules that only really apply on the roll up so long as they are uh globally consistent with the rules that are prescribed by the L1 now because the roll-up needs to know about its own State as well as the state of the L1 at all times then the cost of running a roll-up is strictly higher um than running just in L1 because you're basically now having to take care of two different chains so this must mean that there is there must be some utility that's generated by using a rollup that justifies spinning it up now this might seem obvious um but since we're going to talk about the economics of Road Labs I think it's important to highlight now something to remember is that blockchains body design are extremely computationally intensive it costs approximately 10 million times more to do a calculation per byte on ethereum than it does on AWS that is the cost of achieving the properties that are promised by a distributed Ledger and and this is really the sole motivator of considering uh roll-up economics because a minor Improvement in the execution environment can reduce the costs dramatically and you can see that this Gap is enormous and so there is sort of a lot of margin um to be gained by trying to cheapen the execution environment while preserving essential persistence decentralization and so on now the the basic idea is that instead of submitting transactions or series of transactions on the L1 and incurring the sort of base layer cost you move this process off chain and there's sets of actors that are responsible for essentially doing this and then posting the results back on chain now depending on the type of layer two whether it's sort of an optimistic design or a pessimistic design pessimistic here refers to validity proofs of course um these operators behave in slightly different ways and use very different technological architectures but the basic principle is sort of the same and as economic agents they have to think about the following question how do I provide these Services while pricing them in such a way that it is still profitable for me to do so and while the user experience is at least as good as what they would experience if they were executing directly on the L1 now there is this concept of application specific blockchains which I refer to more generally as specialized execution uh chains whether Roll-Ups or not um while using validity proofs or fraud proofs or whatever type of mechanism they handle the execution in a modular way and this modularity relies on a simple principle that you are able to share the security of the underlying L1 which has a cost and perhaps there is also a way to communicate between these different uh layers with or without going through the base layer now the utility of these modular setups lies in their ability to remove the constraints that are normally imposed on a network that's trying to bootstrap a cheaper execution environment but also incurs fragmented liquidity different ux additional latency and the premise is that these New Primitives that are being built from a technological perspective allow these modular networks to build a stack that is virtually vertically integrated and does not detract away from the user experience and certainly offers a much cheaper execution environment now when it comes to the economics of these layer twos the name of the game here is to achieve positive Network effects for unit economics without materially sacrificing decentralization and certainly not security and what I mean by positive Network effects for unit economics is that you basically escape the Paradigm that you're normally constrained to on the L1 where the more transactions that you have on a monolithic L1 on average the higher the cost of a single transaction becomes and that's basically a statement of um Network Demand right for a for a fixed uh computational Supply as the demand for computation increases the price increases on the L2 and that is really the fundamental insight there is this ability to batch transactions together and then there is this economic regime where it is possible that as the number of transactions increase across the network you can actually have lower costs right it's it's simply due to amortization right you compress um uh some parts of uh the data that's required and you do so um by allocating costs to multiple users now of course if there's only a single user of the network this is sort of moot but in the case that you have many many users each with a small subset of transactions then you can actually have decreasing costs Now by definition again the utility of the user of the L2 has to be strictly larger than the cost of running the L2 individually and the cost of publishing data to the L1 which I really term here is the cost of inheriting the security there's sort of no free lunch when when people say that the layer 2 in some way inherits the security L1 what they're really saying is that there there is a cost to publishing data back to the L1 as long as well as the cost to actually reading the state of the L1 yeah why is all this important well a layer two economically must be designed such that the utility of using layer 2 for a particular user is strictly higher than using uh than the utility generator from using L1 and the same is true for the operator we call a layer 2 economically sustainable when the utility of both the operator of the roll up and of the user is strictly higher than than uh operating on L2 than it is on the L1 now here's where we get into some of the architectural uh trade-offs or design trade-offs associated with these different agents so in general we have um somebody's responsible for the data somebody who's responsible for ordering the data somebody who's responsible for uh the state of the road lock itself and then somebody who's responsible for generating um some sort of proof uh that's associated with the validity or the correctness of the state transition now again depending on whether we're talking about a validity roll-up or an optimistic roll up um the the generation of the proofs is a sort of fundamentally different process but when it comes to sequencers in many ways they're sort of the same the sequencer is the agent that is solely responsible for ordering the transactions and then they send that sequence of transactions to someone who ultimately uh uses them in some capacity to update the state and the idea behind running a sequencer from an economic perspective is straightforward to basically the sequencer has to design usually algorithmically on uh an order of the transactions that is strictly different than the order in which the transactions come in that maximizes some sort of Revenue now of course there there might be there might be some cases or some instances in which the ordering itself um is constrained by some sort of fairness algorithm I mean in in the in the case of centralized sequencers where the user simply trusts that the sequencer will uh or your transactions let's say first come first serve basis um the sequencer is simply paid for the fact that they do this of course the sequencer runs a particular set of hardware and which requires some off-chain compute costs and they at least need to cover those costs and perhaps make a small uh profit on top of that the rules by which they order the transactions uh highly depends on the environment and also the actual um designation of the agent as a sequencer so in the case where you have a single sequencer it's sort of trivial um this person is guaranteed or this approach is guaranteed to be sequencing their standard actions and so their economics are sort of straightforward whereas there are cases in which the sequencer is decentralized so that you have multiple agents that can perhaps bid or can be randomly selected to generate the canonical uh ordering of transactions and it is in that case that the economics start to become non-trivial first because you now have to ask multiple agents to participate knowing that only some of them might be able to propose the sequence of transactions and so if that's the case then there also has to be some sort of profit sharing scheme amongst them so that the ones that are idle or at least the ones that are not at the front of the line are able to cover their opportunity cost or at least their sort of Hardware costs now this in and of itself is an entire uh sort of area of research but the thing to remember is that the sequencer um even if they were to try and be malicious really at best they can only sense their transactions or they can um slow down or in some cases stop the operation of the roll-up but they can't insert a malicious transactions because the process to guarantee the Integrity of the transaction is sort of completely separate from the sequencer function however what that means is that we still have to worry about censorship resistance and the aliveness of the Roll Up the roll-up is uh useless if it is completely secure but doesn't do anything um and so you what you you want to design the sequencer architecture so that a it maximizes uh liveness and B it also enables multiple sequencers to come in not only for the sake of decentralization but actually for redundancy and and this is where the economics become very very important and there are different uh sequencer design approaches um some of some of which include bonding or a proof of stake type sequencer where the sequencer guarantees some ordering of transactions and if that guarantees is sort of not upheld by the sequencer then they they are slashed they can also guarantee certain transaction selection criteria they can price their own transactions they can use a global market for determining fees or a local market for determining fees and most importantly as we mentioned before they can also say something about the fairness of transaction processing and what that means basically is that they commit X ante to a scheme that orders the transactions and you can go back in time and audit that and and make sure that that's indeed what the sequencer has done now the really interesting uh Parts about the economics of running a sequencer is that now that you have basically separated the security of the underlying L1 from the execution environment you now have opened up a design space of different types of economic instruments like derivatives that could be used to run a sequencer a sequencer in many ways has to um hedge against uh the fluctuation of their variable costs and their variable costs are of course tied to the L1 and they're also tied to the transaction demand and so you can easily see that having something like a collateralized sequencer obligation which is an instrument that's sort of analogous to a debt obligation but in this case the debt is not monetary but rather the users commit to using a particular sequencer and their commitment has to be uh guaranteed somehow and then the sequencer in response to that commits to some sort of soft finality and deposits collateral as a bond and what that does is instead of being subject to the fluctuations of the spot Market whether it's on the L1 costs or on the transaction demand costs on the L2 the sequencer has some sort of forecasting ability in terms of the demand for their services so this becomes very interesting because the you can now have a set of sequencers in a competitive market go out and issue different types of instruments that are tied to their future performance and these instruments can be priced based on their previous performance so in the same way that you have credit rating agencies um issue different analysis and ratings for the credit worthiness of different financial institutions or companies based on their balance sheets based on their Workforce based on their previous performance and that enables the marginal creditor to think about the risk that they're taking by allocating capital or committing to a long-term contract or or some sort of service agreement with this particular provider you can now develop a reputational system upon which you price the uh future performance or rather discount the future performance of these different operators so that becomes quite interesting not just from a pricing perspective but also it makes the economic forecasting problem for the sequences themselves um much more tractable and this is particularly valuable in the case where you're using um some sort of proof of stake based sequencer where it becomes uh very important to know up front what the cost of running the sequencer is going to be for a given period of time because there is also the cost associated with hedging that exposures particularly if the um a bond is not in a numerator but rather in a different currency something like eth so that the operator can hedge their exposure to that currency as well so now it's probably a good time to talk about Mev and what that generally refers to is the capacity to reorder transactions in such a way that the person who does the reordering or the entity that does the reordering benefits at the expense of another entity and of course from their perspective this is a positive and from uh the counterparty's perspective uh that's a negative um but it's useful to make the distinction between basically the two different types of Med uh there's a creative what I call a creative and what I call parasitic mov parasitic Mev uh is exactly what it sounds like um where the uh all of the agents external to the sequencer experience negative or at least the vast majority of them it's negative due to the reordering of the transactions and accretive is sort of the opposite now a good example of parasitic mov would be something like sandwiching right um uh on on some sort of Exchange and a good example of a creative mov would be something like interoperability as a service whether it's with a different type of Roll-Ups auction clearing is another good example these are sort of accretive to all of the participants external to the sequencer now my hypothesis there is that there is enough diversity already in roll-up providers and now I'm referring to different Roll-Ups and not even necessarily sequencers that parasitic Mev uh the operator has no choice from a social perspective but to rebate it or pass it back on to the user because there is enough of a competition for roll-up providers let alone different sequences that will incentivize users to move away from the environments in which there are parasitic Meb now obviously in in the case where for a single roll up you have a market for sequencers that just becomes even simpler in the case of creative Mev that's where the argument could be made that in the same way that a private company provides some value for a user and it gets to keep a bit of that value in the form of profits um the operator or the sequencer that provides a creative mov to the users will be allowed to keep that as part of their p l and it shouldn't be surprising because something like auction clearing uh obviously requires running some sort of algorithm as well as incurring Hardware costs so of course the operator should be paid for that and the users would be happy to so long as it's within their utility framework to pay a little bit extra for the profit of the operator now speaking of uh profits we know that as you mentioned before a marginal Improvement in the cost of execution of chain can provide a dramatic Improvement in the overall cost of execution because the cost of execution on the L1 is so high but another thing to remember is that on the L1 the inter machine workload variance is actually quite small so by that by that I mean most validators are are running similar workloads and why that's important um is that on the L2 the variance is actually quite large and the reason for that is the same uh reason that we mentioned before this idea of decoupling between the execution uh its efficiency and security this leads to what I call a sparse market for L2 computation in the sense that you now have a very wide spectrum and therefore Economic Opportunity for The Operators to prove different types of transactions different types of workloads and sequence uh different types of transactions as well so now the name of the game becomes uh what different optimization routes or strategies could be used to reduce the ultimate cost of running the L2 whether it's from the sequencer's perspective whether it's from the approvers perspective and so on and so forth and this is where of course we have validity proofs that come in in all different kind of flavors and forms we have data availability solutions that change the architecture and therefore the economics of storing and querying State data um how you can become uh extremely efficient in terms of sampling some of that data to reduce the burden of storing the entire State and perhaps maybe you only need to store a small subset of that state which then leads to um this interesting problem of State growth management so l2's uh while they way in which they operate a sort of fundamentally different from the L1 because of the decoupling that we discussed before at the end of the day the uh Roll-Ups themselves have a state that they need to keep track of and um the state the the state will grow over time it it might grow slower um than what you'd expect for normal L1 but again that that sort of depends on the architecture and the operational model but at the end of the day they run into exactly the same state management issues as the L1 does basically it becomes um more difficult to sync the full history of the node because you simply have a large amount of data so the name of the game then becomes how do I incentivize external operators to efficiently store parts of the state and enable querying parts of the state and how do I do so without burdening the roll up itself because if you need a roll up for the roll-up then we haven't really done we haven't really uh achieved very much so instead in your palette of uh resources and and operations that one considers in designing a roll-up they also have to consider how do you manage the state of the rollup so that you don't run into exactly the same problem that you would normally on the L1 and this is where the data availability problem um comes into play and uh ultimately the cost of verification of the validity proofs or of the fraud proofs as well as the cost of publishing the data back to the L1 um always provides a sort of ceiling for improvement because you then rely on the L1 itself to improve in order for you to reduce those costs so instead you have to think about okay how does one improve State Management on the L2 side and incentivize off-chain data availability using different types of models you know volition validium and so on and how that introduces coordination problems latency problems um but also how that allows different roll-up operators to bootstrap um their their systems uh from from scratch so now another topic that we touched on was how does one enhance liveness and finality on these layer twos and again returning to this decoupling we now have this interesting opportunity for for different types of economic agents to come in and actually improve uh the aliveness properties on the l2s without necessarily having to coordinate or having to participate in the day-to-day operation of the rollup and here's what I mean by that you can now have what I call dedicated Fail-Safe sequencers that are paid a small fee that is weighted towards liveness without actually processing any load day to day they sort of behave as an insurance sequencer and if you think about insurance contracts in general you generally pay a premium for something that you hope that you will never use or never need to use and that's interesting because in that model you're basically paying for an agent to just sit there and do nothing and so long as they can provide certain guarantees that in the event that something happens that they're able to intervene you're happy to pay that premium these sequencers are basically being paid to be redundant and just sort of sit there and the reason that's interesting is because you can pay these uh agents in a non-native token so that it's not tied to the security of the L1 in in any way the other interesting uh aspect is that we know regardless of redundancy regardless of decentralization you can never guarantee 100 uptime that is a real problem for l1s but for l2s you can also never really guarantee uh 100 uptime but what you can do is actually make the net cost of downtime much lower when you use these sort of clever economics so you can essentially socialize in the event of significant downtime you can socialize the loss across the multiple set of insurance providers and that are willing to underwrite that process now when it comes to finality we know that it's ultimately determined by when the proofs are posted back to the L1 and then the uh last roll-up State um that's that's considered to be valid is now sort of canonical and and immutable so the the name of the game here becomes how do I provide a market for uh finality and that's simply determined by uh the transaction inclusion and we know from a user's perspective that they can force inclusion in the L2 at the expense of some minimum costs so assuming that they have no time preference it is strictly cheaper for them to send their transaction to the sequencer but in the case that they do have some time Preference they now have to weigh that against the cost of including the transactions themselves and the probabilistic guarantee by the sequencer that they're going to include the transaction so in the same way that we discussed these issuance of these derivatives tying the performance of the sequencer um to the native demand we can also include a similar type of instrument that guarantees inclusion within n blocks or within a a finite period and these themselves could be priced so not only can the sequencer uh guarantee or forecast something about their future costs and the transaction Demand on the L2 the user can also instead of using this very coarse uh uh gas scheme lever that they're normally used to using in Memphis on ethereum they can now have an entire spectrum of inclusion guarantees which gets us into the topic of transaction quality and by transaction quality uh I'm I'm really referring to the spectrum between a pristine transaction which is a single Atomic transaction that is very cheap to execute and is included effective immediately versus a series of complicated uh transactions that are expensive to perform and atomicity is not guaranteed um and most transactions or most series of transactions as a subset sort of lie somewhere in between and again you can now not only have a market for single transactions you can also have a market for bundled transactions and the reason you can have this is because you can now have these specialized sequencers that offer you these sort of services you can have in the case of multiple sequencers where one of them specializes in D5 like transactions or an nft like auction like transactions and so on they can offer you these bundled services uh as the most Market efficient way of executing them so you can see just based on the discussion that we've had before that the added uh um benefits of having multiple sequencers is not just from a redundancy perspective but also because Vector of customizability of transactions that on the L1 were normally handled through um off-chain agreements or through something like flashbots um whereas now you can have a much more general purpose a much more robust way of doing so and more importantly you can price it and so now in an era in which uh applications will start to really think about how to subsidize or even eat the entirety of the gas transactions and just incorporate them as part of their p l in order to improve user experience they really need to be able to price not just single transactions and there are latencies and and atomicity but also um they need to be able to price bundles ethnicity of course is sort of trivial in the case of a single transaction but it's very very important in uh complex so now that we're talking about uh pricing there is uh one thing that I want to highlight which is that the network fees on layer 2's fundamentally uh differ from those on the L1 and it's really for a very simple reason the fundamental difference between L1 and L2 economics is that the native L1 assets command something that looks like a store of value Supremacy whereas the L2 simply does not and the reason for that is is quite simple if you inherit the security as an L2 from the L1 you must be economically subservient and what that means is that the security layer has to be guaranteed by an asset that is not native to the rollup but what it does do is that it opens up the design space for off-chain data availability and other off-chain services because now the roll up can serve as the vendor for these types of services which brings me to perhaps the most important uh question how do uh these l2s if they were to issue tokens um how does value accrue to these tokens and more importantly do does one even need a token for a layer 2 system and there are sort of two schools of thoughts one school of thought says that uh simply because security is inherited from the L1 the L2 token can at best serve as a bootstrapping mechanism for the L2 ecosystem but it is designed fundamentally so that it becomes less useful over time once enough activity is aggregated onto the L2 the L2 token itself it's valuable sort of decay the other school of thoughts um is that the layer 2 itself because it can serve as a vendor for these modular services that we described before that uh it now effectively acts as the marketplace for all of these off-chain computational services and so that can be accretive not just to the L2 itself but ultimately to the L1 to the extent that it enables um more economic activity on the L1 than what you would normally get without the existence of the L2 um so then here the question becomes can a layer two justify the existence of a token not just purely based on the mechanics of running uh the roll up but instead can it offer value to the L1 that wouldn't exist otherwise and with that I will leave you with some open questions and again it was an absolute pleasure to be here if you do have any comments or questions about anything that discussed here as well as these open questions obviously please do reach out thanks again thank you uh hello everyone um so far I hope you enjoy all the sessions around uh my sort of roll up and share sequencing with staking I mean we are also the economics around the uh brand systems and uh I'm Yoshi today I'm glad to share a little bit about what we've done for the past and one and a half years to really help to connect multiple relax and try to build something I would say brand new for this real life space we call bikaner basically it's an ensuring your lab interlayer for ethereum and other chains um so as we go into this uh Insurance develop right we need to talk about like sort of what are the existing your lab categories we have so why is we call smart contraryola or classical blue Labs right like I will from optimism and the various exam and ziki sync so for these Roll-Ups right we have a sort of a sequencer Network right now mostly single sequencer but at the same time since we say it's a roller basically we derive the security from the L1 typically ethereum right so we also place the smart contracts like validity proof or broad proof contracts and also Bridge contracts on er1 to allow user deposit and withdrawal assets and meanwhile as we can see right most of these Road Labs we also post the data typically the comprise transaction data to the L1 to make sure this data available on L1 so literally some users or people want to challenge the existing sequencers right so they can just download data and just do the verification on it and beyond that recently we also got a lot of progress from Celestia and this is Sovereign security from the ethereum or the Airlines and also uh do very uh do all the da and verification on their one for sovereign typically like this romant and Sovereign Labs so what they do is like they have a separate sequencer number one and all the sequencing and the verification are done by the sequencer Network and beyond that they also puts the data to a typical DNA like let's get a d or the eigen DN and in contrast to doing all the proof on ethereum and on the L1 and also the bridge contract on the R1 for the software roll up basically you have more sovereignty so you can have your own sequencer network but at the same time you don't really derive the security from a dedicated L1 so basically you have the sovereignty to really handle all the staking and sequencing by your sequencer Network meanwhile you can also rely on different Bridge providers probably some of them will give talks later uh this conference and as you can see right uh class is Roller mainly uh depend on security from the l1s and for Sovereign we have more freedom to run the nodes and the sequencers and posting the data to a dedicated DNA and compared to the existing these kind of two types of rollout right there's another new type recently being discussed a lot by ethereum Community it's about instrument roller compared to all the two types of rollout right we for the insurance roll up it's to another social Extreme as we all know for ethereum right the future for engineering skating is really about these Zoloft Centric scaling approach so in that case as we can imagine for the next few years once they relapse become more and more mature so we can see ethereum because the main settlement layer for all different Roll-Ups and applications so that means already you can ensure tokens and the transfer assets on ethereum same time ethereum will also become the total of the go to Da platform so for all the rollers you post the data on the ethereum and ethereum will host the data so right now for most of the existing Road Labs like The Classy roll up we still place a smart contract for fraud proof and the validity proof and bridges on ethereum the United States we all know right since it's a smart contract animals can be controlled by modestate wallet right so in that case it's a security I3 really derived from the multi-c wallet operated by the developers of different ruler teams um to enhance or improve the situation as I just mentioned right for The Classy Lola and some researchers proposed the insurance alola way so in that case uh as we just mentioned right for the verification logic like for approved for the validity proof and also Bridge logic previously they are in this smart contract but it is sure in your lab we embed this logic into protocol the next is um as you can see right in the future for ethereum scene if we have hundreds of rulers right for this rule up we can still have the execution uh in different sequences but regarding the proof verification and the settlement and also bridging logic it can be in the ethereum protocol in that case for any major upgrade for each run right probably we require the ethereum to do some soft or hard Fork um so the benefits we can and have as I just mentioned right basically we can further enhance the security uh from like this smart contract security to the protocol security for the attacker if you want to compromise what you try roll up right you have to compromise the protocol level like this ethereum uh protocol level so it's super super difficult compared to journey to target attack a bunch of modesty wallet holders right on the other side right since we can move sort of the logic from the smart contract to the protocol sometimes we can use pre-compel or even we implement the protocol level with rust or Goku in that case we can massively enhance the performance for handling this verification or bridging Logic for the existing or the future real apps uh in our case as you can see from this diagram right user can send the transaction to the execution client the client can be run either by ethereum existing validators or by some other validators but for the Jolla block and the consensual clients they will be handled by the ethereum validation so in that case we can really improve the security guarantee for all the rollers and also improve the performance uh for verification of these roll ups and meanwhile for upgrade right previously technically we we do in something like we use Snapshot or the option voting or governance uh mechanism right people do this option voting and if the proposal passes and we can upgrade the version for example from 0.1 to version 0.2 but in the end it's still a developer need to execute the proposal right by upgrade great the smart contract chain with his own at the main key are basically signed by some Odyssey wallet um without insurance approach as you can see right it's a little bit dangerous it seems in the end you sort of control by the modesty wallet but once we put into protocol level and then we can light the protocol I mean all the users can directly vote their their sort of ethereum level on governance right so we can have a better security guarantee for the upgrade um but as we can see right there's so many benefits and advantages for insurance lab but why it's not that popular I think the main sort of a reason he died it's requiring the protocol changes as I just mentioned right either for the uh sort of the logic implementation for the verification or for the uh for bridging or even for the upgrading right right now mostly at the Smart contract level but once we move it into protocol level right it requires a lot of changes on the uh on the validators and also the uh sometimes a consensus protocol so it's sort of like a major reflection of the existing L1 protocol and meanwhile it's sort of like we push these l2s responsibility to the L1 so if someone remember vitalik previously posted this uh poster right so he's saying that I don't overload ethereum consensus so sometimes like if we push too much logic into profitable right then the protocol I mean the R1 protocol itself has to make the decision for your L2 or your life protocols in that case is sort of like we need to find uh three spots or balance to really sort of have the good position between these rollers and Airlines if you put a lot of logic back to the L1 protocol level and then it becomes a big burden but at the same time if we can offload some of the logic to the L1 without sort of uh challenging too much on the performance of ethereum particle then it's also good right because we can achieve better security and performance compared to the existing smart contract implementation um so since it's a little bit tough to modify the existing R1 but as I just mentioned right there are so many benefits to adopt this kind of in front roll up approach um since I earlier as you guys all know right we're not a lot of Rolex and the users you can directly via our dashboard to launch your lab yourself so we need to manage this roller and uh on the back end we have this automation tools and infrastructure to automate the Enterprise but at the same time we want to provide the best security and also the best performance for all these roller uh since the L1 like doesn't really provide the infrared roll-up implementation at the moment so we come up with the beacon layer so as you can see right Beacon layer you can treat it as an interlier basically between the L1 and also the zolabs so big layer will help to provide all the benefits we just mentioned right the decentralized sequencing verification upgrade and also further with some social governance across roll up messaging uh where are these begin layers so so you can treat it as a really of castration layer for the rollers and uh and the rules will be all enshrined to begin layer but at the same time it still derives security from interior um because all the fraud proof bridging and all things uh still be deployed on the ethereum is that like a big layer is sort of the way for you to do a double check and as an augmentation or instrumentation layer to provide further security guarantee and also further decentralization for different role of systems and uh we just know why we need this sort of Beacon layer right it's sort of like we want to have all the benefits provided by the insurance roll up by the same time since hours are not already also we can have this inter layer to provide these benefits for the Dual Labs at this moment so what are the parts the only or Beacon layer can provide at the moment the third thing is about decentralized sequencing as we all know right right now for all the optimists in your Labs or ZK relapse especially EVMS we are still adopting the single sequencer approach to really provide better performance and the first Improvement on the exact existing development in that case we may have a bunch of drawbacks right as some people posted the comments on various forums like if it's a single sequencer it can sensor user transactions and sometimes like you know right due to the massive participants from from the community for airdrops or or sort of these uh claims sometimes we overwhelm the single sequencer and there will be downtime for the l2s or low life so then the sequencer becomes a single point failure and beyond that um even we can say that right now most of the sequences are run by the team right the team doesn't really have the incentives to basically front-run user transactions but there is still a potential like in the future like these sequencer can have the ability to front run or create mad bad Mev against the users so what we provide where the beacon layer the first thing is about decentral sequencing so we already have the multi sequencers for the Rolex so that means uh either dashboard and you can quickly spin off a number of sequencers of your Lola so in that case we can immediately avoid the single point failure right so since you have a lot of uh like uh we don't save backup notes but is there like you have a lot of alternative sequencers like once done you still have a bunch of them can really help you to produce the blogs and the verify the transactions and be honest since you have multiple sequencers and for the users they have more choices to send the transactions and in that case somehow we can mitigate this kind of Mev issues uh beyond that right we as as we all know like for Roll-Ups we have a sequencer also have the verification um that basically before posting data to the underlying lab R1 uh with a big layer right since a lot of application actually rely on some soft finality from the sequencer for example for most of the rollout right we finalize blocks within two seconds for software finally basically the sequencer generates the block within two seconds but beyond that we also have another fanatical higher finality the nodes a typical verifier or the proofer need to post this uh comprise the data to the L1 and then at that time in somehow we say heart financial and some application can confirm the transaction on the lq side but for a lot of bridges application right also some new wear they want to have a soft finality but at the same time with a better confidence in that case they need to run sequencer verifiers themselves to quickly verify whether the transaction blocks are correct or not in that case as you can see right a lot of bridges to provide this faster uh liquidity across different Road apps so they have to run a lot of infrastructure nodes and uh in the meanwhile right for a lot of exchanging we also need to make sure the transaction blocks are correct before the confirm the user transfer across different Road Labs or across the visual app to The Exchange Server um on our side right as you can see with the big layer It's relatively decentralized layer to do this verification uh before the uh the basically the red fire posted this data back to the L1 so you can really get a more uh confirmed uh finality from the beaten layer instead of running dedicated like sort of a node to do the verification yourself so in that case with this decentralized verification feature provided by a beacon layer right so the applications or some middlewares we can they can further reduce the cost of running dedicated nodes for a dedicated rollout and beyond that since uh um since all the rollers are opted in for the insurance to begin there so as you can see right with uh with the decentralized uh decentralized sequencing and verification all the rollers actually can have some finality on the beacon VR before we have the family on the uh L1 so in that case for some application they require this cross roller messaging across roll up like this asset transfer uh they can also rely on the beginner to give them some soft commitment before the really finalize their transactional data on the L1 so in that case as you can see right the big layer can be used as a low to really help to interoperate across different Road Labs so in that case we can say it's sort of like we make sure the message across a different route can be a term Atomic which means like when your lab is the message Lola B if the beginning or pass the message and confirm the very beginning so in that case you can really sort of like finalized across these rollers so it further helps different ruler to do quick messaging across each other foreign with a bunch of these nice features and benefits we can achieve we are beginner as it is enshrine the rollout interlayer what kind of Zoloft architecture we have uh for the current earlier um as you can see from this uh diagram right we have a well structured like sort of structure compared to other relapse or because we sort of provide this rule of a lot of applications like games empty and social applications so they really need this structure as robust as possible so as you can see right before the sequencer we have this aggregator actually behaves like a load balancer to handle massive traffic previously we did some empty means we got like over 30 or even 50 000 participants we need to scale this aggregator or low balance or elastically to really handle these users before they crash down our sequencer so United is the aggregator is a mass and after that it's a normal producer sequencer to precise a transaction right and after that as I just mentioned we also have the survive fire system but the cool part I uh the verifier is um sort of like in the wasn't block so people can run it either in a PC laptop or server and sometimes you can rewrite in your browser as a like client the United States of a lot of applications if you really want to make sure your transaction to be verified as as early as Paul right you can just embed this verifier into your application on the website so when users in transaction it can quickly verify all the transactions from the user side and beyond that I just mentioned right all the things approving systems are still on the air one but at the same time we also have the extra security guarantee from the beacon layer so regarding the typical precise to set up for Lola and the user can just send the request either at our dashboard or in the future where our public API um to the backend of course you don't really interact directly with the beacon layer and Technical with some RPC import and you send the request to be clear beginner will set up this dedicated ruler for you it's quite customized like you can specify Your da you can specify your L1 you can also specify a bunch of uh prime interest you want like block time and also the gas limit per block after that the beginner has a decentralized infrastructure it will select some of the top right view top reputation like the nodes and sequencers for the eurolab as the um as sort of the first set of sequencers to service the user and uh um all these things will happen in the background users or developer Won't See Asia like the entire protocol is a decentralized and possible and technically I just show you right for the finality side um for most of the show Labs right we have this software analogy and also these heart finality so financially uh for the fun uh when when it's sort of uh screen service finalize the block on the sequencer Network itself and the half analogy basically finalized when the uh when the verifier or this sub meter approvers submit the comprise the transaction data to the L1 and with the augmented uh with being augmented by the beacon layer right uh we have this picking layer to do extra verification for all the rollers uh as you can see uh apart from this software energy and also half analogy the beginning layer we'll do another verification called approved for validity verification uh you can treat it as this um One-Shot fraud proof because as we all know right right now for most of the proof it's uh it's a one it's a step-by-style proof with the bisection but here the beacon layer is strong enough to directly precise the proof for the entire um period of time uh period or challenge time like basically uh for the verification so in that case you can treat it as a canary detection when the sequencer generates blocks and before these blocks submitted to the L1 and the beacon layer will do the extra verification at the time whether these blocks are correct or not so that's why we call it Cannery detection and once the pick and layer fans die okay your block is in our Valley and the beginning will signal um the Challenger to do the challenge to the L1 uh much earlier than like this seven day traveling period or it can just happen within a few seconds the United States as a canary detection right the beginner can quickly do the verification and the further uh secure these security guarantee for the L2 while at the same time signal The Challengers to do the challenges as early as possible as a summary right compared to the existing L2 or rule of solutions right uh there are these big layers you can see we have uh like sort of two more uh lines of difference for the security guarantee the first one as you all know right uh if it's a single sequencer then the softer fantasy will be a little bit weak but since we run multiple sequencers then soft apparently at least uh much stronger than a single sequencer that's called stage one the execution level software finality beyond that I just show you right we have the picking layer to do this uh Canary detection for all the Azure Labs there are these uh proof of validity checks so in that case if all the checks pass that means uh to the verification level we are sort of quite secure I have a lot of application and bridges exchange the sort of contrast this level because a decentralized verification layer for them and uh of course on the stage three it's a typical hard finality we also have basically how we we implemented all the verification in solidity for the wazam instructions so all the proof and execution for the blogs and transactions can be verified on the L1 if there's a challenge happens and beyond that Beyond this uh decentralized sequencing replication and we and also this message passing right we also have this uh sticky and the slicing for the sequencers and rafiers as you know right once we have multiple sequencer verifiers and also the proof sometimes we need to know who behaves a good uh who gives on a good manner and who behaves uh wrongly in that case we need to apply this penalty mechanism it's called sticking oscillation so you can easily you can either do the staking slashing on the pick and layer which is you know in Shrine away and you don't need to deploy smart contract it's have a highly sort of secure with uh Beacon layer or you can also do it on the R1 side where the smart contract we already deployed for these different Road labs and beyond that right as we just mentioned at the beginning the upgrading is very important because we don't want the smart contract uh like to be upgraded by a single admin address right so in that case uh begin layer we also handle the upgrade for you so one you you can you can lie to the community do the unchain voting on Beacon layer and if the proposal is passed and then we can sort of pass over the wazen binary basically for the next upgrade very the beacon layer push two different Rolex that's how we really handle and manage multiple rulers and for their security patches and also these uh this upgrade for some major features so everything is automatic we are the beginning and pushed to different uh your lives I mean well you can also arrange like some unchain governance to be honest accurate and since it's fully answering you can just get all the traces for the votes and everything is transparent so as I share so much about this uh picking layer right is the insurance for ethereum and in the future probably for body chains the uh the end goal is really to provide the best security guarantee and also decentralization for the roll apps as much as possible for a lot of these customers and the projects who embrace the web 3 as the future um so right now what are the stuff you can do on in the community so you can participate in our bikiniers taking and also this is your last staking as I just mentioned right these features are quite Advanced you can just try it out right now I mean well you can also interact with this decentralized sequence in your lab and we already run a bunch of these sequencers for the Zola and with proper proof and Beyond the eyes of what you can expect to come soon uh one is like uh so you you can start to use the docker and some other two to join our begin there there is a no joining like page and later on you can also try out to do some challenge um to these uh Pro uh to these like sort of your Labs set up by us or by some community members I mean well uh the Ross dashboard will also support this persistent flash layer and uh and as uh Android and I just mentioned right the average from orbit is also integrated into our last platform very soon I mean well there's another very big partnership announcement coming soon and just stay tuned uh yeah thank you for your time and I hope you really enjoy this new concept of bikini and the meanwhile I hope you really enjoy your future sessions in today's conference thank you foreign hi everyone uh nice to be here on all layers throughout this is earliest day uh yeah so um hey everyone my name is Arnold and I'm a full-time research analyst at the Block uh so today I'll be giving a very short presentation about the overview of developed landscape so a quick introduction of myself right um I'm a full-time research analyst at the Block and I focus mainly on researching about blockchain scaling Solutions I have a background in chemical engineering at the Technological University in Singapore and for those of you who are interested you can reach me via email you know if you have any questions or you would just like to Simply connect so uh on today's agenda we are going to start with a recap on roll ups which I think is something that you know most of you guys in the audience will already know uh but because of the Divergence in the roll-up landscape I think it is a great time to just go back to the definitions and how we classify them and then I'll move on to the current state of roll ups where I cover a handful of relevant metrics before ending off with the future of roll ups you know where I give my opinions on how I think Roll-Ups might develop in the future so without further Ado we can begin with recap on roll ups so I said earlier that we should Define what a roll up is because I think the idea of a ruler is starting to you know be used as a common password in the scaling domain but uh but you know a roll up at its core right should at the very least be able to abstract computation from the base layer while simultaneously deriving its security from that very same layer and as we begin to see variations of Roll-Ups ranging from data availability modes to enshrine Roll-Ups and even you know Roll-Ups as a service software starting to appear I think it is important to remember the necessary features that rollouts are supposed to have there are two main types of Roll-Ups right optimistic and zero knowledge roll ups which uses frog proofs and validity proofs respectively uh they are to submit to be submitted on the base layer for them to be counted as a Roll-Up right so with that you know aside this table here shows a quick overview of the existing rollups in the space today and how we can classify them so you know the top half we have what we Define as a traditional rollout right which are solutions that use on-chain data availability and then we also start to see more varieties more Alternatives such as validiums and plasma chains that you know can uh they are commonly referred together as layer tools together with rollups but validiums and plasma chains they actually use off-check data availability so you know with the whole concept of data availability abstraction coming in it has kind of blurred the lines for layer 2 Solutions but I think this table pretty much sums it up you know gives the clarity that we need on how we can Define and differentiate the different scaling Solutions today so moving on to the main part of the presentation we're gonna be looking at the current state of roll ups we start off first by looking at the total value lot or tvl overlaps today and I think one of the encouraging signs of growth in the role of space is the fact that tvl has actually increased over 114 percent amidst the bear markets over the past six months where it actually grew from 4.1 billion in tvl to up to 8.8 billion however you know we should know that part of this search was actually due to the arbitrary air drop as can be seen by the sharp uptick in March right after the arbitrum or after arbitrum airdrop is token right another interesting fact about TV the TV out here is that 86 of all TV house across roll ups I actually found more optimistic Roll-Ups and this can actually be attributed you know namely to the ease of uh deployment as well as convenience of the user experience on optimistic rollups and that can onboard and retain users a lot more effectively than ZK rollups uh additionally we also know that you know the four domestic rollups as seen in the previous slide they actually have launched their own native tokens to incentivize liquidity in their own respective ecosystem so naturally this helps them you know capture a larger market share of the entire roll-up domain now if we were to look at the daily number of transactions on Roll-Ups because you know it provides a proxy for the adoption uh we notice it has actually grown by over 153 percent over the past six months uh comparatively to the number of transactions on ethereum which has only grown by 44 uh this means to say you know that Roll-Ups are actually starting to account for more on-chain activity than on ethereum and one of the really encouraging signs here is the fact that optimistic rollups are now processing more uh daily transactions combined and ethereum at least over the past three months ever since the arbitrum airdrop and although this figure has you know fallen off since the air drop height I would say that generally Roll-Ups are still showing signs of gaining significant Traction in terms of adoption because we can see that uh although the the hype has passed the overall transaction growth is still generally on a slight uptrend especially for uh both optimistic rollups and ZK roll ups so the next metric here that is uh highly relevant is the amount of gas right that Roll-Ups consume because essentially the gas that they consume is an indicator of how much block space that they are actually taking up on ethereum right so as adoption on Roll-Ups increase we should also see a corresponding increase in the gas consumption on ethereum and I think you know from the charts here we can see that you know both optimistic rollups and ZK rollups they consume comparable magnitudes of gas and that is a pretty peculiar namely because uh ZK Roblox uh you know touted to scale ethereum a lot more efficiently than optimistic roll ups right so you know where is the advantage here uh it should be noted right that this advantage or the ZK Roll-Ups have over optimistic overlapse is only really significant at high transaction volumes which is not really present today right because uh ZK relapse only start to show that kind of scaling uh when uh there are more transactions being compressed into a single proof but at low transaction volumes the main bulk of the gas consumption actually comes from uh you know the transaction data right so uh in March we can see that there's actually a huge spike in the gas consumed by ZK Rolex uh this is partly due to the Civil activity on Roll-Ups such as ZK Roll-Ups such such as statnet and suitcasing era which have yet to launch a token and a lot of these civil actors are you know go into these roll ups because uh they are transacting on them in expectation of a potential uh air drop in the future right uh and from the chat we can see that Valley games on the other hand consume far less gas than both types of Roll-Ups so Vanadium is a type of scaling solution that uses off-chain data availability and this also demonstrates right the impact of Hosting transaction data on chain on a Roll-Ups gas consumption and that I think is why you know they're starting we're starting to see a market for alternative Data Solutions and the next metric here that I think you know would interest some of the uh viewers is the revenue of uh optimistic rollups the reason why we're looking at revenue of Optimus develops here is because uh the revenue for them is public data so it's much easier to get an accurate count of this data and here we can see you know optimistic rollouts are generating millions in Revenue every month right uh just so you know the this revenue is calculated by taking the amount of gas expenditure that is uh spent on the layer two right uh we see arbitrum and optimism generating over 4.6 and 2.7 million in average monthly revenue and these figures are continuing to Trend upwards even after the arbitrary air drop in March even after the recent optimism token unlock and even amidst you know the overall bearish crypto Market sentiment right so this actually indicates the relative like Independence for the demand of block space with respect to the general crypto markets uh something that is interesting here would be the fact that you know both Matthews and Boba they appear to have fallen off and diverging in terms of their growth Trends uh and this shows how you know optimistic Roll-Ups even though uh you know they offer similar user experience similar services but they are not all the same right and there are various factors that can account for this such as you know first movie advantage trade-offs and security as well as the depth of liquidity in the ecosystem which can significantly impact the adoption right basically launching a roll-up doesn't guarantee that you will find adoption right uh it's suddenly seem like almost like uh it's like a winner takes all kind of situation but obviously uh this can change over time and this light is just accounting for the guest that is picked by uh Roll-Ups on ethereum right for posting batches of transaction and transaction data uh and we can see that you know most Roll-Ups are still profitable in including Boba and methods right uh we see optimism and arbitrary generating 2.7 and 4.5 million in average monthly profits right and uh mobile and methods are also relatively profitable uh obviously you know this analysis isn't perfect because it neglects the cost of running a sequencer node which uh you know may or may not be trivial uh but this profits are also calculated in US Dollars whereas you know the actual currency being used is ether which means to say that uh you know these profits uh can fluctuate depending on when they choose to sell The Ether for US dollars right and then we're gonna move to the third and final segment of the presentation where I give my opinion on uh where we might see the future of Roll-Ups gravitate towards and in the first notable development here would be data availability abstraction right it's a pretty hot topic that has been talked about uh we see Solutions such as Celestia Avail previously known as a polygon available right uh as well as Arbitron any trust starting to surface and see certain levels of adoption right these Solutions offer an alternative environment for transaction data to be posted right both Celestia and Avail there are data availability layers independent data availability layers and they use data availability sampling to guarantee that blocks data is you know posted on is made available right uh with just a fraction of the same data being posted on chain so effectively what they do is that they reduce the amount of data that has to be posted on chain and therefore it should reduce you know the amount of cost that I roll up in curse for posting data on ethereum and now we have arbitrary many trust which basically uses a fallback mechanism you know where it stores data off-chain that is secured by uh data availability committee and if the committee fails the any trust chain will just resort to uh posting the transaction data on ethereum like a normal optimistic Roll-Ups would right so these are some of the solutions that we're starting to see to address you know the high cost of posting transaction data on ethereum and then the next uh unique development I think you know would be would would be the evolution of Roll-Ups as a service or res for shop right uh we have up there the host of uh today's event as well as Caldera and slash uh being you know their own Ras in their own terms uh they each of them have their own unique focus and their unique features uh so firstly we talk about oddly which I think one of the unique features that they have is The Flash layer feature right where a developer can easily spin up a roll up to perform one-off activities such as an nft mint before settling back on ethereum uh or another layer that you know called layer might support uh this is significant because it can reduce the stress of ad-hoc events right because we've seen how gas spikes when there's a popular nft mint on ethereum right and that can affect the experience for all other ethereum users not just the nft mentors so having a flash layer that takes away this ad hoc demand can certainly reduce the stress on the base layer then we have Caldera caldera's focus is mainly to you know spin up uh Roll-Ups with alternative VMS virtual machines before settling them on evm chains so this should improve the accessibility uh for developers to build on evm chains because they can spin up their own preferred VMS as a roll-up uh and then settle the outcome of the error on an underlying evm chain so this once again should you know bring about scalability for not only ethereum but other evm chains as well uh we also have slash SDK which actually allows uh developers to spin up layer trees on stop net and I should note that while this isn't exactly you know like a roll up uh the idea behind you know spinning up and additionally uh easily or conveniently does make it analogous to uh Ras software and I think the main point of all of this rollouts as a service is to abstract the complexity of launching a roll-up and they make it more accessible for both developers as well as users to assess you know roll up block space that is uh much cheaper uh than the blog space on ethereum so I think here is a certainty that like Roll-Ups as a service have a huge potential to scale ethereum in the future and the last notable development here would be uh Roll-Ups building on other blockchains right uh we have output next which is uh trying to build like ZK relapse on bitcoin uh we have uh Dimension which is uh roll up as a service platform as well that is building uh Roll-Ups on Cosmos and then we have Bezos with their own and try and roll ups right so uh we start off with the Bitcoin ZK Road Labs right uh one of the challenge that they face right now is uh you know the fact that Bitcoin lacks a lot of expressibility in terms of their programming language script uh also album is currently building in stealth so there isn't really much information on uh their progress but uh it is something to keep an eye out for that you know Bitcoin May one day see Roll-Ups of his own uh yeah then we have uh dimension on Cosmos which is as I said like uh Ras solution um but it's main focus unlike the previous Ras that were covered is more on interoperability so it's meant to make like a roll up speak to each other more uh easily to facilitate that uh cross chain across roll up interactions right net Hazels have their own like and Shrine Roll-Ups which uh basically means that the rollouts are enshrined within tables itself so they don't actually need to have like uh additional like layer to scale it and this allows you know the entrance roll up to skill the exhaust without needing to sacrifice a lot of security or yeah basically is inheriting all of the exhaust security and while these Solutions don't necessarily you know scale ethereum I think uh they are highly relevant to understand and keep an eye out for because they can provide a clue of valuable insights as to how the roll-up landscape can develop on ethereum so that is something that is uh worth you know keeping an eye out for right so just on to the closing remarks uh I'm not too sure if there's been too much content but like uh there are three main takeaways here right I think uh if you want to leave this presentation the three main takeaways firstly Roll-Ups they are becoming an integral and inescapable aspect to scale ethereum right we see more transactions on Roll-Ups combined than on ethereum over the past five months and this trend is you know could continue especially if the bull market returns and ethereum becomes highly costly to use number two Roll-Ups can be very profitable right we see optimistic rollups alone generating over 8.5 in aggregate monthly revenue and this is just optimistic roll ups much less to say if we talk about ZK Roll-Ups or even validiums which have very low cost uh I think scaling ethereum is not only in the interest of the ethereum community but like those who scale ethereum can stand to gain as well and the third and final point is that you know the landscape for you know improving and enhancing Roll-Ups is very vibrant and it's diverging right we're seeing a Maria of solutions starting to appear ranging from data availability uh Roll-Ups as a service and Roll-Ups on other blockchains uh becoming increasingly common and this will all add to you know the diversity of solutions that we all see uh that will scale blockchains in the future so with that I think you know the overview of the rollout landscape in short is just full of activity and this is amidst you know the Dow bear market so I think that when the bull market comes back uh it will be very interesting to see how all of these Roll-Ups continue to develop how the technology will continue to grow and you know more importantly for those people growing the technology uh what it means for their transaction costs as well as because running the roll ups what it means for them to be a profitable business so I do think that this is a very uh robust ecosystem that is uh ready for the bull market uh and with that I thank you for your time and uh if you have any questions you can reach me via my email or on Twitter and I hope that my presentation was insightful available to all of you out there thank you hello everyone I'm Julia Ma I'm a research at the robustinc center group within the ethereum foundation we usually look at mechanism design for the ethereum protocol so think of problems like erp1559 and proposal bill of separation and today I'd like to talk about the efferential in a trust main world first of all I would also like to thank outlayer for hosting this session and I hope it will be very informative so if I'm thinking about ethereum's role in the Christmas world the first thing that pops up to me is how proposal Builder separation is really the design philosophy and not necessarily what we think of it in his current form so many of you will probably know his current form of Mev boost um which is the out of political software The Flash bolts developed and is clearly used by large sets of the validator sets and it allows proposers to connect to builders who built the entire blocks and then proposers read the benefits of this so it allows everyone to get the same benefits of sophistication however proposal buildership creation is actually way broader than this in general it allows any Outsourcing of protocol juices that proposers may have to third parties in order to fulfill these duties so in essence uh PBS or proposal separation means that a proposer who's tasked to perform certain doses by ethereum possible may have a desire to Outsource some of these these um duties for whatever reason it may be so in the case of um blood construction this might be because construction a good block is very intensive work and it requires a high degree sophistication so what the proposal could do is they could draft the Baton tracks that outsources this job to a builder and there where we see a builder as an out of Proto agent that's sophisticated and rational and has the ability um to to fulfill this job well and then PBS really refers to the formation of this contract so in math boost it may be that the contract has to form that the Builder makes the entire block but as we'll see throughout this presentation and there we've seen in the literature there could actually be different forms of this contract so as a quick recap to get everyone on the same floor so where are we now Nev boost is a current out of prototual instantiation of PBS and so it's not actually proposed to build a separation uh proposed to build a separation or enshrine PBS is something that we're working towards and map boost allows proposed Outsource blocked construction to builders as we've just seen through these contracts and thereby gives all of the proposers the same access to sophistication so this ensures that there's still decentralization of the validator set because there's no incentive to co-locate with Searchers or with Builders as we would have probably seen otherwise uh however it does come with a few challenges for example there are lots of trusted components now in Mev boost and meaning that there could be some liveness issues um or unbunding attacks like we've seen um and in the process of Mev boost they Define a very rigid contract which is the blood Orchard so as I just stated the Mev boost says that the a builder can make an entire block for a proposer so they don't give you the option for example um to build half a block to build blocks in different ways they they Define a very solid fusing for what the form of this contract must be and then a logical question to ask is if this is the optimal form of the contract this is something that will explore further in this tool and the idea about whether this is the perfect form for the formats or not terms from Barnaby he's the team lead at the robustion centers group and I've linked his post in the bottom left corner of the slides here I also share the slides afterwards and these are actually clickable so if you're interested in any of the links here you can click on the slides and go through the links yourself so the idea that there may be a different format of the of the contract that might be more ideal for certain situations uh is referred to as the allocation mechanism of of PBS um so with the allocation mechanism means is that the the pro skill facilitates certain proposal commitments so for example in the case of um nav boost MF boost facilitates that proposers commit that a builder can build the entire block if we were to enshrine PBS uh the ethereum protest people would facilitate this commitment um but there could be other commitments and then there's the market structure which means that the proposed Proto provides as safety guarantees around this commitment so for example uh that the proposer um will only use commitment and also that the payments from for example the Builder are enforced um some some other forms that could be interesting for PBS are actually related to First main Mev so what form of The Proposal Builder separation contracts would be the best for across the main world this is what we'll be looking at as a political designer so first of all we need to consider whether this is even the protocol's concern putting stuff into the pros field is quite a heavy duty task it will take a lot of time and once it's in it's very difficult to change so the plus doors are very rigid mechanism and it's not it's not easy to to change and also it doesn't have a view of a lot of things for example currently it doesn't see what the block production process is like this is also the reason why um move boost exists and because uh does the ethereum protocol currently doesn't see how blocks are constructed it's possible that proposers Outsource this task tool um into Builders and the protocol isn't aware of many other things for example protocol isn't aware of what the state is of on other domains or whether there's even coordination between these kind of domains and the question is whether the protocols should be aware of all of these things or not an advantage of being aware means that is less likely to be a principal agent problem for example in the case of where the the principle in this case the the Proto and there's another line with the incentives of the agents so let's say that the agents are not aligned with decentralization or other factors like this um but an advantage of the approach to not being aware is that there could be out of protocol software spanned up that would allow for coordination mechanism so we see a lot of experimentation in this field and this is probably where other torch will be centered around today um another question that we're concerned with in within the EF is whether these kind of solutions should be enshrined or not and also what the options are so this is something that we'll go through so why would um trust domain coordination mechanisms be a concerned for the ethereum port at all I'd like to Define three things that I see as important first of which is allocative efficiency and with this I mean that is block space used to reach the social welfare maximizing States for example we could see that if we're not aware of any coordination or of any coordination between domains it could be that we use States in a way that doesn't optimize um for for what could have happened if we had the rise coordination mechanisms which brings me to 0.2 are there actually applications that you benefit from close to main coordination mechanisms that we currently do not have and my third point is informational efficiency informational efficiency refers to whether all available information is incorporated into the protocols this is a somewhat um softer point so for example you can see um in Arbitrage between a centralized exchange like binance and addition slides has changed like uniswap um we could ask the question whether all of the information about price movements uh or or about um tokens in general is incorporated into the into the decentralized exchange um or not and this would probably mean some more um effective use for users because they would be trading at the price where all information is incorporated and all that some more outdated price oh yeah and then a side question for this is can we achieve the same level of allocative efficiently of coordination and informational efficiency if the solutions are out of protocol and if they're in portable we'll start with a soft preconformations which is an out of protocol solution and you can see again that this is a this is a hyperlink so you can click on it and it will redirect you to a slideshow by Alex stoics who has falls about this results so soft pre-conformations are example of Builder Services which means that a builder provides adjusted servers to users and to guarantee certain things so in the case of pre-conformations they could either guarantee that your transaction will be included uh maybe in a certain range of blocks maybe in the next blood not they win um or whatever inclusion guarantee that they are willing to sell and it usually bring to buy of course um and moreover a stronger form of soft pre-commitments could be that they would give you a confirmation on the post dates of your transaction so they could give you for example exactly how much balance you have of if and of some other tokens after this transaction and these could be very helpful things because it would allow you to act on this information act on the fact that you know something is going to happen on it um the only problem though is that these sort of pre-conformations are based on the effect of given that the Builder wins the PBS option they will only their commitments so it could very well be the case that the Builder doesn't win the PBS option um and to prevent this uh usually would actually want um a guarantee would need to go through multiple Builders um and buy these sort of pre-confirmations uh to get guarantees and this may be a coordinate this might need a coordination mechanism in itself and could be very centralizing actually um so we need um so we might need something that goes beyond this um the advantage of of having such a pre-confirmation though is that it allows users to act with the guarantee on other domains and we'll see this power with an example later on and then something that I'm very excited about is schlotz auctions uh which is an enshrined version which could also um yield some pre-confirmations basically and again this is a clickable link so we've seen that the Mev boost mechanism determines your contract for form where you can sell the entire block and people bid um based on their blood so this is what we see on the left hand side we see a blocked option and Builders they bid and their bid is for this specific block so the the Builder has built the block already and if they win this plot will be included a slow solution is is a slightly different from this in the sense that Builders bid um but they don't they don't necessarily build the Block in advance they will probably make some estimates of the value and base a bit based on this but it could be the case that they would actually um only build their block after they know they won um so this is a different form of the the tone track that we've seen which is the essence of PBS and this is also why PBS is really a design philosophy uh fall into the the allocation mechanism of PBS and so they're quite different from um blog options again for more depths I'd recommend you to click on the the link here and some of the consequences of slow torch inside that the difference between the commitment time and the execution time which is the commitment time refers to the time ads uh at which you made the your blog so for example in the blood Georgians part we saw that the block was already made at the time of the bid and the execution time refers to the time when it when the block is published um so now because the the auction has to happen between the time of commitment and the time of execution in the blood auction there is a large gap between this and if the auction happens before this and your commitment time and education time are then very very close and the obvious benefits of this is latency reductions so we see now that in Arbitrage between centralized exchanges and decentralized exchanges latency becomes very important that's and we've observed this in the high frequency trading in traditional Finance as well obviously um so this is one of the obvious advantages of slot auctions but another maybe more accessible Advantage is that blood content can now be made conditional and guarantee of inclusion so in terms of the this doesn't seem to make very much sense so um in principle you always make uh your blood contents um conditional on inclusion because if you're not included the blood contents are irrelevant and so they yeah they wouldn't be executed anyway so you would make the international on inclusion and this is something that um some people have uh dived into and there was a paper written on Mev invariants Satan at the cape doesn't grow so the Mev Pi doesn't grow um in if ethereum would Implement um a conditional blood building basically um relative to centralized changes and the the caveats here is that centralization changes would also have guaranteed inclusion and they you could basically always commit to them so they're in instead of there being a blockchain whether it's a street commitment times and there would be continuous treatment times so an open question still is what's the what would happen to the overall cake um of Mev if um you're dealing with two blockchains that have both have discrete times and also both have probabilistic inclusion so this is still an open question so if anyone is working on this I would love to hear about that um and then a separate question is whether we should ensure noodles So currently there's been a debate about whether we should enshrine PBS or not and there's there is an article written by Mike Noida with some arguments in favor and some arguments against so Martin's in favor that's out of protocol software like math books could be quite bristle and there could be some dangers um factors in this for the for the protocol so for example University equivocation attacks that we've seen from the low carb Crusader and potential liveness issues with relays um if if their software isn't implemented correctly and however a very important factor here that he also points out is that if if we implement the Rhone Market net or Market management and allocation mechanism we could be sort of stuck with this because it would be very difficult to change so if we Implement their own contract form of the PBS design philosophy into ethereum so we we move from a philosophy into one specific field and for example we use the contract form.m Boost imposed it could be that this isn't optimal for for our future and so it could be that if we use the contract formula and they've both imposed uh there's less coordination possible for um across the main commitments um because we don't for example have slow Solutions or other forms of PBS that would allow for more coordination and so points this hours I'd like to use one example um so here I've taken a trust domain Arbitrage from the paper Unity strengths and formalization of trust domain maximum retractable value um and we see that um we need to do both on ethereum and on polygon we need to do a Swap and this will netters some profit so let's take this to a more an easier setting let's say that the total profits from this swap um in this case it says something like 111 000 US Dollars let's say for a very simple case it's just a hundred and we need to pay fees on both um chains so in this case we're first working on ethereum and then we're working polygon let's say that we need to pay um 30 US dollars so that's 30 of our profits um to to ethereum to get our transaction included we need to do this first and then we can only move to polygon where we will see a fair transaction get it can get included there so now we paid 30 and then we moved to Polo Dome and there we don't want to pay more than 70. however the problem is that a rational arbitrizure would actually be willing to pay up to 100 because um because the the 30 before our sun cost so the best that you could do um if someone is if the polygon um validators are not willing to accept 70 is to bid up to 100 and this could actually be a difficult problem because in this case you're actually running a loss if your profit is only a hundred so um the fact that you could potentially be running a loss even though and there should be some Arbitrage opportunity means that it could potentially not be exploited what if someone is actually setting up these kinds of traps uh in order to get people to pay more um and and similar problems you could imagine that people just don't want to uh deal with this kind of stuff um so this is really where you see the power of pre-conformations or slot tortions what it could do um is for example if you have um the powers to determine what's going to happen in the next day transition you could um you you have the like the the pre-conformation basically and you could act on polygon first um sitting there and only if this part goes well you can um do your transaction on ethereum meaning that so you don't have this this problem anymore where potentially someone could hold out from setting an Arbitrage opportunity to you on the second domain because they know that you're willing to bid more and they know that you're willing to run a loss so this would prevent um loss and it would allow for some sorts of trust domain atomicity and because of this we see that there's a um again in informational efficiency if this arbitrust opportunity does become possible because suddenly there's more information incorporated into prices on both domains we see that there's an increase in allocative efficiency um because um the Mev that's represents some sort of contention is now actually extracted and blood space is used to to drive up discontention and gets us to a more socially welfare maximizing State and this is possible because of the trust domain coordination mechanism so these are the three point things that are that I pointed out before so um yeah please keep that in mind if you think about close to main coordination so should we wait and research this more well there are some reasons to wait and to prevent uh to immediately go ahead and prevent some of these these attacks but implementing the wrong thing means that we could be stuck with the the incorrect contract formats and Pepsi which is a post from Barnaby again proposer and a post-going force proposal commitments really dives into this allocation mechanism and Market my and and Market design so I'd recommend everyone to read that and then finally some interesting ideas that are there is the the nuclear button idea that I've heard Phil Diane talked about which decreases the investment efficiency of certain potential extraction techniques basically what it does is it says that since we could always Implement some mechanism like PBS um it would be unwise for people to invest in so it's in very extracting um techniques because um if so it's in very extractive techniques become very profitable um we would use this nuclear button and insert uh PBS so this is a these are some of the ideas that I'd like to leave you off with and there's a conclusion we need to be very careful about considering whether ethereum should be aware of what's the main coronation and this remains an open question um the out of protocol Solutions may affect some things um like the whole out problem that I showed you before which is also sold very in product all solutions but they require some trusted role and maybe some extra coordination mechanisms but in general allowing for cross Main Attraction might increase allocative efficiency of blood space informational efficiency of prices and expand the possible mechanisms that we can use for our applications so the open research question that we'll be working on is what the value of source options are in the case of overlapping commitment times of blockchains and no no overlapping commitment times of blockchains um House of prequel informations could actually work and whether if you're interested in trying these trust me in coordination mechanisms or not so thank you very much for listening um if you have any questions please feel free to reach out to me and again thank you for outlayer for hosting this session I do thank you bye hey there good morning everyone thank you all for being here and thank you guys for hosting me out there uh my name is Rahul I'm the co-founder of connects for those of you who have not experienced connects yet connects is a protocol that has been around for a really long time so the company actually started in 2017 when the crypto Market looked very different so we started out doing very different things uh always the focus was around building good secure scalable infrastructure with the end goal of onboarding more users onto web3 so really that has changed over the years to initially focus on scaling and now focus on interoperability an interesting thing is scaling and interoperability are really like two in the same thing because you know when you have scaling your offloading transactions to something else whether it's a secondary layer one or a layer two or a layer three or whatever but unless you have that interoperability the scaling actually doesn't do anything on its own so anyway the title of my talk is the HTTP of web3 and this kind of defines the future for connects as well as the interoperability space as we see it um so just to kind of take a step back here let's time travel a little bit back to the 1980s and see what the early stages of the internet looked like so back in the old days of the internet you had to do a lot of manual stuff and really like interact directly with infrastructure so you had to like know basically the subnet of some Network that you wanted to connect to use telnet to connect to a server then you had to like FTP some Storage off of it and it really was a very manual process that exposed a lot of these layers to the end user and this is because there was kind of a mishmash of different protocols and uh other kind of uh like a lot of other things that you had to kind of think about and do manually so then we fast forward to today so today it looks very different you have Rich interactions that you can do with very different pieces of infrastructure under the hood so let's say YouTube is an example YouTube uh will send Network requests to this very well-defined uh very robust networking stack and it will be able to query content from cdns and deliver that to a user it will be able to go to its storage locations uh and get data it will be able to query other servers eight through apis so the whole stack has been very much like abstracted well-defined and uh it's very expressive and the reason for this is because of HTTP right HTTP is basically what made the web composable to some extent so bringing us to what the web 3 is like in the 2020s I don't mean to kind of pick on Ave here but if you go to something like Ave or basically any of the web 3 daps that are pretty big right now uh have this this property where you have an app that is multi-chain deployed on many different chains Roll-Ups whatever and you have to manually interact you have to pick what kind of network you want to connect to then it's going to go to your wallet and it's going to tell your wallet you need to change to this network and you you have to kind of manage all that stuff so really this feels like you know dial up in the internet where we had to like manually turn on our internet and you know connect to some website by sshing into some server or something like that or using telnet or something so it's it's while it's very cool and there's very cool things we can do and we all love web3 this is this is definitely not the future this is not how things are going to look in the future and not to mention this case like all the liquidity is kind of fragmented you need to think about bridging funds from one chain to another chain if you want to use a product on that chain uh so then we fast forward to how things look today or sorry not how things look today but how things will look in the future um as all of us are here because of this roll up as a service day and all layers kind of uh pushing this this Vision uh I think we can all get on board with the fact that the main interaction that users will have in the future is with app specific layer threes so I I strongly I'm a believer that a lot of things will move to different siled layer threes where they can kind of control more mechanisms around things so you'll have like a swap layer three you'll have a landing layer three you know Uber and Airbnb or eventually are going to become peer-to-peer applications that live on their own layer threes and then you'll have storage and decentralized commute that compute that are also running is like decentralized services and our vision for the future is that all of these will be connected by an interoperability layer which will make these interactions extremely uh abstracted from the users themselves so then how do we get to this Vision really we need to Define this kind of interoperability layer as a modular layer that is fully specified and standardized so with this then we can really start to build the infrastructure that makes this type of stuff possible so let's go back to the internet and say how the networking stack looks in this modular Vision so the networking stack is already modular so you have a transport layer which is TCP you have a security layer which is TLS and then you have this application layer and this is important because the application layer is really what allows everything to be composable it allows developers to build new apps it allows content to kind of be linked to each other over the internet and it allows users to not have to deal with any of this they can just go into their browser and start browsing so then how do we translate this Vision back into interoperability and web3 in the blockchain so what we can do is we can kind of break the same uh the pieces down into like very similar looking pieces so what you have is you have a transport layer you have a verification layer and you have an execution mechanism built on top and let's dive in a little bit and explore what each of these layers do look like uh so the transport layer is the very base layer of this communication between different blockchains how do you bring a block header it can be any payload but let's just say it's block headers because as we'll go into later this is the most efficient way to transmit data from chain a to chain B you know this may seem easy in like an all ebm world you just have some kind of relayer that's just like reading data from one chain and putting it on another chain however this will get more and more complicated one of the things I really believe is that a lot of these app specific layer 3s will end up moving away from ebm into you know more highly customized different versions of VMS that are highly tuned and efficient for their own use cases so you know then you need these kind of adapters to basically bring this evm based process that we have right now to be able to be translated into any type of uh transportation so what I'm saying is we already have an extremely well specified and battle tested transport layer protocol it's called IBC so those of you who know IBC it's basically a cosmos based system for uh transporting messages and doing this whole process of Transport between different blockchains uh if you've looked into it it's extremely well specified it has multiple clients running it it's it's basically like extremely well documented and it's battle tested because it's being used across many different blockchains for a very very long time and that's what you need with these type of infrastructure projects so that's for the transport layer so then we move on to the verification layer so a verification layer works like how do you prove that something like whether it's a block header or something else is valid from chain a to chain B um so this can happen in a various number of ways uh the bottom line here is that verification mechanisms are going to be commoditized at be an Open Marketplace so this is going to be like uh basically an Open Marketplace where different actors can compete on speed cost and chain support and since basically if you if you boil us down to the fact that you're just sending block headers from one chain to another chain this is like a fixed piece of data so the cost is not going to scale with the amount of data that you're sending so it's really going to be kind of like a race to the bottom and an Open Marketplace and the the other thing to note here is that there's a lot of teams working on this right now there's a lot of really smart people who are trying to solve this problem of securely verifying data from one chain another chain using ZK proofs using light clients using message aggregation Etc and these people are very focused on the singular problem so they don't care about transport they don't care about execution all they care about is verifying that a piece of data from one chain is properly has Pro is like is valid and uh usable in other layers of the stack so then that brings us to execution so this is a very important piece of the puzzle I I do apologize because I had emojis and things that don't soon be showing up on this presentation some of these diagrams look a little weird um but you know I can I can point people to the slides later uh so anyway execution so after a block header we go through the transport process we go through the verification process now on chain B we have a block header that has been verified then how do you build Rich interactions on top of that so this is where we really get to the HTTP analogy because really like doing something with this block header and root is the hard part once you have this then you have a ton of infrastructure you need to build to actually like build apps build cross-chain interactions so truth is application developers don't want to deal with this lower level stuff they don't want to submit proofs they don't want to read data things like that the the data cons yeah sorry so then like this is also where the costs in the flow are because when you're consuming this data cross chain you're actually like consuming a lot of data potentially because you're going to be sending like a big Merkel proof that needs to prove some kind of data and then building complex interactions on top so the bottom two layers do not really have much value where you know users are going to be paying a lot of fees or something like I said so that's why these are going to be really like highly commoditized and really like kind of Race To The Bottom let the application layer the execution layer this is where you kind of have a little more room for the flow and really execution is what brings us cross chain composability like without it you can only consume data across chains but really when you have this you can actually create applications build Rich interactions so like to bring it back to the web analogy the web is a graph of hyperlinks between content and it's independent of the links under the hood and that's really where we need to get to with this with this uh blockchain as well so what we say is this exclusion layer is global State composability on The Interchange and what connects is trying to do here is we're trying to focus in and build a really well specified generalized extensible execution layer framework so then that breaks it up here a little bit so now at the transport layer you have teams that are specifically working on this exact problem like Cosmos is doing all the work around building the the base layer for uh the the specifications around IBC polymer is a team that we're in touch with that is building IBC everywhere basically so we want to let them focus on that problem and you know eventually like build a client client that will plug into that similarly on the execution layer uh you have these really great teams some which are mentioned here which are like I said laser focused on solving this one problem they don't care about anything else like we talk to a lot of these teams they're not trying to build like full Bridge functionality they just want to build this verification mechanisms that allow you to move things from one chain to another chain and prove them very specifically on on the destination side so at the execution layer this is what we are focused on it connects we want to focus on building the ability for developers to interact with verified cross-chain uh mechanisms and you know if you look if you look at this from the big picture you'll see that a lot of bridges are trying to build this entire stack so obviously if you try to focus on too many things at once you're going to end up like compromising on things and not really being able to develop deliver the entire solution to users so that's why we're really trying to laser focus on this one part of the stack which is the execution layer so then when you have an execution layer what are the things that you can do with it so this is the most basic uh use case of an execution layer or of the execution layer is to do a write and I'll call this a slow right right now and you'll see why later so what you can do is a user can make an X call which is a call that is meant to go cross chain from chain a to chain B and this assumes that the block header has already been sent to chain B so after some time of making this x call this block header gets packaged up it gets sent to chain B through the various transport verification mechanisms and then at some point in the future you have a root a block header that has been proven on chain B so then your execution layer mechanism will do all the computation to send a proof that the user's transaction is actually within that block header and then it will execute any type of data or or call or you know funds transfer or anything so the use cases for this are around things like secure cross-chain messaging uh managing governance things like that things where you really need a very secure mechanism for sending messages across chain that may or may not include value so the next part of this is uh a fast execution mechanism so the reason I call this fast is because you have this verification process and this transport process which can be actually very slow because you know when you're dealing with this type of system there are going to be a lot of things that these Protocols are going to do to reduce their own costs because you know at the end of the day these are kind of like race to the bottom protocols that won't have much margin they might be even public goods so they're going to try to like reduce the cost as much as possible uh for that reason so what that involves is a lot of batching then you have these things like ZK proofs which might be really heavy and take a lot of time to run so at the end of the day you're gonna have to this process from sending a block header from chain a to chain B is definitely not going to be an instant process but something you can do is you can have something called a fast execution layer so what this is doing is this is listening for this event on chain a and because it sees the event on chain a it has a certain guarantee that's this block header will eventually be received on chain B so it knows at some point it can prove this so then what it can do is it can actually go up front and send a transaction on behalf of a user so let's use like an any to any swap for example so what that means is a user is going to maybe swap on chain a then then send the funds to chain B and then do another swap on chain B so what this fast execution layer can do is all is it can do the swap for the user and send the user directly the output of that Swap and then you know we'll have something in this execution mechanism to understand that this is what happened and then have this uh fast execution layer be able to claim the funds back after this block header has been proven so this is kind of like the use case where you get with many like cross chain bridging right now where you want your funds immediately you don't want to wait for some uh execution mechanism to happen so this is what enables that this is this is what enables an instant like really nice clean user experience so cross chain swaps like any any swaps are a really good example of this cross chain zaps you know fun movement and then the other interesting thing is you can also do things like verify signature so you know if you have a signature that can be verified from chain a then you don't need to wait for the block header because you can already see that that's going to happen and verify that on chain B without needing to wait for a block header so then the other really interesting mechanism is a read so say you have these block headers that are coming you know they'll be coming on like various intervals uh then you want to prove some data against that on chain a itself so really all you need to do this shouldn't be an X call this should be an X read so an X read is where you're just gonna still submit the same proof for a block header that you already have and prove that something has happened on chain B so say you have an oracle that's like updating price data every 30 minutes or whatever or every minute or however much they send their thing then you have all that information in a block header that has been sent so once that block header is received here then you know you can read it immediately and you can basically read data from any chain anything that has ever happened on that chain uh so other other ways this would be useful read gas price data read token voting power as well that's a cool one right because you can basically read what your uh ve token balance or whatever is at any kind of snapshot in time after you've received the root on the chain that you have your Dow on for example so yeah I've refer to this a few times but X call is basically how connects is envisioning this uh whole execution layer process starting so this is basically a primitive to facilitate asset and or data transfer across arbitrary chains so it's designed to mimic solidity's built-in call methods so what we want to do is we want to make the process for developers around making these really expressive cross-chain interactions extremely easy to build and since it mimics cilities built-in call method they don't really have to think about much or know much beyond what they already know in solidity and here is basically a a snapshot from the code about exactly how you would do an x-call so it's it's really simple as you can see the parameters are very straightforward you have uh some kind of fee that you pay to the relayers for sending this amount you have like a destination domain and Target uh token address uh you have some some safety fallback parameters around the message sender um the amount you're sending you know slippage because uh you know Bridges will have some kind of slippage and things like that and then you have call data so this is basically the bare minimum amount of parameters that you can have to be able to make really nice uh kind of cross chain interactions so you know one thing I'll get at here is that specification and standardization is positive sum I think a lot of people a lot of companies might view this as you know eating into their bottom line or this kind of like goes against what normal companies want to do because Normal companies they want to control everything they want to you know close their apis if you look at like Google Twitter Facebook things like that like they're all charging for API access now because they're built like companies they want to make profit but if you look at what the end user wants the end user wants truly open apis where anybody can compete use the apis like if you look at the example of Reddit this is a great example right now so Reddit had all these different like clients and user interfaces built on top of it because it had an open API but now that it's shutting down its API it's been a huge backlash because you know a lot of these apps are being forced to shut down because they can't consume the apis right now so what I'm saying is that like we want to get to a world that connects where everything is specified everything is standardized this might make our own protocol like you know less profitable I guess you could say because it allows competitors to just come in and build multiple clients but that's not what we care about we care about making it a better place for the actual end users to interact and uh you know be in this world so you know just to reiterate that connects is not a company connects is a public good because we're building web 3 infrastructure it needs to be something that is given to the public it's given deployed and it is usable by anyone so it operates on the standardized playing field where anybody can come and operate and build tooling around it so you know I like to say this a lot but you know at connects we don't want to compare ourselves to Apple Google Etc like that we would rather hold ourselves to the bar that ethereum holds themselves to like ethereum is the ultimate vision of a public good that has been realized and you know if we could all be like ethereum I think we would all be pretty happy so you know just just to wrap up and recap here these are some thoughts that I want to give here so standardization at all layers of the stack encourages competition and a race to the bottom which is better for users right so obviously if all the all the companies are competing in a perfectly free market they are going to always undercut each other until they're making the minimum amount of profit that just allows them to survive and that's how web3 is built web3 is built so that there's no middlemen there's no rent Seekers there's no people trying to like take a cut there shouldn't be any of these Cuts there should be you know zero fees zero like protocol fees should really be a race to the bottom and not even exist in a lot of projects right now I think there is protocol fees but you know that is something that is getting slowly undercut um then you know we need to break things down into modular pieces so modular pieces allow standard standardization specification and they allow like I was saying they allow the best teams to focus on specific parts of the stack and really kind of compete on cost and speed and what matters to them you know they get to focus on like one really small piece of the stack and really kind of build it out the best way they can they can possibly build it then execution application layers are what make protocols composable so this is where we want to focus on it connects this is how we want to be able to interact with developers interact with users and make uh end user interactions possible and good and then to end it all you know web3 infrastructure is a public good I strongly believe that these are not companies that are being built a lot of the other teams you know some of the other teams are definitely acting like companies trying to make profits trying to figure out how they can increase the revenue as much as possible but I strongly believe that web3 infrastructure at the end of the day is a public good and it needs to be compared to ethereum like I said ethereum is the ultimate public good and all of these companies should be striving to be more like that so thank you all that was the end of it and yep I will make this presentation available everywhere okay um hello everyone hope you enjoying the event so far we have heard from some really wonderful people on topics spanning Roll-Ups modular blockchain design space Network design and architecture interoperability and economic considerations of rollups some truly insightful presentations what I want to do now is have a panel on the present and future Roll-Ups there's a lot to explore here from various angles and so we thought we'd invite people of different diverse backgrounds we have a richer discussion on where we are and where we are going so it's my pleasure to welcome three such people from the industry here we have Veda kochari from wavint Patrick McCory from Barberton foundation and stuff return bar from Masari hi to all of you and so happy to have you on this panel so I think oh thank you thank you thank you for joining us and making time for this panel given that and all of you are from different time zones so we are really appreciated it and so let's kick this off uh let's start with a quick intro so Patrick would you like to go first yeah hi everyone my name is Patrick uh I think historically speaking I mean you know my research at my PhD in cryptocurrencies uh started in 2013 on bitcoin and then 2016. I ended up doing stuff in ethereum and then I guess for many years until 2019 I was a researcher and then I left the Academia and the university system to try to do a startup you know as you do uh the startup was acquired by amphira and then eventually after infuri joined the arbitrine foundation but during all those years of my research I mostly focused on Layer Two protocols or more specifically you know how do we secure cryptocurrencies so people can actually afford to use them by still retaining all the nice benefits of a cryptocurrency welcome Patrick you want to go second yeah sure hey everyone I'm maida so I'm currently an investor investor at crypto focused Venture fund um I kind of started off my career in crypto back in 2017 as a software developer um primarily spent most of my time in the cello protocol so I was building out a lot of developer tooling for that ecosystem a lot of like smart contract sdks clis things like that and now at very end I spend a lot of my time looking at different infrastructure protocols so definitely a lot of research into Roll-Ups and things like that I'm excited to talk more about about the panel today thank you Amanda Stephanie hey everyone I'm Stephanie um so I'm also a researcher um been in this space uh last time thank you Patrick I jumped in in about 2019. um I I was a technical writer uh focused on um really any kind of emerging technology at helping companies kind of describe you know their products and also rate Grant applications and things like that um by accidentally started writing about blockchain projects and then learned about you know the public side of blockchain and you know fell in love uh and went from there I started my time in crypto mostly in ethereum but ethereum staking um so not voting for technical documentation for non-custodial staking protocols um and then joined Masari about two years ago as an analyst started on their diligence team um so it was really focused on the nitty-gritty details both on the technical as well as the economic side trying to become more well-rounded um and here I am on the Enterprise research team I mostly focus on infrastructure layer one layer two heart to pick a favorite to be honest I just love talking about you know how all this fits together um and you know just what amazing things we can build with the technology thank you Stephanie um now let's go with the first question and this is a question that I think is very relevant given uh you know some of the debate that was happening over the last um week or two on definition of Roll-Ups and what really defines the roll-up so Patrick you know maybe you can start with so what's your take on the recent debate uh what what defines a Roll-Up yeah well I mean there seems to be two different campuses in there just roll up equal equal database or notice Bridge equal equal database um and actually most people think I have the opposite opinion actually agree with that appeared out there so I always think that's quite funny um I think I think the best way to describe that argument is to step back a bit and think no as an industry why do we actually train the build I know and then how does this relate to rule apps so the entire point of a blockchain system is that a blockchain provides one purpose I gave you a copy of a blockchain and you can compute a copy of a database and that's all it does you know this allows anyone to compute a copy of the CM database so then the question is you know how do we all agree to the one database and to the one blockchain so then anyone in the world can download the one true blockchain and then compute a copy of the CM database as everyone else and there's two ways to do that one is when we could deploy your consensus protocol build your entire ecosystem build your peer-to-peer Network it was something that looks like Bitcoin or ethereum you know get all the participants together get your honest majority get all the stick in there to defend the system um so you could do that that's one approach you know build an entire ecosystem from scratch and that's very very hard to do you know there's no easy feat doesn't hurt your feet the other approach if you want to you know instantiate a new deal to be this is to say well communal has reused that system that already exists you know could we pretend it's like a sort of like a public bulletin board where as long as we can post the data there then anyone in the world can pick up the data they can you know parse it according to some set of rules and then they can compute a copy of the database themselves and that's basically the roll up you know the purpose of a rule up is to create a database or whatever whatever purpose that you want and the way we all agree to that database is that we just post the data somewhere else like ethereum or Bitcoin you know like ordinals and then everyone can say okay well I know the data is there the data is ordered I know the rules I can process the rules I can get the same database as everyone else so at you know at the at the heart of it that's all the Roll Up is it's very it's a database but the data is publicly posted and we've known how to do this for at least 10 years so the early examples were in a color coin a mastercoin I don't know if you guys remember that it was the you know issue assets on top of Bitcoin but the Bitcoin platform could not enforce the rules it was just used as a daily availability layer where we could push the data there and it's the software client could then parse that because the software client was aware what the rules were so we don't have to do that for 10 years and it sort of died off for a long time maybe six seven years but they originally got popular and this sort of gets to the heart of the debate because we now know how to build better Bridges and this is like the second half of the beard so normally when you use a cryptocurrency system you know SSM on ethereum and I want to move my assets to coinbase there's a bridge in the middle you know I lock my phones up I give them the coinbase yes coinbase protects the funds then it shows up on their off-chain database so the whole point of the roll up movement today is to build a validating bridge where I can take my assets on ethereum move it on to an off-chan system something like arbitrine or optimism or Stark net but at the same time when I move my phone to that system I don't have to trust The Operators of that system now the entire off chain system could be freely corrupt malicious wanting to steal my funds and the bridge will protect my funds and that's basically at the heart of the pH of the softchain database and then you have this bridge that then protects your assets when you bridge onto this opt-in database and all the debate is really being focused on does the bridge Define the database or does the community Define the database and that's sort of at the heart of the entirety be it but that's maybe that's a bit of a long answer actually so I apologize well I think that's basically what they've been it serves the context very clearly and so Mita and Stephanie what do you what do you take on this do you feel like yeah yeah sorry um yeah I think Patrick had an incredible explanation of what blockchains and Roll-Ups really serve to do and like adding on to that for me it really comes down to semantics I think it really depends on what you think is scaling and the most like simple sense of the word for me scaling it ethereum means steps being able to use ethereum native assets so like erc20s eth Etc on different block spaces and essentially scaling expands block Space by allowing new execution environments to be built on top of ethereum which can then be made available to all of the youth-based applications and I think Roll-Ups unequivocally do that so that's kind of like the side that I land on Stephanie yeah I'm coming in the same boat as well I think at the end of the day right like there's also that bit about social consensus that you know the community could get together and they could then you know move the bridge elsewhere um but I feel that that would be you know it's incredibly costly at the end of the day you could say that there's social consensus behind everything right um so I I fall more in the camp that the bridge is really important um and is you know a big part of the roll-up um but I do see the other side as well um but in terms of scaling ethereum I know there was you know that argument about does Jersey scale um New York and I you know I thought about that a little bit as well and I I also understand that perspective but I think I was hearing in a Twitter spaces about this idea that well if you add a new disc fork in a city that does scale the city um so I think that at the end of the day although a roll up at this present point in time might not be scaling the base layer of ethereum there is the future in which we have dank sharding you know other upgrades that will um you know the will change in order to you know do the ethereum-centric roadmap and move forward and scale the entire ecosystem so when we get down to semantics I can understand how Roll-Ups don't technically scale ethereum at this given point in time but if you look at the long term I believe that it does I believe that they do I mean a follow-up question would be again about semantics mostly um should we consider Roll-Ups as l2s or layer twos Patrick um I think layer twos are a terrible way to describe all of this elevation I mean it says a word that popped up because of the lightning Network back in Bitcoin let me sort of hijacked it and I feel like because of that people get confused about what raxi building and as I mentioned all we're really building are bridges to offshoot out the other other blockchain systems and so it just really comes down to one of the trust assumptions when you use that bridge and one of the trust assumptions of that other option system as well so yeah I I use the layer to freeze because there's a buzzword that everyone understands but I think it's a really bad buzzword because it's just confused but like even like Layer Two you can think let's just say it's like starkness Palladium where you have this trusted committee that keeps the data off Chien is that really a layer to you there's a trusted committee versus another Bridge that's a civilized in the Twisted committee to move the funds back and forth so yeah I think it's a terrible phrase because it's sort of confuses totally agree I mean I think all of this is just serving to expand block space so I tend to just use the word block space as well I agree I think I think it's so hard now because it's been in the popular discourse for so long that like if we start to change the words like too drastically like as we as we've seen it kind of gets everybody riled up um but I do agree I think in the far future this idea of the kind of parent versus child relationship is going to change as the ecosystem expands and we see more Roll-Ups um but it is hard to change the terminology at this given point in time okay so I'm not moving away from you know roll-up world and the model of world if you go back to what people often refer to as the monolithic worlds right you know people I mean at least in the crypto Community right it's pretty kind of standard now that they all agree that model architecture is fundamentally a better design but what are the benefits of a monolithic architecture that people working the model space should definitely acknowledge and maybe should learn from as well sorry I was gonna say I can just jump in with one example and then would love to hear everyone else's um I guess one of the best parts of modular structures in my opinion is that you have specialization at each part of the stack right but with that you're basically introducing all of these dependencies at every layer and that increases kind of like the exploitation risk of every part of the stack so in a way monolithic structures you kind of know exactly what you're getting um so a new trust assumptions as well so for example if you're using a new dla with something plus assumptions then you are kind of following that as well exactly yeah so you I I would say monolithic architecture definitely has like some level of trust minimization and because there's less experimentation happening um I guess you could maybe assume that there's like less mistakes or like there's one thing that people are working on so those teams are spending years building out those those layers of the stack versus again with kind of trusting different modular parts of the stack you're trusting that each one of those are acting perfectly and they might be totally independent actors so I think yeah just add to that um I had a different um thought in terms of what um you know the modular world or what or sorry I'm gonna jump to my second thought um something that I've been thinking about a lot is about infrastructure infrastructure concentration in general so although when you add you know different pieces to the stack you know you have the potential for there to be failures of the different pieces there's also kind of this unforeseen you know underlying stuff going on between you know any system including monolithic structures so um you know we're all um the nodes located where is the majority of the stake what how many clients are there using and you know that's fundamental to any blockchain but I think that um people don't realize that like I think that at the end of the day it's when you're just scaling one system like it it can be just as dangerous as scaling you know I think I'm sorry I think it's actually less dangerous when you have the different components because you have the optionality and those people that are gonna be building different things and thinking in different ways and using different infrastructure so I think that it can be seen as a benefit as well as a kind of a drawback that there are different you know pieces put together um it can lead to like some additional issues but also protect against other kinds of failures as well I've been trying to think how to answer this one it's quite tricky um because I I historically speak and I lived through the block size Wars you know I sort of saw the conflict amongst the community and at the heart of the block size Wars was because he didn't understand anything about modular systems we just assumed that you can we have a monolithic architecture and that was the only way to scale these networks and so either you have the and then you have this trade-off and decentralization you know do you make it the system more affordable with bigger blocks you know bigger blocks cheaper transactions more users but then you reduce the operators of the system and people who can verify the system the other hand you have small blocks you know I can run this in my Raspberry Pi and everyone can verify that it's correct but then this is a very expensive chin is a wheelchair and you know it's not decentralized in terms of users because it's only useful for Wheels um and that conflict was very toxic because we just didn't understand the design space of oh there is a third way of actually just modularizing out these different problems um maybe the benefit of them all I think approach is really bootstrapping maybe that's the best benefit you know you want to most of the most of the success with projects in our space got code out as soon as they could and then they let a community emerge around it you know if you try to start from the beginning they build your Ivory Tower then you're going to you know fall behind the market because it takes five years to deploy then no one cares after that point so maybe the model I think was useful for people that bootstrap a project see if it works then afterwards they begin to modulize it but I think if you were mean as a model that they project then you just end up in this toxic War because you know you can't achieve the stated goals of of the project because it just doesn't allow you to achieve it that's the Bitcoin problem because it's just a shame I mean talking about yeah I think that's good sorry I was gonna just add one more I think that was a really good point I think similar to that um different economic models might also lead to very different structures so like imagine if each part of the stack has its own token so like the da layer execution Etc so beyond the fact that coordinating all these tokens might add complexity you also just have the fact that these tokens might have totally competing incentives and so one of the benefits of the monolithic approach is that you don't have to deal with this at all and it might be more Capital efficient this is a big question mark but I think it's one thing to think about yeah and actually maybe this is the jump thing because I don't know people are listening to this when we have to they may not actually know what a modular architecture is um so I mean there's basically like three I like to think of three different layers you have the datability data availability layer which is basically saying here's all the data blobs and here's the tool ordering of the data blobs you know this given the data blobs hopefully we can then compute a copy of the database then you have like the execution layer where we take the data blobs we execute them and then we actually you know compute a database itself and then the third one is sort of like the settlement layer which is uh this is like you know the ethereum layer and then the action so I'll actually get that in a second the settlement layer basically is the database that records all the offsets and it should really be minimal execute well there's actually at the beat where the settlement layer should just be data or it should be data plus a little bit of tiny execution so the ethereum root map would be that ethereum serves as a data availability layer on a settlement layer then the execution layer are the rule UPS something like Celestia would have just a data availability layer then an execution layer on top and there are a settlement layer on top and then your execution layers you know the Sovereign roll ups uh ordinals would be the same for example and so we're even in the modular space where the bidding on how modular it should be because you lose some nice benefits if you really do distinguish the data and the settlement layer and that's something that's going to be tested soon with the launches last year so I'm really excited to see that happen versus something like ethereum I've been talking about bootstrapping off I mean it's easier to a bootstrap it's a monolithic Network or chain talking about that you know it's it's I guess an equivalent of that would be an app chain right uh model in the example in cosmos space um certainly you you did write quite a few articles on this at least one article I remember that I've read what are your thoughts on Roll-Ups versus app teams right what are the benefits of uh building a roll up directly for your application uh versus building uh monolithic app chain for example using uh you know sdks like Cosmos sure um so so app chains um you know born under the cosmos ecosystem are this idea of having an entire monolithic blockchain um for the for one usage uh so you could have you know osmosis is a great example of you know a defy um specific chain um you could have um another infrastructure specific chain I think dvpn does decentralized vpns um and so it's in the cosmos world it's kind of a collection of these you know Sovereign independent chains and they're connected um through you know a bridge message passing layer which is IBC um so so the difference between an app chain and a roll-up well the biggest difference is obviously the the you know an app chain is a full monolithic structure so you have um all of the layers you know put together um you have you know a validator set and that's you know I think one of the biggest detriments to an app chain is you know having to bootstrap that validator set like we were talking about before um it's a Herculean effort I think you said Patrick um so super challenging right and you have to attract you know a good number of just distributed um you know individuals to operate your chain um and that can be really challenging um so the difference between you know a roll-up or an app specific roll-up and an app chain is that you have an app specific roll up can be you know specific to a certain type of um you know need so defy or vpns or whatever it be um but you're you're abstracting away you're using different infrastructure for that base layer so your your da and your consensus is um you know outsourced outside of your system um you know that way you can only focus on your execution environment uh you don't have to worry about boot straight but bootstrapping that validator set um and you just have a little more flexibility there in terms of you know also you can you know be a part of a settlement layer so if you're you know on if you're an L3 sorry I'm going to use the terms and L3 you know on an L2 you know you share uh the benefits of sharing a settlement layer with other Roll-Ups um and that would be the primary differences I I think in terms of what is best built on a roll-up as opposed to an app chain I think it really comes down to you know developer preference like what what are you looking to do um you know do you really want to build that base level infrastructure is do you want to have that customizability right to the base layer or do you want to you know Outsource that and focus more on your execution environment and focus on the benefits of being you know uh just kind of the um what's the word uh the Simplicity of just operating in that space um I think I'll pass it off to to meta I think you probably have a lot of good ideas about what would be really specific examples of you know an app or a roll-up or an app specific roll-up as opposed to um an app chain um but for me I just feel that it comes down to the developer preference and you could go both ways it's just about kind of thinking about your specific application first and then building from there yeah totally I think Stephanie did a great job giving an overview of like why you'd want one over the other I think for me like an interesting use case that pops out for kind of the opposite like why an app Chain versus a roll up is when you need to really have fine-grained control of like Security in the consensus layer so a good example of that would be like um an application where like the the regulatory um boundaries are super super strong like you need to know like who your validators are you need to know what the rules are to like pass any sort of like transaction validation Etc I think it makes a lot more sense to build um an app chain rather than a roll-up because you can have control of every part of the stack kind of like what we were talking about with the previous question uh Patrick anything to add yeah um I was just trying to make him prove that's what they're saying there or maybe a great points um maybe the way I like to look at it is sort of um so I know we're using the up term option but basically what we're saying is we want to build our own entire network and then obviously pass messages using IBC for example um and I was mentioned before that's sort of like a Hercule effort you know those systems are secure because you assume an honest majority of the participants are honest effectively and they're willing to you know protect the cfd of the system and the liveness of the system and that's that's a pain they asked to do to be honest just a lot of work to put out together where what's really nice about a roll-off system is that we are what we're really doing in this annual all these systems do need this ecosystem this honest majority this consensus protocol they run the system you know basically they're lowering these do exist a rule I'll just take something like ethereum or Bitcoin right now and it says take that as a back box they say okay we've got this now and we can just use ethereum it already exists it's done all the hard work for us then we deploy the bridge and then the way the bridge can be set up is that we just need one honest party to protect the system um as long as one on this party can stop up they can stick they can participate then the entire cure on all the funds are protected as well and so it really just comes down to this path of least resistance you know do you really want to deploy your entire whole network or do you just want to set up a system where you need one on this party that could be anyone in the world at some point you know right now obviously it's still being built but long term it could be one honest part is a steps up you know even if the operators are of the system are malicious and so it just becomes a technology stack you know long term it's like an SDK I want to deploy idea to vs why do I do if I get I get the code I deploy the bridge creates my database for me I have a sequencer made out of one or two executors but is open as permissionless then anyone can step up to do the hard work if that's necessary um for me it's just again least resistance this is an easier stack to deal with and you get a margin thousands of these emerging because it's just way easier for people to do and now there's lots of roll up as a service companies even out there again it's like a roll up as a service company because it's way easier for you guys to write software to deploy this versus crit and you know unique networks for everyone uh so it's just this way easier to deal with I don't know that's true it's much easier to deploy a roll up as opposed to binding I don't know even 100 Valley leaders let's say if you want to have for your apps it's pretty complicated and time consuming and then you know exactly yeah is this your point so that's a great point you know if I want to set up an option maybe I need to have 100 validators I have to go search these Source these people initially and bootstrap it or you just have one on this party it's just easier isn't it just a lot less indeed indeed yeah go ahead I was just going to add that or a lot of these um like a lot of the no code you know world of deployment interfaces you know I think like the goal is to like get a roll up and running in you know 10 15 minutes um which is just fantastic yeah you know let's let's talk about that so uh of course you know I mean autoclave is building a roll-up and service product and the other other you know parties and you know vendors and you know Builders out there they're building similar products uh you know some some of them are I'm not sure if they can be called roll up a service but more like some people offer this SDK model and the others offer kind of a managed service model like a SAS model but the main question like at least uh common criticism that I often hear from some people is that there isn't enough demand for application specific holdups today so what do you take on this uh do you feel like like there's demand and you know it's just that you know you guys don't have it or is it like it's still building and it will take a few years for the demand to really come to a place where it will be noticeable yeah I love this question I think it's a bit of both like there's like the Chicken and the Egg problem like yes like right now maybe the demand isn't there yet but I think part of it is because it's not super easy to stand up a roll-up right now so like to Stephanie's point the reason why I like Roll-Ups as a service exist is because not every team has the developer bandwidth to like manage all of these things and also like we shouldn't have to reinvent the wheel every time right you can't expect like a consumer founder to have to keep up to date with all of the research happening at the infra layer it's like not a good use of their time but I've definitely seen in my portfolio for example people are very much thinking about how to scale how to like control their own block space how to look at different things like Mev as a business model Etc and all of those things kind of require specific knowledge at the infra layer which if a company is willing to kind of stand up and make it easier for them I think definitely people are going to use them so I think abstraction is the key so once once these things proliferate and once people understand why they should be using their own block space and kind of controlling their own rollups once that idea launches it'll be a lot easier and that'll kind of increase demands for for role of as a service products um but I'd also love to hear your case for the summer since you're thinking about this every day so at some point would love to hear your thoughts yeah sure I mean like I can go ahead so uh well I do feel that um there are certain types of developers or at least companies and nieties out there that definitely do not want to go and build everything from scratch right even SDK is because that's not the expertise they like it or they still have to go and understand sequences and things like that right and so what they want is they want to Outsource the entire stack to someone else and say guys why don't you go and manage this for us while we go and focus on our game development for example right because that's what the core expertise is and so in those cases we have seen uh people coming towards from the gaming industry and people are building around experiments around social fight you come towards to us and say okay guys we don't we want to have our own chain because historically too right games have always tried to build their own infrastructure right because you know look at taxi Infinity initially it was an ethereum when they want they went to one do they own thing uh more recently running is a good example of of an app chain for games games are or game developers are definitely very keen on uh building up specific Roll-Ups themselves or using infrastructure provider um D5 has has certain restrictions uh many of these people feel like you know they would miss on liquidity you know it's not good to have sitting on your own roll up and things like that maybe that will change with better Bridges and so on I'll stop here uh Stephanie if you have any anything to add um no I guess I would say I really agree with you know both of your points um I also just think that I guess to add to that um there's just going to be so many different people that want different things um and now that we make it easier um the more abstraction we have and the more you know Roll-Ups and service kind of products we have the easier um I think uh before you were kind of talking about how um there's the role of sdks and kind of the no code deployment and I think I I'm really curious to hear what Patrick says about this specifically but um my original post I wrote a piece about roles as a service and I'd kind of grouped them all together I put sdks and sequencers as a services and as well as no code together but the more I think about it and I think there was a debate on Twitter about it as well I think that sdks probably don't fall into that definition of roll up as a service because a lot of these sdks are you know open source or like you know Progressive licensing and they can just be used and so roll up to the service providers can build on top of them um and I think um you know arbitrum would be an example that and correct me if I'm wrong but the way that I kind of looked at it with the licensing model where you know you can freely deploy um anything with arbitrum Nitro on top of arbitrum but if you want to deploy um you know roll up with Arboretum Nitro elsewhere you're going to have to you know make an arrangement with the Dow or get the data you know agree to that and so I thought you know there has to be some incentive for the Dow to want you to do that so in some ways you're kind of treating the use of the code base as a service um so here's to your thoughts on on that Patrick yeah that's cool I mean the other sort of three ongoing questions now isn't there so let me tackle that one first around the license I think that's important um no that's really important so basically uh everybody comes down to the heart of how whether companies can offer roll up as a service because if they can't get the license to the software then they can't offer the service come they um and this is something that the orbit from dial really needs to do for some point I wrote a blog post a long time ago before I joined arbitrine and it's and what I was really looking at was sort of the licenses of like the problem with you know the business source license and I only wants to say business Works licensed for now is a lot of startups don't want to go and interact with a company in order to get a license so then they deploy you know the actual type of software that sucks from everyone's perspective um you know we don't really want Gatekeepers that have to deal with this on the other end of the spectrum is sort of the Apache license and the MIT license is that what you what you can also end up fostering is that someone might take your code and compete directly with you and then you know harm the ecosystem in a sense we sort of saw it with sushi and uni Spa but then uni Spa pioneered the business source license um that also sucks because you know if someone takes your code deploys it they don't feed back into your ecosystem then that well that sucks for arbitrary doesn't it or any other network or any other project that's building software so I really like to see the opportunity to deal with and it's up to them to deal with it is a ecosystem license or as long as you satisfy some criteria maybe you share some of them you back to the Dow then you're free to take the code and deploy it any way you want in an error 2 format we have some evidence that at least for the optimism up stack like the BS protocol or coinbius base and the other uh I forget the other company now that's doing it but they've all agreed to your Revenue share with optimism or at least optimism Foundation whatever whatever legal setup they have and that's cool you know like that's something that could be incorporated into a license and then incorporated into the SDK and then people just that's how they launch l2s so I think that's definitely a problem for the arbitrim down I would love to see them start talking about that um because you know it's their decision that they do with with the arbitrine code that they want to do now so that's the the answer to the license question then the other one is sort of the original question was sort of like why should we care about these systems what are the use cases are resist too early with them and I like to look at I think there's two solid use cases you can think of one is we want users to interact on the roll up itself uh and I think from a user's perspective users don't care you know users don't really care who custodies their funds but operator Studio you know if I'm a trendy startup today and I want to compete with coinbase uh can I really replicate there's operational security that protect billions of dollars now that is a you know how they protect their assets even binance as well you know they're I think they have like 60 billion dollars or something crazy like that you know setting up the human process to protect those funds is very difficult what's really nice is roll up as a service or as a software stack is that I can take the code deploy it and then the software protects the funds for me so as a trendy startup I no longer have to worry about custody or protecting users funds I can just actually focus on the service that I want to provide you've seen this already with like Loop rain DUI DX another you know X genes like rule ups and obviously coinbase is now doing the exact same thing they've realized oh we can deploy a roll up and we don't have to protect our users funds now you know we can have coinbase custody then we can have you know a real app that revise Financial Services like creating and swapping and Loans Etc so I think that's like one value I think it's the operators that care more about that because it's way easier for a startup now to compete with coinbase um the second one is and it's more like a it's more from the perspective of smart contracts that live on the blockchain already so maybe you know maybe they already live on ethereum but they want to get some external information it's like an oracle service but like chain link or maybe the subgraph when you think about these Bridges at the heart of it is that they just pass messages you can pass messages from ethereum to the option system or from the opt-in system back to ethereum so an ideal world because we all ever I'm sure people know about subgraph we can index data and then query it and get it results it'd be really nice that if I was a smart contract living on ethereum I could ask the Oracle for data the Orca could respond to me but I have confidence that a certain algorithm or process was executed on that data so you get confidence about how all the data was manipulated now the source of the data obviously depends on you know garbage and garbage out you know you always have that issue but at least have confidence about the execution on that data and that the result could be correct where today users rely on certain parties to attest to the fact that it was done correctly so there are two use cases I see that you could build a much better subgraph or you could build a roll up that targets users and protect and that'll be more for operators probably than the users themselves so sorry I know that's a really long answer but I think that's sort of how I look at it today give the follow-up question to Stephanie this one because I think she mentioned about mentioned about sdks or shared sequences and so on right so do you feel like uh for some of these for example shared sequencing to be successful you need a lot of rollups did you agree with the statement um you know I think there certainly is um I think that the success of you know Roll-Ups as well as sequencers it's obviously coupled right you know you can't really have one without the other I mean you can have a centralized sequencer um but you know which is also an option but I'm sure a lot of projects are not going to go in that direction they're going to want either their own decentralized sequencer set um or a shared decentralized sequencer set so I think that you know there's a lot of network effects like you when you you get a lot of benefits when you share a sequencer set um you have you know that if you share a dealer in a sequencer set you have a lot of composability benefits and in terms of you know bridging and um Atomic transactions even um so I think it's it will be really beneficial for some of these shared sequencers to have you know a lot of Roll-Ups they're plugged into and it'll be mutually beneficial for both um the thing is is that I don't think anyone has really mauled it out modeled it out yet unless I'm wrong but there's probably going to be a limit to which um you can share a sequencer set eventually you're going to have too many any Roll-Ups and it'll just get the load will be too much if I understand correctly um so at the end of the day I think that there is space for you know a variety of different shared sequencer sets and it's also good to have that friendly competition and it's a positive sum game in the end you don't need to have you know just one sequencer set that everybody uses but if you join a sequencer set you want to be a part of that ecosystem and have those composability benefits um and so I think that sorry I think I'm kind of blanking on the original question if you I think the question was whether they would be successful the question you you have this these I guess uh services or service providers today like you know shared interesting networks or interoperability Solutions out there um and of course they can be successful with let's say a few uh you know Roll-Ups out there but they become even more successful when you have more and more Roll-Ups right so for example the grants of every smart contract is a roll up essentially then you have uh these interoperability Solutions or shared sequencing really become the biggest I guess uh The Leverage on these uh you know the growth of Roll-Ups in general right so the question was that do you do you feel do you agree with this or you know do you feel like no they can and they can find other ways to to tap into the consumer base and user base well I mean you know team teams are smart and you know if they needed to Pivot for some reason Roll-Ups didn't work out and they didn't want to be sequencers they wanted to be validated or you know something else I'm not sure they could pivot and do so um but yes at the end of the day right the shared sequencer networks need the Roll-Ups to be able to succeed but but I do really feel that the ecosystem benefits that you're going to get from these shared sequencers are gonna really create all these ecosystems that we haven't even seen yet um so I'm super curious to see what happens um but I think that there's a huge need for it especially right now as we get closer and closer to seeing a lot a lot more live roll-ups and roll ups yeah totally I was actually going to mention this later but I think I would really hope that there's more of a focus on this in the future like I'd love to see kind of a greater emphasis around like interoperability and shared liquidity on all of these like shared um roll-up infrastructures because for example in an ecosystem like optimism's op stack um I think shared sequencing could really help something like interoperability where you're making sure like base is eat is um like easy to access which is like normal eat and all of the other like optimism chains Etc right now that isn't like super easy actually like the bridging ux is not great um so I think that this is this is a place where people a lot of really smart teams are working on shared sequencing and I think this will help a lot I also think around value capture this idea will be really helpful so shared sequencing kind of opens up a huge possibility around Mev and so with more research on shared and decentralized sequencing I I think a lot of these l2s or blocks based companies will be able to think about value capture better because right now um most L2 tokens like aren't really doing much yet they're they aren't capturing value in a way that makes sense Beyond like governance and I think shared sequencing is kind of like a potential Avenue like a solution for that it's not trick any thoughts um I think I think both the comments are really good I'm just trying to think what I can add to it um I think uh I think it's really important to distinguish what the sequencer is supposed to be doing again for people listening to this maybe not actually know what the role of a sequencer is and lots of teams get confused about what it's supposed to be doing as well so in a roll-up you have two different actors you have the sequencer you take you know they interact with the user they take the users transactions they order the transactions for execution and then they pass them on so all they're doing is ordering they're not executing you know they they know what the execution will be because they decide the order but you know they don't actually do the hard work of convincing the bridge about what the database looks like then you have a set of executors you take the order transactions they execute them and then they convince the bridge oh bridge this is what the database looks like so with that in mind um what's really nice about that architecture is that the sequencer potentially has nothing to do with the security of the system you know if the sequencer goes offline users can just take their transactions send it to the bridge directly it gets ordered for execution then you assume there's one honest executor out there who picks up the job and obviously executes it so that's cool you know the sequencer can be one actor and it doesn't necessarily impact the fundamental security of the system now there's some of the things in the short term they could do you know they could just promise to include your transaction and they don't and that could be of half implications for these D5 protocols what's nice is that you don't have the necessarily decentralize the set of executors we don't need we don't need proof of sticker Theory we don't need 500 000 different sequencers we and and then if you think about from that architecture the sequencer's job is to offer a fast path and the user experience and to make sure that the user gets the best experience but interactive with this rule up and that comes down the one part of that is you know latency who do I talk to you know the user needs to know where do I send my transaction the basic problem the second one is already been alluded here is composability you know if there's two different rule Ops let's say the user wants to you know take their money on arbitrim and then send it over the optimism do a transaction there and then they bring the money back in that case there is a benefit to have a shared sequencer who runs on both systems because then they can coordinate the transactions uh I think personally for me that's a short-term solution until we have better bridges that connect the various relapse and it can provide you asynchronous you know where I they cut protocol you know I send the value over I interact and I bring it back it's not immediately composable it is composable but it's not synchronous it's async it may take two minutes versus a very nice promise that'll be done atomically or as close to atomically as possible um and then I think when you think the sequencer said you know the goal is really just to make sure that uh you can meet you can make sure that the fast path is always available to anyone in the world or at the most people in the world you know that's really the goal there is to make sure people kind of actually transact on the system that is credibly neutral thank you Patrick I think they're running out of time so take one last question so uh there's a question about what the Futures would be or what you think or hope a feature should be so what are your predictions or what are you hoping for the rule of space to achieve in the next two or three years um Stephanie you want to go first yeah wow sometimes it's so hard to Envision what's going to happen even in a month from now you know everything moves so quickly um but I'm really excited to see a lot of these projects launch on mainnet um I and so I think you know Celestia being a huge a huge project that we're all expecting um and then you know all of the shared sequencer networks that are going to come online as well and just you know with real with the the growth of role to the service which are you know getting also a lot more established you know just all the different plethora of Roll-Ups we might see um in two to three years I to be honest with you I can't exactly tell you exactly how it's going to look but I feel like there's going to be a lot of lessons learned um you know we're going to discover some issues things that didn't quite work the way that we thought that they would in theory um and we're going to correct them and I'm just excited to you know be a part of this journey and you know documenting and seeing you know where we all go it's it's just super exciting me though yeah I have a couple um I guess for one I'd love to see I kind of mentioned this but a bigger focus on value capture so if we're right about the roll-up Centric future Roll-Ups are going to be in the flow of so much value and so how are they going to be able to capture that value so that kind of requires effort done both around shared slash decentralized sequencing um around like thinking about like incentive structures Etc so I'm excited about that and then the ZK space is moving way faster than I think any of us really expected so I'm really excited to see a lot of those kind of come on Main net in the next year so and then I'd love to see more out of the box models for specific use cases I think a lot of the roll-up as a service companies are going to be driving this but let's say I'm a game like I want something like so easy to use that it's like specifically built for like high throughput high tech rates Etc and it's just built for this use case I think you can do that for a lot of different use cases so I'd love to just see like developer experience getting better around roll ups and then the last one I'll say is around um kind of Greater emphasis around like bridging ux kind of like what Patrick was talking about like how can we get Atomic interoperability between all of these connected Roll-Ups um but yeah I'm really excited about all of this I feel like the roll-up space specifically has a lot of engineering bandwidth behind it right now some of the top projects working on it which is great I think it's proliferating a lot faster than any of us thought Patrick um yeah maybe some concluding as well so I think what's important is that we can see the results today of how Roll-Ups sort of are going to sort of look in the future but it's important to remember this has been a violent Journey it sort of started in 2014 with the original side chain paper for Block stream they don't use the word bridge in the paper but they were basically trying to build a bridge where I could take my Bitcoin lock it up put on a side chain transact there and then bring my funds back um and it's taken a very long time to solve a lot of theoretical issues and of course practical issues are just deploying these systems they get to where we are today where there's Maybe four to five big rule UPS you could say you know or six you know let's say five now that are probably live you know so then the question is where do we go from here and I'm I'm really cautious that like this is very this sort of resembles the beginning of you know Computing in the 1940s there's recently was cool by Thomas Watson from IBM Murray basically says uh you can't foresee a world where you need more than five computers and today we have about five Roll-Ups you know it's very very similar Dynamic here isn't there um and to me I definitely foresee the idea that roll ups can just be an SDK that anyone can deploy and it will protect the users of that system I mean what you're really doing is that you're removing this Power Balance between the user and the operator today the operator holds all the power and because of that and that the requirement of trust in the operator it creates a lot of friction for people that interact in the transact and work together but once you remove that par Dynamic where the user is equal to the operator and there's no longer this you know power Dynamic that you don't have to trust the operative assistant you just have to trust the platform itself then it's very easy to for people to just transact on the internet for example I can lock my phones into this Chinese web so that I can't read I could then play their horse racing game you know maybe they rig the horse racing game and they beat me but at least I know they go offline I can always get my funds back out where today there's no way I've assigned my money to a Chinese website because I just don't know the operator I have to trust them so once we can remove that par dynamic uh then yeah this is gonna flourish it's going to be hundreds of thousands of deployments of real Labs because it's just a better way to build an open database and protect users and if anything when you think of the regulatory sort of environment now that the US finds itself in sometimes you sit there and think you know are we the bad guys you know are we actually doing something really wrong in the erotic The Regulators are correct but it's wrong it's the opposite you know it's sort of we're building systems that is a regulated wet dream you know they're so transparent that you there's another financial information is all there on the blockchain and we're building systems that protect users at the very heart of it is just to protect the user when they transact so yeah I'm very very bullish on the field obviously and hopefully for the right reasons but yeah there's one by views for the future of course of course well with this we will end this panel thank you very much Stephanie Patrick and maida for your time it was great having you all talk about Roll-Ups uh Roll-Ups and service providers and other related topics hope you all enjoy this conversation as well thank you very much thanks everyone yeah thanks for having me today so today I'm going to be telling you about permissionless interoperability why I believe it is the key to realizing the potential of modular blockchains and why the potential of these two can combine to allow blockchains to finally scale to be able to support a global audience so now in doing so I'm going to have to tell you about hyperlink the first promotionless interoperability layer and in doing so we will examine it and say overview first and then dive into the specifics now before we get into that a little bit here I probably like to talk about this concept at The Interchange Highway what is the interesting Highway think about a path a path connecting between the increasing array of these chains that are just kind of otherwise floating through space The Interchange highway is the way to put on and off ramps right away to connect in between these between this increasing set of chains and like regular highways making it easy making it accessible making it relatively face um safe and certainly making it fast so today we'll cover how permissionless interoperability and modular security combined to give us something like The Interchange Highway which is of course enabled by hyperlink so where are we today today we have these internet computers these internet computers are really cool they're amazing they can do a lot of things all the way from giving us Sovereign money to giving us permissionless access to computation and that's all really cool in a primary way that these internet computers are able to do this is due to the fact that for the first time they have both created an environment in which we impose and also materialize scarcity and visual resources I really like this visualization here what we see here are Bitcoin and ethereum blocks visualized as buses and I think it's really great at illustrating this point of materializing digital scarcity because if you think of it the bus has a certain amount of seats and that amount of seats is kind of fixed through time it can get another bus with more seats a single bus has that same amount of seats and think of that like the amount of space in a box the fact that more passengers want to get on the bus does not change the amount of seats the fact that more transactions want to get to a block has not changed a space set block and this is both the most interesting but also perhaps the most problematic thing about these internet computers because you know think about all of us here today if you're listening to this time you presumably care about blockchains very much you want to advance them you want them to have more adoption globally maybe even have like myself Ambitions of creating an alternative Financial system it's a global one that is built on these blockchains but we only have so many buses and so many seats on the bus so if all of us here are working to increase demand we know what's going to happen right there's the same problem the same scaling issue is going to hit us every single time and it's of course the issue of congestion and what happens with these scarist digital resources when demand decreases well we can't produce on demand more seats we don't have you know the amount of seats on the bus don't change dynamically in response to the amount of demand for those seats and so we end up with surge pricing and a gas is just too damn high so what do you do about it right if you're here today it's presumably because you believe that the most promising or at least one of the more promising solutions to this problem is the modular vision for blockchain it's a module of future for blockchains if nothing else it seems the most promising path for ethereum the road the roll-up road map for ethereum is certainly something while working towards here and I'm not going to go into the details of what this means because presumably again if you're here you know enough about it but um separating the different services that are provided by a single chain into different layers on a stack and those being provisioned by different providers this is really what we mean by the the modular vision for blockchains and in particular one consequence of it is that you're going to see not just a few very large or capable internet computers but rather an increasing number of smaller internet computers that are all working off of larger ones and settling to larger ones but so there are tremendous efforts by a lot of the people who are here today to make it increasingly easier for someone to launch their own modular chain to launch their roll up whether it's rolled up Frameworks like the op stack whether it's data availability Solutions like Celestia and whether it's roll-up as a service providers like all player that brings us here today all of these to make it increasingly easier for you to spin up your own chain and roll up in fact it's not a feasible to say that already today within a number of hours you can go from from nothing to having a roll-up live and so think about this world where we're going to have an increasing amount of change because again it's increasingly easier to create your own modular chain of all these chains coming up and floating through space each one you can think of it as like a new island in the ocean but how does it connect to the others how does it connect to the mainland how does it connect to the other continents and countries that exist in this world of blockchains right sure you can use an optimistic route to go between your roll up in the mainland but that's a compact right that barely barely suffices what do we do when there are thousands maybe even hundreds of thousands of these streets how do we connect them all enter hyperlink and with hyperlane we have the first permissionless interoperability layer as I mentioned before the tool that lets anyone connect any chain a solution that fits this modular world of blockchains where we're going to end up with an increasing number of chains this little visualization here think of it as that interchange Highway that we described before the uh another pathway that will connect between these new chains as they begin floating through space and providing a safe and secure and easily accessible path for folks to Traverse on so why is permissionless syndrop really really necessary well blockies historically have had an issue of connectivity but by 2023 today there are modes that connect between blockchains the issue with those is that they are entirely permission and what does it really mean to be permissioned it means that you need to ask someone to allow you to use that service all right if you create a new roll up today you can't just have one of the major interoperably criticals supporting you just because it does not work that way enter hyperlink hyperlane as you said before is the permissionless interoperability layer the tool that lets anyone connect any chain hybrid means that for interrupt really you don't have to depend on anyone you really can get sovereignty in your interoperability and if you look at this visualization here into something that I put together using a mid journey to try and illustrate that concept of The Interchange Highway that I mentioned before think about our modular vision for blockchains materializing it really does mean that ultimately we're going to be ending with thousands then hundreds of thousands maybe even millions of little modular chains and a permissioned interoperability solution is simply not going to work it's not going to fit this world so we need something permissionless something that can be served and owned ultimately by the chains that want to use it and this is where hyperlink comes in to solve what has been the major issue in blockchain connectivity today which is not just the issue of connectivity in general which was a problem leading into the last few years but today we have Solutions in 2023 the issue with them is their permission in nature now how does this happen why is this even a problem well primarily it's a problem because the current landscape control early Solutions has these one-size-fits-all security modules primarily this is the case because they have tightly coupled the product of connecting between chains the interface for connecting between chains as in this illustration here the messaging interface they've typed a couple of it with a security model and you can't blame them for it right crypto is the most adversarial environment that's really ever existed for computation or at least one of the most adversarial environments and in it security is Paramount we know what happens when security goes wrong and so it felt uh probably felt very prudent and sensical to tightly couple these things but certainly they don't have to be tightly coupled and security can be interfaced in modules now as long as it is the case that they are tightly coupled if you want to use that messaging interface you are going to need to receive access or receive permission from the operator of the security model for you to be able to use their messaging interface can this leads us into the concept of modular security being a solution for the problem of permission interoperability so all of this leads us into modular security being a solution to the problem of permissioned interactive building we're going to get into the concept of module of security and further and further depth as we start talking about how hype building works but it's a high level which you should take away from this is that modular security decouples the messaging interface from the security module at hand and most importantly it allows for customization and configuration on behalf of the connecting application the connecting roll up such that you can choose from an array of different security models and I think most interestingly use different models based on what your users are doing right so if you think about you go to the bank and you're trying to take out a hundred dollars well they're really only going to ask for your PIN number but you try to wire out fifty thousand dollars and now they start asking more questions right they want you to come to the branch they want you to speak to a banker and answer well you know five or six questions about recent transactions the type of the account some different security measures what is the bank doing the bank is using a different type of security protocol or security module in this case based on your action so this is something that becomes available for us uh with the concept of modular security and again High building we believe that modular chains deserve modular security as opposed to being stuck with the form of permissioned interoperability as we advance the conversation it's probably worth getting into different flavors of interoperability so that we can better understand how modular security stands out now to do that we'll start with perhaps the ideal form the form that um many of us would opt to use wherever it is applicable and that is native verification what does data verification really mean at a high level it just means that you're leveraging the existing consensus that exists between the change that you're connecting and most importantly you're just not trusting any other thing or party other than chain a and chain B now first let me say verification is not magic right it does not mean you're going to have a secure interaction it just means that if you're connecting a and b as long as you are comfortable with the security on both A and B what you're getting has no additional trust assumptions but if something wonky is coming from a going to be you know native replication just passes that long and that's okay um now ideally we in crypto we want to minimize the Trump's trust assumptions that we have to take so native verification wherever applicable is really great the problem with it is that in certain environments it is not quite applicable because unless the environment was constructed to be able to support native verification what does it really mean it means replicating the consensus of the genes you're connecting on each other so you would need to replicate a on B you'd need to replicate B on a this is where like client verification comes in quite handy but what do we do when we don't have like flight verification well replicating consensus without it is incredibly incredibly costly to the point of being infeasible so while data verification is something we'd love to use wherever we can unfortunately we can't use it everywhere at least not yet now not all hope is lost there are advancements right now things like serial knowledge proofs that ideally should make the cost of replicating consensus and being able to have um you know data verification in more locations notably in the evm it's much more feasible so it's going to happen eventually we're not totally there yet but okay do not despair so if negative verification is one flavor what is another well it's external verification if and Native we only trusted the chains connected with external verification we're now introducing a new validation mechanism separate from that of the chains connected it's a third party so we are trusting a we're trusting B and now we're trusting some C write some external party and if you're taking a look at this and you're like oh I I've seen something like this before I know what this looks like you're thinking about oracles yeah you're right like external verification boils down to being no different from the Oracle problem if you think about the issue of connectivity with blockchains in general the fact that they don't know anything about information and data that's not endemic to them well again that is a core issue and the Oracle problem but whereas generalized oracles are trying to solve a general problem being able to fit any data regardless of source in an interoperability we have a much more narrowly focused Oracle problem where we just want to sort of organize data and state coming from other genes so there are many possible configurations in an external verification Realm the external verified system could just look like some type of proof of Authority or a multi-sig or we have a actor or group of actors that we trust and we are comfortable with them and their validation results it can look something more like a proof of stake system and here it's important to note that these profile State systems are very different than proof of State consensus and say like you know ethereum where we use proof of stake to decide on processing of blocks who was taking an external verification system is very very different and very important because it can actually have devastating results if collusion or malicious activity is happening because the other end if you're connecting between a and b and you're doing it through external verification using proof of stake whatever B receives from a is completely dependent on the external verification system so if you use proof of stick and those validators are telling you that the sky is blue even though you think that the sky is red like it's blue from from all the intents and purposes right so if they are misleading you about the state then that is all you know there is no way for you on B to be able to verify that that state is incorrect and there are some very interesting implications to it and this is why generally speaking there are two modes for proof of stake and an external verification system one where the stake is concentrated in a single chain and then that value is put at risk for every other chain that's connected and another where a stake lives on the on each chain and there are different benefits to each one of these configurations in the first configuration you can get a mistake all lives in the same place so you get to benefit from some economies of scale from some concentration but what happens when a punishment is needed to be issued right when slashing needs to happen to one of the validators their stake lives somewhere else than where the offending incident happened right so say they're connecting between A and B the steak lives on the chain that is C and there needs to we need to communicate back to see about this this incident that warrants some slashing well how could we do that unfortunately the only way that we could do that is through the same system and so in a case where just one you know you have One Bad Apple presumably the other actors the other validators would want to read them out and so staking would be ensued in this case they can get cannot be executed permissionlessly but what do you do if all the validatives or a majority of them are colluding well why they're not going to wrap themselves out right so punishment really can't be issued through slashing in a system where the majority of validators are colluding and in that case there is a complete breakdown of security Now to the defense of these system this has never happened before but certainly it's a it's a theoretical um concern that is worth worrying about now in the other case you do get permissionless selection you're always able to prove and you're always able to punish an offending validator right then you can do it again permissionlessly you don't have to depend on any kindness from the validators nor do you have to have some type of honest majority assumption as you do in the case where the stake lives on a different chain but what's the drawback right like you now separate the stake and it is no longer concentrate and you lose out on some economies of scale another configuration that you get an external verification systems is an optimistic instruction think of this as being somewhat similar to the optimistic bridges that power things like optimism and arbiter but the key Insight here is you put some type of delay on the processing time and any message that is received is not going to get processed until that time has surpassed during that time anyone can run a watch or some type of software to check the validity of the transaction and if something looks invalid submit a fraud proof of course with external clarification there's a lot more configurations that you could get you know we are probably going to see more in the future but for now I think we've summarized all the major ones so now let's get back to this world of modular blockchains so with modular blockchains we're going to end up with a lot of hubs what does it really mean to be a hub I think of hubs as places that entice the formation of new Roll-Ups and where those Roll-Ups are settling back or sending back something to them it might be using them as data availability as indicate Celestia it might be settlement and sometimes even different forms of execution as is the case of ethereum But ultimately in my view a hub for modular blockchains is something Beyond which activity concentrates and leads to the creation of more and more roll-ups so why do they need permissionless interoperability because as we touched on before these hubs are going to lead to the creation of more and more and more and more and more rollers and again the creation of these Roll-Ups is going to happen seamlessly it's going to happen with you know decreasing amount of clicks and decreasing amount of time on the account of their operators of their deployers and so it seems completely infeasible that permission Solutions will ever be able to cater to this world of modular blockchains where we are probably going to end up with you know on the lower end if we're successful thousands of roll-ups so permission interoperability is not going to work right we covered that hopefully by now it's pretty clear why modular blockchains and permission of interoperability are like oil and water they're just not going to feasibly mix with each other permissionless interoperly we only covered that we covered the concept we didn't really tell you how is it going to work so with that let's get into hyperlink I promised before we'll tell you how it works now it's time to do that so in this visual you can see the entire hyperline system as it connects between two chains here we're actually seeing the life cycle of a message as it progresses through um through hyperlink and we're going from its origin all the way to its destination we're going to walk through this before we do that you should understand that for hyperlink to be supportive of a new environment three things need to happen or really you can say there are three layers or three levels to a hyperlane connection the first is the mailbox the mailbox does exactly what it sounds like it is a contract that sends and receives information between chains the second are the security modules you need to configure the security and there needs to be something in place that allows for the state being reviewed and sent from chain Inc chain B to be secured really all a security module is is some wrapper on something that makes you comfortable with the state that you're receiving from another chain so that's the second level security the third is the relayer the relayer is a permissionless agent anyone can run a relayer for their messages but it really are is like this off-chain bot that observes the mailbox and if it sees that everything looks fine and the transactions look okay invalid it will take those and process them on the destination chain so now that we covered kind of the high level let's get into the life cycle of a hyperlink message and it'll help us understand how hyperlink works so we have some contract that wants to send something to another chain it constructs the message and it sends it to the mailbox where it encounters dispatch dispatch is the primary function that you'll be using to send information from hyperlink to another chain dispatch takes in the message body right the bytes that you want to transfer over these device can have something as interesting as an interchange function call a extension of sending of assets to something that's not interesting as a message to say something like hey John how are you doing no I would advise you to do that but certainly you can it also takes in the domain ID that is the destination shape that you're sending it to and the receiving address now once that message is received in the in the mailbox it needs to be reviewed in some way it needs to be signed for it to become valid for uh for it to be sent off so in this case we can have some type of validators as we talked before and we might have a module that uses validators that might be either a multi-sig type of situation it might be a proof of stake situation or we can have an optimistic situation again as we talked about in the external validation case and so all of these combine to basically provide something that that creates some type of validation for this message being acceptable and ready for processing after that's happened we now move to chain B where or sorry the destination chain where Again The Interchange security module or the ism this isn't hyperlane lingo this links back to either those validators or those Watchers again in the optimistic case and before that message can be processed it needs to be relayed so a relayer looks at that original mailbox and if it sees that it's been validated by some validators or that the amount of time say on the optimistic uh on the optimistic case has passed and that message is ready for processing the relayer will then take that over to the mailbox on the destination chain world will initiate the process function as it does that now the final logic tests that come in from The Interchange security module are tested against and if everything clears right if there is a valid signature from a validator or we find that there has been no fraud proof submitted right the the integrator of hyperlink decides which security modules to use and how to define their logic and as long as those tests pass the message can be ultimately sent off to its destination importantly about hyperlink you can totally have a case where the message is included in a block on the destination chain but never arrives to its ultimate destination why is that well the relayer doesn't know yet whether the um the message being sent will actually cast all the logic tests of the security module it is totally feasible that the realtor take something begins its processing but then because of the security measures put in place by the integrator it just doesn't pass that is by Design and so certainly we can end up with a case where that message is again it is a transaction it lands and it is included in a block on the destination chain but never reaches the receiving contract again because it fails the logic tests through the security modules okay so now that we've covered that let's take the next step and do a deeper dive into the concept of modular security as I told you before preparingly believe that modular blockchains deserve modular security we touched a little bit on what my Social Security means in the context of hyperlink and I think uh you want to have some fun with it I think an explanation that works for kind of like a smooth brain like myself is think of a security module or hyperlink parlance and interchange security module in ism is really just some type of wrapper and that wraps logic that helps you get comfortable with the state that you're receiving from a chain you're connected to this can take many many different forms it can take the form of some type of Economic Security where the logic is hey I'm putting this amount of money at risk and if I misbehave you can take that money away from it you can slash me that's one form of an economic answer it can take the form of optimistic logic where I think you will accept that message only after a certain amount of time is passed during that time anyone could have observed that transaction and see does it look okay is this a valid State transition optimistic systems could even go past just if valid State transitions because if actually if you think about it majority of exploits in the category of interoperability they all look like valid State transitions so an optimistic system actually could go one step further and not just be limited to um fraud proofs and the valid State transitions rather it could just be sort of like a veto system you know in maker down we have this concept of the emergency shops or at least that's what it used to be called maybe it still has that name where maker Governors can kind of unwind the system in the event that they think something malicious is happening that's never really been an issue so far even though existing for multiple years it's perhaps only been an issue on governance forms but in practice that has not been abused it could be a really great defensive mechanism for maker down so why shouldn't there be optimistic systems that allow their operators to veto certain transactions if they feel that they are malicious or in some way are going to hurt the protocol that's integrating them what does this really look like in practice imagine now there's an exploit trying to drain funds from a protocol I'm watching and send those to another well that transaction then would not get processed because it would trigger it would go through the optimistic module and during that time there would be the option for um people involved in the operation of that security module to veto it to basically lock it prevent it from ever happening while this could certainly go against the ethos of certain people feeling that you should not be able to censor these transactions but realistically speaking don't don't when we want because that's their exploits right and so an optimistic module could also allow for that and then we also have something that could look more like a multi-state module right like where we trust a certain set of actors right imagine if you're going to be using something like circles cctp and affect what you're doing there think of ccdp as a system in which the security module really is a circle multi-sig you are trusting that circle is going to do this correctly and of course you are you're using cctp to use usdc so it's inherent it is the minimum it's implicitly or drastic Circle when you use that interaction now if an ism an endocrine security module really fall it is is a wrapper on some logic then the beauty of it is that while this looks like an external verification system it can actually just be narrowed down to a native verification system where it exists because all it is again it's wrapping some logic so certainly a good wrap the type of Lifeline verification that has been used for protocols like IVC it can wrap some type of zero knowledge proof really you can use it again for any type of arbitrary logic that makes you comfortable with receiving state from another chain and this is why we believe that modular security is the path forward for modular blockchains and when it comes to their interoperability most importantly is what enables them to operate permissionlessly so with that said let's talk about the differences in um bridging normally and bridging with a protocol like hyperlink so first things first you can definitely use a regular bridging mechanic with hyperlink and what does regular bridging mechanics look like generally they look like an Omnibus Bridge a single contract where all the assets are deposited and that contract creates ramped assets that then go to the destination chains so what's it look like well you get you get all the assets deposited to a single contract ends up kind of looking like a Honeypot and all of them have the same security module now if you did construct a regular bridging Paradigm using hyperlink one key difference you have is you'd be able to utilize different security modules for your Omnibus bridge but is that really interesting with hyperlane we would recommend a modular merging approach and our in Arlington we call these warp routes and warcrafts are unique because warcrafts are unique per asset so each asset gets its own Warcraft and warcrafts get the benefit from the modular security features and from the permissions features of hyperlink so anyone can create a Warcraft permissionlessly and then assign the different security modules they'd like for that Warcraft and now the assets uh can Traverse to any chain that they would like the tokens that are minted by a Warcraft are what we call in hyperlane interchange tokens now an interchange token is interesting because it does not have a canonical home it's not an erc20 that only lives on ethereum and then is represented elsewhere rather its home are all the genes that you can figure it for and at the moment of transfer it's not another wrapper that's being created rather it is being burned on the origin chain and it's being minted on the estimation chain so that's that for uh within a limited amount of time that we have we don't want to spend too much more time on this concept of modular bridging if you want to learn more um at the end of the talk I'll leave information for how you can contact us and learn more about all these things but certainly just look up hyperlink warfronts and you'll be available to learn all the information that you need now let's get into some other use cases that you can affect with hyperlink I actually should have probably spend more time on this because I find these much more interesting than just bridging but usually the first use case that modular chains have because they need to be able to import economic activity assets people from other chains so that's what we cover it first but what are some other cool things you can do with a protocol like hyper and again because you have generalized message passing you're able to affect any type of interchange activity you can go ahead and create interchange so you can imagine pools of assets living on each chain and the answer is being swapped without ever having to be bridged or wrapped hyperlink also supports the concept of interchange accounts imported over from the realm of IDC or I should say inspired from the IDC realm interchange accounts are really cool because they allow you to have a contract on launching be the ultimate owner and controller of contracts and other chains you could think of this really as like a smart wallet or we used to call them remote identity proxies but with interchange accounts something like Adele on ethereum can own all the sub-dials uh it could be like a parent of the different child Dallas and thus when there is a governance update it can be pushed into all those accounts and if individual users Smartwatch can be an interchange account and really there's no end to how you can use interchange accounts perhaps um these these last two really are my favorite uh use cases but with a system like hyperlink if you think of um what are some of the things that are immediately necessary when you launch a new chain well first you need to be able to bring in assets well the modular bridging you talked about solves that for you second you might need to have access to some data feeds you need to organize some data from somewhere else and so with a system like hyperlane you can create a stream of oracleized information from any major chain where you trust privileged even that information so you could import chain link fees you can import you know uni uh Unity Web feeds whatever type of information you need to work a lot so you can do that via system like hyperlink and last but not least my favorite use case interchange margins if you're building a derivatives protocol taking this into account what are your objectives your main objective is to have more healthy trading volume well that is a function of assets that you have collateralized the amount of Traders and their willingness to trade with the consequence of chain margin you can increase the amount of assets you have collateralized because now for the first time Traders don't actually need to bridge over into your chain rather they can leave the collateral at home deposit it into a contract of yours on its origin chain so say uh yeah that you can stay on ethereum have avax the avax can stay on the Avalanche atoms to stay on Cosmos Etc because as a derivative protocol what matters to you is simply the ability to liquidate and take control of that collateral once once it's been margin called and so you can do that with a system like curriculum so I'm very excited to see some derivative protocol coming up and building with hyperlane to create for the first time this concept of interchange margin what's the end game for hyperly what comes in on to these four things I think it's all about inter roll-up networking so being able to switch between not only between different individual growth so between different roll-up networks I think of those hubs that I talked about before each Hub existing as a sort of roll-up Network while we need to communicate between those as much as we do within each one of those and others relaying services today when people use hyperlines permissionless interoperability they are primarily running their own relays I'd like for that to remain the case indefinitely and certainly the entire controlling community of support of Realtors always remaining permissionless this is part of our great vision however being able to offer relay or services for those who do not want to relay their own transaction is quite important for that to be a seamless process is something that should be coming sometime in the near future additionally as hyper and matures we believe we'll see a security module Marketplace we're already seeing new operators of iOS come on um most interestingly right now we're seeing some zero knowledge proof operators beginning to build systems with hyperlink to create security modules that are secured by these by their zero knowledge logic Alexa hyperlink is a wonderful system to express and execute on interchange intents and so right now several things are being built to allow for that that's something that we are very excited about so that really was all I had to all the time I got and I'd love to tell you more about hyperlin if you want to learn more you can go through our docs at docs.hyperlane.xyz you should join our Discord it's just Discord GG slash hyperlink you should feel free to follow us on Twitter at hyperlane underscore XYZ and you can always reach out to me on telegram I feel Nimbus or my email at Johnny's so thank you everyone I had a wonderful time speaking to you today and again don't hesitate to reach uh my name is Neil and I'm the founder of eclipse so for this presentation I'm going to just go through one slide deck which basically explains some of the technical things that we think about at eclipse and how we're approaching them and then we're also going to get into what we think are ideal use cases for app specific persistent Roll-Ups such as what we're building at eclipse so I'm going to go ahead and pull up this deck here all right so uh this is a technical explainer so it assumes some knowledge of what Roll-Ups are and things like that so this gives a little bit of context on what were the scaling solutions that historically existed one being uh you have alt l1s like Solana you have uh what was previously Tara you have a bunch of near Aptos so these are all basically new layer one blockchains that do their own consensus and they also handle execution and other functions that a blockchain typically does uh and then you have in that same vein in the sense that they're trying to find a scalability solution uh that provides a general purpose chain for everyone you also have Roll-Ups or traditional Roll-Ups that's like Arbitron which is anchored directly to ethereum they have a canonical Bridge contract and then they do a bunch of execution execution off chain and eventually commit that result back to ethereum so these are shared chains and I group them together because you might think that it makes more sense to group The All Out ones with app chains which I'll get into next and then Roll-Ups would be its own category but in our opinion the way that these are grouped and the reason why we've grouped them this one is because you have share chain which is inherently non-customizable but it's highly convenient and on the other hand you have an app chain which is extremely customized but in order to spin up like a Cosmos app chain or a polka dot parachain you have to typically in the case of Cosmos you have to bootstrap security meaning that you have to get a validator set you have to put up stake and in the end of the day your fixed cost for running this chain ends up being seven figures or more as opposed to using something like Solana or one of these shirt chains where your fixed cost is basically zero you don't have to run any activity if you don't want to but now you have this marginal cost which is whenever a user runs a transaction they have to pay some amount in rent so you're basically paying rent to this shared chain or you're trying to roll at your own and be a homeowner so the question is given that app chains gives so much more customizability they give a lot of dedicated throughput why did they not take off and our answer is that it really comes down to convenience and it's really difficult to spin up an app chain as it exists right now the other part of it is that you end up with security fragmentation and we'll get into that so these are the properties of Roll-Ups that we really like and we want to borrow that so that's what we're offering with the clips uh in the same way that alt later offers these temporary Roll-Ups which are intended as burst capacity Eclipse offers a permanent roll up and this is intended to be app specific we support some chains that are not app specific such as what we're doing for injective or polygon but a lot of our chains probably the over like 90 of them are dedicated to a single app and they build in customizations that make sense for their application so this is just a thesis on what this will do to Value accrual once a bunch of apps are on their own layer threes and Layer Three is kind of a term that's a little bit fraught because typically like L2 implies that you're using ethereum for a DA and you have a bridge contract there which is not what an eclipse roll up looks like so instead we want to separate it a bit and we'd say its own execution layer so when you have an app that's on its own execution layer and it's free to switch a DNA layer then that basically forces the data availability layers into competition with each other whether it's ethereum or some other GA layer and that's good for the app because it means that the layer one doesn't have as much pricing power so historically the fat protocol thesis was basically that if you're an app on a layer one blockchain people will speculate on the layer 1 blockchain if your app succeeds and therefore the layer 1 blockchain will get a lot of The Upside even if you're an app that's very successful whereas when you're on your own chain you can capture a lot of that value yourself so that's one of the big values of being on your own chain in our view so these are some of the design trade-offs because what we want to do is offer customized Roll-Ups where you can pick a virtual machine you can pick a base layer and sometimes you can pick some other features such as a customized gas token or custom block times that are suited to your type of app so how can we provide that in a way so that we don't have to do everything in a one-off manner and we don't have to do something that doesn't scale so the first question is how do you offer multiple buy codes we want to offer evm and we want to offer svm in the future we want to offer move VM by code or possibly something else so there's a few different ways we could do that one is we take an existing roll-up framework and we basically Implement an entire virtual machine within it and that's actually something we might do with Sovereign Labs where we take the Sovereign SDK and add in uh I mean Solana has the virtual machine implemented as so on our BPF of course then you lose the that's more of like a by code interpreter rather than a highly parallelized virtual machine but my point is that you basically have to take this for every bytecode you take the role up SDK you implement the new bytecode and you get it working with the rest of your systems and hopefully the settlement for whatever roll-up framework you use is built generically so that you can support arbitrary VMS like that uh hopefully your bridging doesn't need to be rebuilt substantially but this ends up not really having a lot of there's no uh like waves that you can ride there's no one else who's going to be doing this work for you you're going to basically have to re-implement all this yourself similar and in that same vein as VK stink and what they did with the llbm intermediate representation where they basically compile everything down to this IR uh some so they did this with salinity with Yule with a bunch of other languages uh and that's nice because it's one General uh intermediate representation that you can build all of your infrastructure around but the downside is again you have to kind of do this all in like a one-off way you have to compile everything to this IR so what we did was we basically picked the Solana VM largely for this purpose which is that I mean one it's highly parallelized you get a lot of throughput but two it's already on the path to supporting multiple by codes and most of our chains are actually evm and we support that by using a project called neon on Solano Solana obviously supports its own by code which is sometimes called SBF Solana by code format and then we have other bycodes that are on the way like move VM so this is a huge advantage in the sense that we get local fee markets we get all the tooling it's built for the Solana VM it's a very powerful execution layer but the downside is it's really not built to be a roll-up so we have to do things like insert a smart Merkle tree we have to sometimes change some things about how the in the future we might have to change the mempool in order for it to work with shared sequencers so these are all things that we have to make uh like we have to roll our on our on our own all right so the next question is how do we do start online should we do a typical small contract roll-up which would be on ethereum and that's be that would be the smart contract that determines the canonical chain another option is you make it do Sovereign settlement so somehow you pass proofs directly to like lines and one big Advantage here is verifiability I mean the advantage of being on a smart contract on ethereum is that people say it's trust minimized in the sense that if you run an ethereum full node then you know that whatever state that ethereum flow node contains is a correct State for the roll-up as well but that only works because running a full note on ethereum is essentially running a Lifeline for every single roll-up on ethereum because that's smart contract Bridge is a light client so alternatively you can just write uh like a settlement that you can run a roll-up that directly passes those fall proofs to all my client that your community or your users run and they can directly verify the correctness of the rollout and then the last option is actually what we ended up going with which is we have everything uh deployed as a one event and Trust assumptions smart contract roll up onto a shared settlement layer so that's that's nice because you get that trust minimization with respect to the settlement layer where anyone can submit a flaw proof in the case of it being a ZK roll up then you can just validate the ZK proof within the cell the settlement layer but then the settlement layer itself is a sovereign proofs directly to like clients and in the future the settlement layer will be a ZK roll-up so that's the design that we ended up going with and it also gives us a lot more ways to build in convenience features for our Roll-Ups for example you can imagine a world where you don't want to move your liquidity between every single single Eclipse chain so instead you just leave all your money on the eclipse settlement layer and you have you use account abstraction or something to signs transactions for these different chains and then someone relays those transactions and they get compensated on the settlement layer or you could use a settlement layer as a sequencer set so that's that's the idea behind why we have the settlement layer you don't have to use it you could theoretically run your own Eclipse chain run the sequence of yourself but then you're lacking a lot of these Network effects all right the next question is how are we going to get um from an optimistic roll-up to a ZK roll up we built the fault proof ourselves in-house but we really didn't want to build a full ZK VM in-house because it's a lot of work and it's kind of an arms race in the sense that we'd have to continually update this virtual machine so we really didn't want to build our own in-house one so we looked at a bunch of existing ZK VMS or ZK proof systems as a service and that's things like Risk zero that's like Axiom nil Foundation ultimately we picked risk zero uh for the reason that it's highly portable risk 5 by code is pretty generic and we could probably switch to something else down the line if we really needed to but the risk zero team is also interested in working with us and we found through our cost benchmarking and it ends up costing about 10 cents to prove a non-trivial transaction and that's without any additional optimizations it's just using Straight Risk by code and what I mean by that is we take a Solana program and rather than compiling it to Berkeley packet filter by code which is the normal by code for Solana we compile it to risk five by code instead and we run that within the risk zero virtual machine so that's the high level implementation in the future let's say we wanted to cut down that cost even more we might natively implement the BPF circuit and we can still reuse risk Zero's proof system so this is just summarizing when I just went through uh this is on decentralizing the sequencer this is enough of an open question that I won't go into and I want to start getting into the applications that uh that we're able to support so I'm going to just gloss over this uh this is stuff on wallets bridging and uh this is the application part so this is a case study on worlds uh worlds is a game that's building on eclipse and they basically use the blockchain as an open API so they write a bunch of read-only data onto the blockchain uh these guys build uh Unreal Engine 5 previously so they're really hardcore game developers but they see the purpose of the blockchain as a highly composable API where players in the community can build things like financialization on top they can build their own Primitives they can build their own user generated content but you also have this like base read-only on primitive which people can use and compose on top of so that's what they're doing and if you think about it they're already conceding that to some degree this is centralized because the company themselves is acting as the Oracle they're signing off on the data that's being posted to the chain so they don't really care about the base layer of being decentralized right here I have Celestia written as the da layer but actually in practice right now we just have them on a centralized because it's a very high volume of transactions and they're very low value on average so that doesn't make a lot of sense for them to be posting and paying money to an actual decentralized da layer at least in these early stages so that's what the base layer looks like the uh the sequencer itself is centralized because they don't really need to have us a sophisticated decentralized sequencer setup and this is a very consumer oriented chain so they even take usdc for gas so that's the setup of a consumer chain on eclipse this is what we're doing for worlds we're doing this for a handful of other projects like Gridiron boulevards ether games we have social future on the social side so it's basically like a highly centralized chain that supports extremely high throughput but it can compose with a bunch of decentralized chains on the side you can build your own Primitives on top of it as a community and that's that's how they use the blockchain and this is just a way that we think about different types of applications this gaming template is more like a general consumer template at this point and as you can see the it's running the svm but it's actually running evm by code within the svm this is not actually what our D5 template looks like but we're still hammering that out with some of our defined design Partners we do a lot of svm Roll-Ups for layer one blockchains so this we're still thinking about how we're going to incorporate into our self-service offering but basically you spin up a chain that has a custom data availability layer so a post blocks somewhere and then Eclipse needs to also know how to read those blocks as well so this might require in in the future maybe teams would make a PR against the Eclipse code base they'd show how to post and read from some new DA layer and then they can spin up an eclipse chain that does that and then we do a bunch of dpin projects So decentralized Physical infrastructure networks so that's like a react wave wind iotex these guys are taking real world things and they're doing some work off chain and they're able to prove that it happened on chain and then they can incentivize people to participate in the network via some tokens such as what helium did they can take that data they can optimize it so that they can save you money in the real world they can sell the data there's a bunch of different business models that you can do once you have that all on on Channel so this is just a bunch of other um more Fringe use cases and that's pretty much the stack so uh that's uh the first presentation that I wanted to get through and I think we have a little bit of extra time so I'm going to go through some more detailed case studies so I'll go ahead and pull up this case study which is just a deeper dive into Worlds and what I want to drive home here is that for a consumer chain like a game that's posted if it's fully on chain posting something on the order of hundreds of thousands or millions of transactions on chain then even if they're paying something like 10 cents a transaction that's probably prohibitively expensive for them and why would it why not even say 10 cents it's because if you look at the ethereum da cost uh which is a fee Market it ends up being about 10 cents to post 200 bytes to ethereum and then maybe you'd want to like charge a little bit of a markup on that because you have a fixed cost associated with running the sequencer for your roll up so maybe you charge like an additional two or three cents per transaction so you'd end up paying something like 15 cents per transaction so you'd have to ask well their users are really comfortable paying 15 cents per transaction and whether it's worth it for them to post millions of transactions on chain given that the value of each transactions pretty low so given that structure and given the economics of it it doesn't really make sense and we'll see what the fee Market on celestion job ends up looking like but in practice this kind of change should use some sort of centralized DNA and in the future once this is a ZK roll up it's probably not that big of a deal either it's essentially a validium construction and by centralized you could do a DAC like a data availability committee you could make it quite affordable to have data that theoretically could be withheld but at least you get guaranteed correctness by virtue of the validity proof so that's um what we have as far as how we've been thinking about gaming and why our setup makes a lot of sense for games we let people take the da solution so if you do have higher value transactions and of course you could use a decentralized da it's evm because they wanted that portability they don't mind a centralized sequencer and then they want to use something like USBC for gas so they've demoed their game in a couple of our game nights we had one over in Denver we had one over in San Francisco at GDC and if you're building a game then we'd love to chat with you because we can likely arrange some similar setups with you okay so that's world's and then let me go ahead and pull one up for injective since this is another one that we're pretty public about so injective's chain is called Cascade and this is a Solana virtual machine chain that injective runs so they're a layer one blockchain injective supports uh web assembly programs so it's rust as well but let's say a Solana program wanted to deploy to the injective ecosystem they'd basically have to rewrite a lot of their logic so by having an svm chain injective can basically promote these projects to or they can encourage them to deploy to this svm roll up they can easily deploy and they don't have to rewrite any smart contract logic and then that chain charges injectives native token for gas so that's like the high level on how the value occurs back to injective of course they also have an IBC connection back to injective we're working around some uh some other stuff they want to do with the sequencer whether it's by adjusting the block time they might want to use a shared sequencer such as espresso but this is the high level setup but these Roll-Ups for l1s typically look like it's bringing them a new execution layer uh and it's charging gas in their native token so soon we're going to be actually deploying an evm chain for injective as well because they want ebn compatibility so this setup is pretty General so um there's probably not as much to cover on this one we gave a couple of options for them uh initially because this was one of the earlier l1's that we ended up working with since then we've engaged with BNB chain polygon near uh we are spinning something up for Nibiru there's a few other Cosmos chains which uh we're still in the works with and we haven't quite uh quite finalized what that would look like but we've basically done this a handful of times so that's what um that's what we've been doing been doing Vlogs for all Ones been doing games and consumer and more recently we've been entering D5 that's the high level on what Eclipse has been doing and feel free to reach out if you have any questions hello hello my name is Evan Forbes I'm with Celestia which is the first blockchain Network and on Celestia you can build whatever so if you're interested in anything that I talk about in this talk or in my work in general you can find me on Twitter I'm Evan s Forbes on Twitter or on GitHub I'm just Evan Dash Forbes and today I'm going to talk about trust minimized and impact maximized rollups as a service and the thing I might get a little a little opinionated here a little political maybe even but uh I hope that that adds to the spiciness and like maybe just the general entertainment that we can get from this talk and the entire thesis is with this trust minimized portion is that sovereignty not just Sovereign relives but Sovereign blockchains in general are actually the difference between what has come before us with like Banks governments web 2 and crypto I know that's a little spicy but I I do think I do think that there is some truths there and there's some value there and I think if we critically think about it we can design Roll-Ups as a service we can design these services that help build these Roll-Ups and do so in a trust minimized way we can preserve that property and perhaps unlock something really truly special and get to some insane crazy properties that I don't really think have ever existed before just that's really exciting so anyway the first start answering that question is we can get to what actually makes blockchains compelling what actually makes blockchain special not necessarily the protocols built on top of the blockchains but the blockchains themselves why is Bitcoin special why is ethereum special and I think it comes down to sovereignty where the ability to verify right don't trust verify the ability to verify these chains I can verify ethereum on a laptop on bitcoin it's probably something even significantly smaller I can choose the rules that I follow I'm not trusting another committee right we're building a network from the ground up and we have this like sort of crypto anarchic promise and this this like property that is to build a network to build a structure that is resistant against abuses of power and hierarchical structures that's crazy that's a really special property and right when the title of this talk impact maximized we don't want to just do the same things that we did before we don't just want to have the banks but faster and a little bit more decentralized and a little bit more permissionless that's good having these permissionless decentralized systems is really good but if we're still trusting committees if we're not actually verifying then we're not the ones who are choosing the rules right we choose the rules that there's only 21 million Bitcoin we choose the rules and we enforce them for Dogecoin and for ethereum and for all of these other blockchains we enforce the rules and I think that's what makes things special so when we're designing rollups as a service how can we even capture that how can we amplify that and um so to get to get to more of like this sovereignty portion I think Mustafa al-basam he's the CEO of celestial Labs co-founder of uh yeah co-founder of Celestia labs she's done a lot of work uh there's a paper published data availability and fraud proofs of course lazy Ledger which is now Celestia and you frame this I think summarized it perfectly Sovereign blockchains not just Roll-Ups not just modular blockchains Sovereign blockchains allow people to create social contracts and enforce them directly without going through any intermediary top level social contracts right this is a crazy thing this hasn't existed before this is the new thing of blockchains this is the thing that makes them different from what came before we should amplify this to to Really create compelling services for roll-ups and being able to verify the chain choosing the rules ourselves sovereignty being able to hard Fork this puts people over tokens people over validators people over governance right this property is really something truly special it's not just token holders who get to make a decision it's node operators people who are verifying the chain and to better answer this question we can even ask like why do banks and governments suck why does why does BlackRock suck that BlackRock owns everything and this this funny cartoon of this Futurama representation of Richard Nixon who is known to abuse Powers right why why do blockchains suck or why sorry not block things why do governments and Banks suck and it's mainly because we can't really pick the rules and these Banks and these governments they have monopolies they get to decide what actually occurs now in theory in theory it's like a democracy and people get to decide right but in actuality things start to get messy really quick I mean just like go look outside a lot of things a lot of the reasons why a lot um people are attracted to the space is because they are so easily disillusioned with how the current system works right I don't really need to explain it Satoshi then it when when they made Bitcoin their whole idea was to make a currency that was controlled by people right this crypto anarchic property and and their social contract is 21 million it's only ever going to be 21 million right in the world right the FED has this infinite amount of money can I pause this now oh no the FED has this infinite amount of money right they can do whatever they want they can they can enrich themselves in theory if they want so they can they can design whatever type of society they want and we don't really have a say so then they just this is this whole meme of like this money printer and everything that's what Satoshi was going against at least I think and I think it just reset the talk uh no I think I just had duplicate slides sorry about that okay so what actually makes blockchains compelling that Meme I think summarizes it up sort of is that is that we can go against that we can design our own money if we want to print money though we can that's the crazy thing it's like the rules are arbitrary they're programmatic we get to pick them by verifying the chain so model this is where modular blockchains come in this is where roll ups come in is Roll-Ups make it orders of magnitude easier to verify the chain orders of magnitude this is what d a and roll up solve and they do this in a trust minimized way and right this this enables two things this enables sovereignty which is fundamentally derived from verifying the chain right sovereignty the ability to hard Fork we pick rules node operators and in this case with modular blank with modular blockchains like clients pick the rules right I can run a light clan on my phone I can verify the chain I can pick the rules and the result of those rules right not just the rules the result of those rules from my phone and people all over the world can do that that's the crazy thing that's sovereignty and security right normally when we think about proof of stake blockchains we think about Committees of validators well guess what getting Committees of validators to operate in an honest way is very difficult because you have to stick enough capital in there in the blockchain and lock them up and have it socially slashable it's still secured by social consensus but it's socially slashable funds those funds have to be larger than the funds that are in the blockchain kind of because otherwise if it's more profitable to attack the blockchain it's like oh that might actually happen so that's why that's why it's like this is a really difficult property to get committees to actually behave honestly that's why we use proofs instead with modular blockchains we use proofs committees are expensive proofs are cheap they're also Sovereign and they're also extremely secure right validators are cucks the security of the chain comes from proofs that's the beauty of like modular blockchains and Roll-Ups as a service right Roll-Ups and modular blockchains of this powerful thing but they're still missing a lot of pieces that's where this Roll-Ups as a service comes in is this sovereignty the security without compromises we can because the systems and how they work now there's still like a lot of gaps like how do I have decentralized sequencing for example like there's there's just a lot of gaps still and we can develop these services that still preserve the fundamental things that make rollups so special and when we do this we always have to think about things from proving from a like client perspective right all of this comes from the ability to verify the chain in theory from your phone using some sort of like client just a light computer really but anyway the point being to be able to verify the chain okay that means that everything that we think about everything that we design has to be in those terms how can I verify this using a like client honest majority assumptions are icky like clients if they have to make an honest majority assumption aren't lose their security they lose their sovereignty because now it's not trust don't trust verify now it's don't verify Trust it's backwards that's what happens when we have when we have to make when light plans when light clients have to make an honest majority assumption we don't want that we want validators to be cucks okay trust minimize da this is harder than it sounds I work at Celestia I'm a I'm one of the core developers at Celestia so I I have some a little bit of experience in building some of these systems and Trust minimized da is harder than it sounds for one we have to have d a sampling okay that's cool uh I think I think most of us have a general idea of how that works and then you have to have enough Samplers so you have to have enough Network effects you have to be convinced that there are enough other like client sampling that you can reconstruct the block only using like clients if need be okay then you have to be convinced that the error sharing coding is done correctly okay so if we use fraud proofs for that at Celestia but other chains you can use kdsg commitments you can just prove it in a ZK way and it's not that to be clear Celestia could use those other mechanisms as well right now fraud proofs they're very simple they're very easy they're extremely easy to audit they use normal cryptography that we've been using for 20 years and they're extremely scalable we can have very very large blocks unlike those other mechanisms unlike the other ZK mechanisms but not to get too distracted with that there's also this thing that I don't I'm pretty sure no other blockchain has which is cheap inclusion checks so it's great that okay okay like like as a like client I I sample the block that's great I'm convinced it's available I'm convinced the error encoding was done correctly I'm convinced that there's enough like client sampling that we can reconstruct the block if we can need to but what happens if now I still have to be convinced that the data that I care about the roll-up header it has it has a hash it has a commitment over some block data how do I know that that block data is actually included in the Da Block that's kind of an important thing right otherwise none of that other stuff matters none of it matters it only matters if the data that I care about has been included in the block how do you prove that well typically that's done using a Merkel inclusion proof okay well if we use Markle and conclusion proofs what's the problem there that requires you to download the entire block like the entire point of a light client is that it doesn't have to do that so you have to have significantly cheaper ways and we actually do that in Celestia so in in Celestia we instead instead of having to prove the and prove inclusion to the entire block we prove inclusion to adjust the transaction that pays for that block so apologies I didn't really have a lot of time to put together these slides or a lot of visual aids but you can see this is sort of how we organize the original data Square in Celestia at the top bit we have this transaction portion that's where all these transactions go it's very small I can prove just a a just a single transaction was included and that is good enough to convince me that the blob the message the roll-up block for that transaction was also included in the same clock and we do that by getting a little bit clever there's more information on this in our specs in the data Square layout I think like the rationale for this of how we actually use but effectively that that commitment in the rollup header we change the way that we create that instead of just using a normal Merkel a normal Merkel grouped we use a Merkle root over subtree roots in Celestia because now if you can find those sub-tributes and you prove inclusion to those subtree roots that's just the same thing you're you're because as a role of light client you don't care what the actual data looks like you only care that the commitment that is in the roll-up header has been included in Celestia and all and because we have this we have a special type of fraud proof that proves that some given transaction was included that pays for a block versus the actual blob in the block we prove that if one there if one is there the other also has to be there I won't go too much into that into how that actually that fraud group actually works again hopefully by the time this video is published our specs have been updated and all that information is there you can find that if you just go to the Celestia GitHub go to Celestia app then go to Specs sorry again I kind of had to throw these slides together rather quickly and and so we have trust minimize d a another thing that we need is that for Roll-Ups to be this this magical dream of of rollups as a service where I can just in one click deploy my own solver and roll up is I also probably want it to be decentralized I want it to be censorship resistant and shared sequences are a way to do that they're a way to scale that property because if we think about it like if we're if we have like the normal sequencers we have to have a committee create the blocks if we want it to be censorship resistant or we can have backup mechanisms too where I can force inclusion using the da layer that's a very good option as well but if we want like the aliveness properties we always want our rollup to be alive then we also have to have some sort of committee so but the idea behind shared sequences is what if we can share that committee right so I've been thinking about how do you decentralize this how do you decentralize the liveness properties the sequencing of Roll-Ups since about last summer went through a few iterations spoke with a lot of folks at Celestia and eventually on winter break I finally had time to write it up and um I wrote up this forum post it's called shared sequence sharing a sequencer set by separating execution and aggregation if you're more if you're super interested in that thing I would highly recommend um well I guess that's 2D in my own heart just just go it's it's like it's a kind of hopefully okay description of this thing it's not formally defined espresso systems has also since published a I think more thorough explanation of these things so you can also go and find their documentation as well but again the the idea of this is rather simple it's like we have the sequencing we do that all combined and then we actually do the execution later so in my Forum post I referred to this lazy execution right we we agree upon the transactions first which transactions have to be included no not their order just the transactions that have to be included and then we execute them later and then we actually do the proving so this changes the proving a little bit and it's uh I haven't again we're shipping mainnet we've been really busy so I haven't really had a chance to write up how you could this actually changes proving but there's other people at Celestia well notably Gabriel who has been helping me like sort of ideate around what can you actually do now that with like this lazy proving right so to get a to get like a slightly better view of what a shared sequencer can do the users they submit their transactions for bundles of transactions directly to the shared sequencer layer the shared sequencer layer can produce blocks as fast as they want they don't have to wait on the DA they don't have to wait like that 15 seconds or so for the da they can get soft can the soft finality so it's obviously not as good as firm finality but it's still it's pretty good it's sort of like this gradient of trust like whenever the the shared sequencer Network gives you the soft confirmation you sort of just read it as a soft confirmation and you can you can it's enough to at least update the ux very quickly so you can you can you can have a soft very fast confirmations I don't really have to deploy my own sequencer set this is incredibly easy like I just Define a state machine and like Define a chain ID my users just start using that chain and then and then this is an important bit the shared sequencer part the shared sequencer network will post the entire block data to the DNA layer and this is sort of how they this is the key factor of how they actually maintain trust organization everything works the same as if a singular rollup was posting their block data to the DA layer everything's the same there because because the shared sequencer is posting all of the block data at once to the DA layer they're the ones paying for it how you actually pay for gas for that can be kind of arbitrary ideally it's done in the native da token but I think I'm getting distracted so to elaborate more on this lazy execution again you you can you users can submit transactions they get these fast soft confirmations they don't finalize the state yet but it's usually good enough to update a ux at least get that green check mark get that dopamine hit and then and then we actually post the da layer and then as soon as as soon as the da layer has confirmation as soon as we have that like client verification that I was talking about earlier where the data was actually included excuse me on Celestia or on the da layer now now we have firm finality which is kind of cool another really cool thing about shared sequencers is they have these validity rules that unlock powerful cross-chain moves so this thing is like I've kind of always thought about shared sequencers as this way for Mev searchers to use as tools to sort of build blocks and and more or less a centralized way now I'll touch on that more later that full picture is not painted yet there's still like a lot of work to be done there but I do actually think that it's possible so stay tuned for that but essentially right now all you really have is proofs of inclusion so you do not have Atomic execution you only have Atomic proofs of inclusion meaning I'm only guaranteed that if I have a transaction included on roll-up a I also get a transaction included not executed included on Roll lucky that's it it's only proof of inclusion but you can get more creative with this this is by block validity rules I can say on the shared sequence Center layer I can Define as a block validity rule that these specific transactions only get included if they're included at both the top of the block or one is at the top of a block right and that's sort of like a really powerful again these these block validity rules that you can include with shared sequencers help they're tools for building out more and more decentralized block building at least That's My Hope but anyway that's shared sequencers and from like a roll-up as a service mechanism I think that they're very very powerful because they sort of give us this opportunity to think about like the services that we provide Roll-Ups can pretty much do everything on their own and we want them to do that we want role-up developers to be able to encode their own rules however we can start to add committee Based Services to the shared sequencer so things such as committee based IBC you could have a settlement layer committee based accuracy you can have threshold encryption so fair block just posted a forum post on Celestia that was really interesting and they they effectively do the same thing right you you have a committee that uses threshold encryption that is also a shared sequencer to provide these trust minimized services to a roll-up and oracles you can have socially slashable oracles hopefully I'll have time to talk about that later but maybe not and then what we're going to talk about next is censorship resistance per block so I kind of hinted at this before with shared sequencers but with Mev is is effectively like you end up with one block Builder so you have a shared sequencer that's great and you have um you and that shared sequencer provides all the inclusion guarantees for all all these different Roll-Ups but the problem is the number of Roll-Ups that can go on a shared sequencer is technically unbounded and that means that like a block builder for that shared secret sir it's going to have to have Block Run Roll Up full notes for all of those full nodes unbounded is oh that's this is it's a bad thing it's not good and effectively like that's a very centralizing force of having to run all of these full nodes and like having one giant block Builder it's not very decentralized it's not very good so what are some things that we can do around that so again we're just designing these services for Roll-Ups what can we actually do so again like the main one of the underlying issues here is that these I have a politician here but it's you can think of a politician and a validator basically like the same thing they're they're analogous at the very least and these politicians and these validators they have monopolies over specific things in the case of a validator you when it's your turn to create a block for most consensus protocols you most consensus protocols in use today that is you get to pick everything that occurs in that block and you can abuse That Power by taking a bribe to include other things right we as delegators delegate to you a validator and then now that power that we gave you we gave you you can abuse by enriching Yourself by like taking these bribes to sort of order transactions in a favorable way or censor different roll-up blocks and that's incredibly problematic when with Roll-Ups we just want to be able to Define these simple Fork Choice rules but before I get into four Choice rules we'll go to the next slide which again this is showing the point with these modern systems these modern blockchain systems a lot of the times so far the validators the politicians they're the ones who are getting rich they're the ones who are taking all of this money and then they users of the chain that's a down slope right and the network the network isn't making money in most of these cases for ethereum with the eip1559 arguably you're getting some increased burn rate which in which in the which the in that case the network is going up quite a bit but anyway without having some without breaking this proposer Monopoly right without breaking this proposal Monopoly we're kind of stuck in this boat we're stuck in not being able to Define where this the profits of a system get captured so thankfully earlier this year um Duality the folks at Duality inclusion including Elijah who wrote this paper and this uh this is a screenshot of a forum post on Celestia describing exactly how you actually could do this and this is the key thing for and for not solving Mev but being able to programmatically Define where the profits of an entire system go and they do that by breaking the proposal Monopoly so I'm I'm quoting Elijah here I thought this meme was absolutely hilarious and so perfect because it's like people protesting and the proposal Monopoly and it's true it's really true so I'm running out of time here I'm going to have to rush through some things four Choice rules Roll-Ups can Define their own four Choice rules if you can Define your own four Choice rule that means that you can Define things really simply and elegantly like I want my roll up to the the block that wins is the one that brings the most values to LPS or burns the most of my rollup token I can define those simply if if and only if the proposer does not have a monopoly because if the proposer has a monopoly they're just going to be able to pick the correct block that gets in the block that they want the block that they get bribed to most we don't want that that ruins the property that we want right we want to programmatically capture the entire profits of a system I think I only have 30 seconds left so I'll touch that we have decentralized block buildings stay tuned for that I think there's some exciting things lots of work none of it set in stone this is just me writing things I I do think what shared sequences we have we have a potential like awesome thing and with oracle's um there's some things with ABC I plus plus you can have trust minimize or you can have socially slashable oracles which is really cool and that's my talk trust minimized and impact maximize develops as a service when we build these systems try to consider these things validators are sovereignty is the difference between crypto and everything else that came before it let's build cool foreign I hope you enjoyed the talks in case you missed any the entire video will be available on YouTube and we'll also be segmenting it so that you can jump directly to the talk that you may have missed me now we are towards the end of this virtual event uh this segment will be the last bit on our agenda today and therefore we'll keep this segment as a q a session have you have received several questions from the committee beforehand we'll start with those but please post your questions in the YouTube chat box and we'll try to pick as many as we can but before we start I would like to ask yaochi to give us a quick demo of our roll-up launch pad we have been building this for quite a while now and you know we did a couple of campaigns around this as well um and so let's go through that first and then we'll open uh the the floor for broader q a now see over to you um yeah so so I believe like most of the audience all right no we have this uh Yolanda service dashboard or platform right so easily you can just log into this dashboard dot dot technology and once you enter this page with a single sound typically your Google account you can see roughly the same uh like sort of structure and setup as I'm showing to you so here I will just show you how to quickly set up a roll up or Flash layer in our context um so there are two ways actually the fastest way is to use the flash gbt so we integrate uh like GPT with our dashboard so in that case as you can see right you can just describe what kind of flash layer you want and then within a few seconds I will generate the configuration file for you and and since you see since it's uh it's chatbot right so you can just tap what can what kind of can configuration parameter you want to change example we want to change the name to uh raw state and token simple to rust so after that you directly change the corresponding parameters and uh yeah after that so this is uh so you can change the token symbol again to last two so after that right you can you can basically later uh deploy The Flash layer where the generated configuration file um or we can just use the default one here you just click the deploy The Flash layer it will just deploy within two minutes uh and there is an alternative way if you are a developer right you want to choose the parameters sort of with uh with some Precision right so you can go to this second tab called create flash layer and then by default there are two modes for you one is first conference so the second one is the typical gas mechanism and then you can just input your flash layer name for example Rusty again and then you can configure the gas limit and also the block time um after that you can also Define your token simple and also the the currency name um after that you define the Geniuses account you uh you want it and by default we have this uh block uh block Explorer and also token foresight uh for your loan up or we call flash layer after that you can just click create typical is like take a two minutes to generate the thought of The Flash layer as you can see right it's uh already generated uh one and basically you can look at the blocking scorer um all the blogs are generated and also you can go to the Token Force app and the claim the tokens you you sort of have um beyond that right as as we can see we are also IDE so you can recall your contract to the flash layer or the roll up you just deployed and by default we already have some token uh contract template you can you can basically go to the first tab there's a contract generator and you can just choose what kind of eic20 erc721 to a contract you want to use and beyond that if you launch multiple roll ups right uh you can manage your roll up where this tab called manage Tab and then uh you can decide what kind of uh operations you want to apply on these flash layers or your labs um and as I've already mentioned at the beginning of this conference right we have a very special feature called flash layer settlement so that means when you do anti-mint or some events right so you can deploy a contract on the L2 but once the event is done you can leverage this type we call Roland Tab and then select the contract on the ro2 and then in one shot the product automatically roll up all the entities and also assets within this contract back to the selected one yeah this is roughly the sort of the um Services platform can provide for you and later as I just mentioned at the beginning right we'll provide more services and for example The Divided here with the persistent roll up with short proof and wanting sequencer and meanwhile we are also integrating the original orbit so later on you can have more choices to launch for your dedicated your lab yeah well thank you yochi um let's open so by the way a gentle reminder if you have any questions uh that you want us to answer you can post them on the YouTube chat box and uh you know we are monitoring that chat box and you know we pick questions from that directly as well um in the meantime you know we have uh received questions uh you know I think from Twitter and I would like to take one question and I'll pass it to yauji first and I'll take another one myself so um let's start with this question so this is maybe okay maybe I will start with this one and then I'll pass the second one to yaochie so uh the question is can you explain the advantages of your technology in comparison to other competitors out there in the crypto space right this is something that I addressed in one of my in my first talk so basically there are different types of providers out there so um in particular rasp providers that are you know that are building similar Solutions and by the way when we started building this I don't think there was anyone or even the term roll-up as a service wasn't quite there uh you know used in the in the space uh but yeah since then you know they are there have been couple of at least four or five uh similar vendors or people building similar Solutions so here is the comparison so the first thing is uh we are uh the only ones that has fraud proof enabled at the moment are using our own Tech stack so this is currently on the test that uh but we'll hopefully go main it uh very soon after that once we have we're happy with the results from the Tesla the second thing is uh that we support decently sequencing from the get-go so uh again people are and we you heard talks about decent class sequencing from several uh speakers today uh but decentral sequencing is uh still not enabled in in any platform as far as We Know and uh in fact uh the Roll-Ups that you launched from uh you know from through through dashboard could have multiple SQL so if you wanted to we already have a multiple sequence of tests not already live that you transact with already and there's other thing that is which is uh mostly around uh different kind of like a wasm based because you know that stack is based in bosom it kind of gives you a multi-chain multi-vm support from the kit go right it allows people to kind of have different choices in terms of evm that they can have and finally uh which is we are the only ones that provide this unique type of execution layer that we call flash layer so this disposal disposable layer where you spin up the flashlight you use it for as long as you want I mean you feel like you're done with it you don't need that anymore you can take the entire network down all the infrastructure that was supporting it uh will be taken down as well so it's very resource optimized and we're the only ones to offer this uh let me pause the second question to yaochi this is more of a experience question which is uh someone asks uh could you share some unique challenges that you faced while developing roll ups and you know modular blockchains are all fair and how you overcome overcame them foreign we are providing a lot of unique uh and brand new features for this base uh for example this decentralized sequencing multi-sequencers and also um it's more TVM apart from evm support we also support like WhatsApp and nature for different programming language they can also compare them to WhatsApp for execution and beyond that regarding is the Disposable execution layer with a proper proof system and all these stuff I would say are quite new uh to the interior like sort of blockchain space and today I also share about this um install up interlayer the beacon layer right and I believe this is also the first time for us to explain what's a big layer and how it can really benefit the interior roll-up space so to implement and actually to design these features in a comprehensive way it takes a lot of time and a lot of efforts from the inter team to really make it possible so the implementation side is already very tough and uh beyond that um for the other challenges is really about the automation um compared to some other public blockchains or a lot of these general purpose l1s and L2 is right so you then need to build your Tech stack and you almost generally to run the infrastructure for your chain itself but for us we're building this roll up as a service platform which is quite similar to address in the web tool space right so in that case we need to handle tons of thousands Road Labs uh give you one example like for the current flash layer uh spin-off right we already got over 12 000 free tier spin-off via the dashboard so in that case we really need to build a full automatic infrastructure to really launch this relapse many these real life and and also coordinate these rollers so that's also very challenging compared to for most of the general purpose and once L2 design it to manage their own chain for us like we need a completely automatic system to handle all these automatically instead of like doing it in many ways um and beyond that I write as as I just mentioned because in the long run we want to decentralize the interior infrastructure so it's not like uh a pure AWS in web3 we want to have a decentralized this kind of uh your lab as a service platform so that's why we come up with Beacon layer try to manage like for example sequencing verification staking slashing governance and upgrading right so we only with picking layer to basically help us to coordinate all the rulers and customers so this has been a roll up where the beginner and in the end all the sort of the sequences photographers are from the community in that way we can make a decentralized and at the same time everything is automatic so that that's why it's it's not a single challenge like for example just to achieve uh consensus protocol or some algorithm it's like a systematic uh challenge uh in in general so from the uh the design implementation for particular features and then uh for the instructor infrastructure design the building and then to the decentralization of the internal infrastructure as you can see we need to make a lot of these steps one by one to really make it work in the long run so in the future you can see a very decentralized rollerback service platform um with the sequencers of refers from the community and powered by earlier okay let's take a distinction and I will answer this this is a question that came uh beforehand I think by Twitter and I also saw a question it's very similar question on YouTube as well which is basically the question around um can we expand Beyond ethereum or one is can be expand to other evm chains Beyond ethereum and then the second question of follow-up question is can we expand Beyond evm chains so can we go to non-evm chains the answer is yes uh so the first part uh about the evm part we already support EDM change so it's not just ethereum for example we did an event a paid mint event where you could actually go and Bridge assets from BNB chain polygon and ethereum right so non-empts are already supported uh and you know almost any of them can be easily supported um because all you need is a basic Bridge for non-e-beam chains the answer is yes it should also be possible and the reason is that we have this stack that is built-in wasn't that allows us to basically support any chain and I think Ed Felton was talking about something like this as well during his talk that you know because you have a awesome piece anything that compiles down towards and can be supported so the answer to both questions is yes the evm part is already supported so right now if you want to have a flashlight you can actually connect it to any chain you want any evm chain you want non-em changes it's in the pipeline but first first we want to finish the ebm components completely um let's take another question that I saw on um on I think YouTube somewhere which is there were talks around um eigenlay there were talks around with espresso will it be possible to integrate espresso and um let's say eigen in roll up a service you know offering given that orclay already has some of the big bits already built now see you want to take that yeah so for eigen layer as uh mentioned um in this morning's talk right so um the identity is basically uh quite uh balanced at the end so uh it's sort of like it balances the cost and also the performance um so in that case size rise platform um so later on for the new uh version we'll have multiple options for the developers to choose regarding da so either you can use your own notes as the option DNA system or you can choose probably uh like eigen layer or events last year as your DNA system which are quite professional uh in building uh DNA Network or if as a person right for example as a different person you really feel that your data to be should be posted to ethereum to make sure like all the data available actually this uh ethereum security level then we also provide the option that you can post your data to ethereum engine like it's quite costly um and beyond that uh it's also open to various like evm chains and and also some other story decentralized storage projects um regarding uh also espresso actually obviously we had a bunch of conversations with Ben and uh with you that is ISO is realized it is a pre-execution DA so it helps you to um to basically uh Fetch and aggregate the transactions on the espresso platform and after that it gives you ordered like list of transactions for your your lab to execute the next is also a non-intrusive um like platform for us to integrate to um actually for both of this project right we already have some team members to look at their code base and documentation to look at how we can plug in again mirror um use the nice at the beginning and also for the expression we can look at how to leverage uh espresso Network as this pre-execution um DNA to order this transactions into some of our your Labs if the Jewelers really choose expresso as as sort of the shear sequencing layer at the beginning um okay let's take another question which is um around orbital orbit so the question is explain the integration with arbitrum orbit and the text tag what is its relevance and why facilitate L3 how it facilitates itself rehab development so let me take that question for a little bit of context and I think um so those who listen to the talk from Ed Felton he gave a talk on arbitrary in general but also had quite a few bits and pieces on arbitrum orbit so arbitrum orbit is basically a tech stack that allows people to deploy application specific instances of arbitrum's code bases actually perhaps text tax session and again for context arbitrum is today the largest uh layer 2 or the largest role provided today in the market it has a tvl about 5.8 billion uh in dvl it has a market share about 60 percent so arbitrum one is one of the biggest providers in the market for roll-up or biggest roll-up uh today and so it is it's really interesting for us to kind of help our people who are building an arbitration to give them a choice uh to build uh the application specific infrastructure that uses the same stack the same Tech stack that Arboretum offers as a part of arbitrary one for example so it's it's it's it's it's it's really relevant because there are people uh and you know Ed Felton mentioned that as well there are people who really want customizability they're people who want um their stack to be much more scalable and that you can do that only if you have your own uh application specific infrastructure and that's why it's important for us to support orbit market in terms of integration it will look very similar to what yaochi demonstrated at the start of the Q a we basically will have the same kind of dashboard you know the same experience it will be able to just pick and say okay I want a flash layer or I want an arbitrum orbit what I want on a flashlight connected to orbital one you would have all these different choices and you can pick uh the right kind of Stack that makes sense uh most sense for your business and an application use case hope that answers your question around um arbitrary orbit and why it's important for us and obviously it will help anyone wanting to build on arbitrome orbit and that will basically we will provide the entire let's say managed service that would run behind orbital uh let's take a follow-up question and this is something around beaconlay I think there was uh I mean you actually gave a great talk on vehicle and some of all the benefits that vehicle air provides you but I think it's good to summarize and this is a question that kind of pertains to that as well which is why do we refer bikolay as an interlay like what's the significance of interlay and why this interior is important yeah yeah so um why we need Beacon layer so um from the system uh evolving Prosperity right so um as we can see so we can divide this rolab category into multiple stages uh the first thing that is we need uh basically a single sequencer and also the proof either these flow proof or validity proof and this proof of Journey to verify the execution of the transactions where these evm right so in that case the proof the journey to verify the logic for the evm and also the evm instructions and then in the stage two um since there are a lot of requirements from the community or projects the one is multi-sequencer to make it relatively decentralized to avoid single point failure and to mitigate LED or some other issues right in that case we need to have multiple sequencer and once we have multiple sequencer we need some sort of agreement protocol within this sequencer just to make sure like they agree on the same block of transaction to the executed and the generator so in that case like for the proving system we also need to take the agreement protocol into consideration uh Beyond its execution of the instructions and so then we need some extra implementation and Logic for the proof system to support this uh extra agreement protocol among other sequencers and on top of that so since we have multiple sequences we know who behaving wrongly and who are behaving in a good manner they are the proof system right so we need staking and sliding basic panel in maganda to basically punish some of the sequencers who behave maliciously and that's sort of the stage two I see three is sort of like where where we have why we are proposing this Beacon layer because uh like right now we have probably five major relapse but granularly when more games and social applications different applications who are capturing a lot of users they will definitely spin off their own Labs or app genes so in that case we can foresee tons of thousands of relaxed in the world and in that case we really need a sort of decentralized layer to manage these Rolex that's something we we are building on earlier right the first step that we provide is local platform for developer to quickly spin this uh spin off the relapse within two minutes but after that we want to make it decentralized so that's why we need this instrument roll-up interlayer between the Rolex and also the interior so once we have this decentralized layer right it can help us to handle a bunch of things first one is the decentralized sequencing so as we just mentioned right you'll have me I want to have multiple sequencer where to find these sequencers we need a decentral network and after that we also want to decentralized verification where to find these verifiers we need this uh an hour and beyond that is how we do governance How We Do message passing across different Road Labs how we do the upgrading and even how we do this ancient governance all these we need a decentralized protocol so in the in the ideal case it should be ethereum that's why in my talk I mentioned about like the best case for inferior lab is to light all the Rolex in front to ethereum but we just created too much responsibility for you during in that case so that's why we need like something like this Beacon layer to help to coordinate orchestrate all these Road Labs at the same time provide is a decentralized sequencing verification um stitching slashing governance upgrading all this stuff in that case like we can provide the entire decentralized platform for all the developers especially game developers nft or the social developers which they are not familiar with how to speed up a job or blockchain right so they can just focus on their business logic so that's why we need a billionaire basically it's sort of like we follow the evolution of the technology in the uh in the in the lab space but at the same time it's definitely needed when we have more and more relapse in Easy in this ecosystem and space let's take this question I'll take this question which is which came on YouTube just now any plans to have your blocks Pro that look like etherscan but built on all clear lost Scout experience Explorer scan is somehow confusing so the answer is um you know thank you first of all thank you for the feedback uh around blog Scout um and so um the idea is yes we would love to support multiple uh different types of explorers and but at the moment we mostly focus on integrating those explorers that are kind of Open Source it's easier for us to kind of integrate that quite easily uh without rest of the uh you know stack at some point if you feel like it's not good enough for clients would be happy to go and build a separate Blogspot hopefully at that point maybe someone else will build a much better block uh Explorer and that we could just use and be quick um but for those and some people might uh those people who are looking to have a persistent uh block Explorer persistent roll up they might actually want to have a proper ether scan like like you know like like tool so in that way in that model basically you have to you know contact either scan and ether scan can build uh you know a Blogspot just for your specific use case as you can imagine it's not you know it's it's it comes with a cost and it's not very may not be interesting for let's say short-term usage that's kind of why we are using something which comes open sourced and free but yeah for those that are outlines that are looking for let's say a longer term persistent roll up then I think something like ethers can could make uh could make sense and yeah at some point uh once we have a little bit more time on hand we might be able to um might be interested in kind of building our own a Blogspot as well uh let's take another question which is um on um yeah so let's take a question more on the non-tech side this time so this is for yochi uh what are some of the real world use cases or applications where author Solutions have been successfully implemented um would you take that um yeah there are a bunch of Partners we've collaborated uh recently um like uh I believe I believe some of the questions that requires success successful examples we show like for the past few months once this entertainment as we all know right we maintain this OG badge and also these ort empty and also these carriers um uh cool um and for this afternoon right typically it's quite straightforward for users to use and obviously all the entertainment here within a few seconds or a few minutes so it's quite smooth and in the end uh like the protocol automatically uh settled all the antibiotic ethereum and people can easily trade over the Open Sea and beyond that we also have the partners who are organizing events using some antique case and we we successfully like sort of run is a anti uh system for some of these events like the football events uh recently in Malaysia and um and with some smart wallet we provide this really seamless user experience for the users so they don't need to set up metabastes they can just click on the page um to Mint the empty as the ticket and everything is done automatically on the back end on the other side it's related to the big pharmaceutical announcement and basically um some of the firms we already set up the partnership and they will start to use our layer um I mean our Ras platform to set up these different Rolex for different females again and the game studios so as you you all know right for game and uh um and and also software applications the uh the latency is very important so we got a lot of feedback from the game developers either from the web tool or from web3 of course their requests are quite different some of them requires for 15 billion gas limit some of them require for sub second block time and some of them want to be uh quite like sort of uh a quiet user friendly with zero gas so in that case it's like not possible for general purpose I want to achieve that and so they seek the help and support from us and the leader for all these um sort of a parameters or computer regions you can just assess it up where the dashboard yourself so no need I mean this manual uh support animal and these are sort of the examples we've done for the past few months and of course we continuously received the uh sort of the demands from D5 projects for different projects they want the security to be as high as possible so in that case is the one the multi sequencer and the meanwhile proof system is a must and uh and also some other governance Persia the one is multi-sequencer basically as many sequencer as possible because it's for governance so it would be great to have by decentralized System uh compared to single sequencer or it's rarely centralized rolapse system so as you can see right the requests and also the requirements from different projects are quite different but in the end like um we we clearly see that there's a lot of demand for customization and a lot of Demands for this dedicated rollout just for the applications so it's a it's a quite clear Trend at least for us to provide this local rust platform for these projects as they can just focus on their main business logic and underlying infrastructure can just uh being requested where our layers platform uh thank you yochi let's take a couple more questions we are we are pretty close running out of time so let's take two more questions so I'm seeing a question on on YouTube um all their theme team thank you for such a great event a lot of insights and deep dive into the role of landscape and developments are you planning to onboard third-party validated companies and test that the answer is yes we are at a stage where we're still the nodes are still run by us but we will so it will be kind of staged release the next stage we'll be opening up the test net uh in a kind of whitelisted manner so if you whitelist then we then you'll be able to come and join the network and the second stage would be a completely permissionless or tested by anyone could come and uh you know become a known operator presentation uh he has a list of uh you know upcoming I guess Milestones that uh he touches upon there's another related questions that I will pass to yauchi which is uh what is the system requirements for running a validator node yeah the cool part that based on our uh experiments like uh the specs for running either a sequencer or roll up or running a validator for the beacon layer it's a minimum I typically um like for some of our internal machines is like a two CPU core and 4G Ram but we recommend at least you have the machine with the laptop laptop specs right but in general it will be sort of the the minimum machine requirements and so that's why we always emphasize that we want to have this decentralized infrastructure as early as possible so we can welcome as many community members to join the network to provide their like sometimes machines for uh for this for becoming uh sequencers or becoming the uh Beacon Valley teachers the United States as I just mentioned right it's no longer like the AWS in web3 it's a real truly decentralized infrastructure system to provide your localizer system products uh for the yolab space I was on mute um so let's take one more question from I'll take one more question I will leave the last question would be a simple question for yaochi to close off so one last question for me which I'm picking from YouTube which is how do these Technologies address scalability uh cost efficiency and user experience challenges commonly faced in Design Lab decentralized applications the answer is actually when you have imagine for example when you are interacting today with a uni swap on ethereum you're basically as a user you're paying two fees uh one fee is what you paid to ethereum uh as gas fee and the second fee is what you pay to uh the application itself right so there are two fees that you pay and this is not great for user experience right and that's kind of why people are developing and building uh you know account abstraction where you could say okay I don't want to pay user doesn't have to pay the gas fee user only pay is the application fee for example the the fee for trading for example and all of this could be very easily adjusted on our this is very hard to do on a base chain and that's why you need layers and layers of Attraction on top to be built but if you have something like an application specific a roll up you could actually do and build all of this quite easily for example you could have a completely gasless experience and not in in the form of account abstraction because someone else is paying a gas but it here the motor could be completely gasless where there's no gas at all even at the protocol and you couldn't do that and that that's why that actually helps you in one way at least from user experience in terms of user experience so users don't have to worry about acquiring gas token and paying for gas and the other thing that you can do for example you could improve your latency and scalability which obviously uh gives a lot more benefit to people interacting with the blockchain one last question if we close off as we are running out of time uh we didn't mean it yeah so we are progressing uh step by step uh so right now uh if you look at the dashboard right so uh for a lot of these uh roller Services they are sort of production ready uh if you want to launch this campaign or some gaming sessions you can quickly launch via the dashboard and uh so basically all your operations and activities can be done on the flash layer after that you can settle down the uh selected one ethereum polygon and beam region uh so that part is all right already production ready um but for the beacon layer side right that's why we are running a lot of campaigns on the testing the the features for the beacon layer recently like for the past two we already tested uh sort of like this multi-sequencer and also staking slashing mechanism and later on we will test this no joining and the challenge in Canada um so granularly we will roll out features once this uh production ready and I mean well as you can see right we want to build all these stuff ready before we launch a midnight and so in that case I believe you will see a lot of good news and partnership along this way and hope the community can can basically join us and together we witnessed the progress of the entire development yeah well thank you Yoshi with this we have reached the end of the last day the roll up as a service day hope you enjoyed the talks from The Amazing speakers and this q a in particular for those who didn't notice it was a live q a uh so we're picking questions live from people posting them on YouTube and we are answering them in live uh we also thank all the speakers for making time to prepare for the talk uh given the busy schedules and you know time zone differences or not uh so yeah that's the end of this uh last a gentle reminder that several of our team members will be in Paris for ETC it's an event that happens will happen in mid July this year we'll also have a roll-up day Side Event there so if you will be in town come and say hello until then goodbye and have a very good day ahead thank you thank you everyone foreign [Music] 