[Music] thank you [Music] hi everyone this is omaya here the head of Investments at matalabs and today we'll be talking about the economic considerations of modular roll-up services now normally I'd Begin by trying to Define what a roll-up is but given what we've seen on Twitter recently it turns out it's a little bit more difficult than what it seems at face value no all one has to remember is that a roll-up tries to move parts of the execution from the base layer from the L1 off chain while saying something about the guarantees of the data availability and then in ultimately ends up posting a smaller version of the data that is required to reconstruct the state or at least say something about the validity of the state back to the L1 so the reason Dutch overall up really is to guarantee the correctness of chain execution as well I say something about the data availability behind that execution and a few other things to remember the roll-up nodes actually depend on the host chain in order to be able to update their states because the host chain defines the state transition function and as a place to nominally store their history the history of the actual roller we know that the actual layer one it's consensus and then therefore it's security does not in any way rely on the roll up because it's not waiting on a block by block basis uh to determine what happens on the rollup instead every once in a while the roll up will post its state which has to be consistent with the rules defined by the L1 and the the bridge between them if if you were to use the word bridge is the ability for the layer 1 to check the validity of the state transitions that are proposed by the L2 every once in a while now usually people talk about this idea that they roll up in a sense partitions the host chains consensus are really it's it's state by allowing it to apply a subset of rules that only really apply on the roll up so long as they are uh globally consistent with the rules that are prescribed by the L1 now because the roll-up needs to know about its own State as well as the state of the L1 at all times then the cost of running a roll-up is strictly higher than running just in L1 because you're basically now having to take care of two different chains so this must mean that there is there must be some utility that's generated by using a rollup that justifies spitting it up now this might seem obvious um but since we're going to talk about the economics of Road Labs I think it's important to highlight now something to remember is that blockchains body design are extremely computationally intensive it costs approximately 10 million times more to do a calculation per byte on ethereum than it does on AWS that is the cost of achieving the properties that are promised by a distributed Ledger and and this is really the sole motivator of considering uh roll-up economics because a minor Improvement in the execution environment can reduce the costs dramatically and you can see that this Gap is enormous and so there is sort of a lot of margin um to be gained by trying to cheapen the execution environment while preserving essential persistence decentralization and so on now the the basic idea is that instead of submitting transactions or series of transactions on the L1 and incurring the sort of base layer cost you move this process off chain and there's sets of actors that are responsible for essentially doing this and then posting the results back on chain now depending on the type of layer two whether it's sort of an optimistic design or a pessimistic design pessimistic here refers to validity proofs of course um these operators behave in slightly different ways and use very different technological architectures but the basic principle is sort of the same and as economic agents they have to think about the following question how do I provide these Services while pricing them in such a way that it is still profitable for me to do so and while the user experience is at least as good as what they would experience if they were executing directly on the L1 now there is this concept of application specific blockchains which I refer to more generally as specialized execution uh chains whether Roll-Ups or not um while using validity proofs or fraud proofs or whatever uh type of mechanism they handle the execution in a modular way and this modularity relies on a simple principle that you are able to share the security of the underlying L1 which has a cost and perhaps there is also a way to communicate between these different uh layers with or without going through the base layer now the utility of these modular setups lies in their ability to remove the constraints that are normally imposed on a network that's trying to bootstrap a cheaper execution environment but also incurs fragmented liquidity different ux additional latency and the premise is that these New Primitives that are being built from a technological perspective allow these modular networks to build a stack that is virtually vertically integrated and does not detract away from the user experience and certainly offers a much cheaper execution environment now when it comes to the economics of these layer twos the name of the game here is to achieve positive Network effects for unit economics without materially sacrificing decentralization and certainly not security and what I mean by positive Network effects for unit economics is that you basically escape the Paradigm that you're normally constrained to on the L1 where the more transactions that you have on a monolithic L1 on average the higher the cost of a single transaction becomes and that's basically a statement of um Network Demand right for a for a fixed uh computational Supply as the demand for computation increases the price increases on the L2 and that is really the fundamental insight there is this ability to batch transactions together and then there is this economic regime where it is possible that as the number of transactions increase across the network you can actually have lower costs right it's it's simply due to amortization right you compress um uh some parts of uh the data that's required and you do so um by allocating costs to multiple users now of course if there's only a single user of the network this is sort of moot but in the case that you have many many users each with a small subset of transactions then you can actually have decreasing costs Now by definition again the utility of the user of the L2 has to be strictly larger than the cost of running the L2 individually and the cost of publishing data to the L1 which I really term here is the cost of inheriting the security there's sort of no free lunch when when people say that the layer 2 in some way inherits the security L1 what they're really saying is that there there is a cost to publishing data back to the L1 as long as well as the cost to actually reading the state of the L1 yeah why is all this important well a layer two economically must be designed such that the utility of using layer 2 for a particular user is strictly higher than using uh than the utility generated from using the L1 and the same is true for the operator we call a layer 2 economically sustainable when the utility of both the operator of the roll up and of the user is strictly higher than than uh operating on L2 than it is on the L1 now here's where we get into some of the architectural uh trade-offs or design trade-offs associated with these different agents so in general we have somebody's responsible for the data somebody who's responsible for ordering the data somebody who's responsible for uh the state of the Roblox itself and then somebody who's responsible for generating some sort of proof uh that's associated with the validity or the correctness of the state transition now again depending on whether we're talking about a validity roll-up or an optimistic roll up um the the generation of the proofs is a sort of fundamentally different process but when it comes to sequencers in many ways they're sort of the same the sequencer is the agent that is solely responsible for ordering the transactions and then they send that sequence of transactions to someone who ultimately uh uses them in some capacity to update the state and the idea behind running a sequencer from an economic perspective is straightforward basically the sequencer has to design usually algorithmically on uh an order of the transactions that is strictly different than the order in which the transactions come in that maximizes some sort of Revenue now of course there there might be there might be some cases or some instances in which the ordering itself um is constrained by some sort of fairness algorithm I mean in in the in the case of centralized sequencers where the user simply trusts that the sequencer will uh order transactions let's say first come first serve basis um the sequencer is simply paid for the fact that they do this of course the sequencer runs a particular set of hardware and which requires some off-chain compute costs and they at least need to cover those costs and perhaps make a small uh profit on top of that the rules by which they order the transactions uh highly depends on the environment and also the actual um designation of the agent as a sequencer so in the case where you have a single sequencer it's sort of trivial um this person is guaranteed where this approach is guaranteed to be sequencing their transactions and so their economics are sort of straightforward whereas there are cases in which the sequencer is decentralized so that you have multiple agents that can perhaps bid or can be randomly selected to generate the canonical uh ordering of transactions and it is in that case that the economics start to become non-trivial first because you now have to ask multiple agents to participate knowing that only some of them might be able to propose the sequence of transactions and so if that's the case then there also has to be some sort of profit sharing scheme amongst them so that the ones that are idle or at least the ones that are not at the front of the line are able to cover their opportunity cost or at least their sort of Hardware costs now this in and of itself is an entire uh sort of area of research but the thing to remember is that the sequencer um even if they were to try and be malicious really at best they can only sense their transactions or they can um slow down or in some cases stop the operation of the roll-up but they can't insert a malicious transactions because the process to guarantee the Integrity of the transaction is sort of completely separate from the sequencer function however what that means is that we still have to worry about um sensorship resistance and the aliveness of the Roll Up the roll-up is uh useless um if it is completely secure but doesn't do anything um and so you what you you want to design the sequencer architecture so that a it maximizes uh liveness and B it also enables multiple sequencers to come in not only for the sake of decentralization but actually for redundancy and and this is where the economics become very very important and there are uh different uh sequencer design approaches um some of some of which include bonding or a proof of stake type sequencer where the sequencer guarantees some ordering of transactions and if that guarantees is sort of not upheld by the sequencer then they they are slashed um they can also guarantee certain transaction selection criteria they can price their own transactions they can use a global market for determining fees or a local market for determining fees and most importantly as we mentioned before they can also say something about the fairness of transaction processing and what that means basically is that they commit X ante to a scheme that orders the transactions and you can go back in time and audit that and and make sure that that's indeed what the sequencer has done now the really interesting uh part about the economics of running a sequencer is that now that you have basically separated the security of the underlying L1 from the execution environment you now have opened up a design space of different types of economic instruments like derivatives that could be used to run a sequencer a sequencer in many ways has to um hedge against uh the fluctuation of their variable costs and their variable costs are of course tied to the L1 and they're also tied to the transaction demand and so you can easily see that having something like a collateralized sequencer obligation which is an instrument that's sort of analogous to a debt obligation but in this case the debt is not monetary but rather the users commit to using a particular sequencer and their commitment has to be uh guaranteed somehow and then the sequencer in response to that commits to some sort of soft finality and deposits collateral as a bond and what that does is instead of being subject to the fluctuations of the spot Market whether it's on the L1 costs or on the transaction demand costs on the L2 the sequencer has some sort of forecasting ability in terms of the demand for their services so this becomes very interesting because the you can now have a set of sequencers in a competitive market go out and issue different types of instruments that are tied to their future performance and these instruments can be priced based on their previous performance so in the same way that you have credit rating agencies um issue different analysis and ratings for the credit worthiness of different financial institutions or companies based on their balance sheets based on their Workforce based on their previous performance and that enables the marginal creditor to think about the risk that they're taking by allocating capital or committing to a long-term contract or or some sort of service agreement with this particular provider you can now develop a reputational system upon which you price the uh future performance or rather discount the future performance of these different operators so that becomes quite interesting not just from a pricing perspective but also it makes the economic forecasting problem for the sequences themselves um much more tractable and this is particularly valuable in the case where you're using um some sort of proof of stake based sequencer where it becomes uh very important to know up front what the cost of running the sequencer is going to be for a given period of time because there is also the cost associated with hedging that exposures particularly if the um a bond is not in a numerator but rather in a different currency something like eth so that the operator can hedge their exposure to that currency as well so now it's probably a good time to talk about Mev um and what that generally refers to is the capacity to reorder transactions in such a way that the person who does the reordering or the entity that does the reordering benefits at the expense of another entity and of course from their perspective this is a positive and from uh the counterparty's perspective uh that's a negative um but it's useful to make the distinction between basically the two different types of Med uh there's a creative uh what I call a creative and what I call parasitic mov parasitic Mev is exactly what it sounds like um where the uh all of the agents external to the sequencer experience and that negative or at least the vast majority of them it's negative due to the reordering of the transactions and accretive is sort of the opposite now a good example of parasitic mov would be something like sandwiching right um uh on on some sort of Exchange and a good example of a creative mov would be something like interoperability as a service whether it's with uh different types of Roll-Ups auction clearing is another good example these are sort of accretive to all of the participants external to the sequencer now my hypothesis there is that there is enough diversity already in roll-up providers and now I'm referring to different Roll-Ups and not even necessarily uh sequencers that parasitic Mev uh the operator has no choice from a social perspective but to rebate it or pass it back on to the user because there is enough of a competition for roll-up providers let alone different sequences that will incentivize users to move away from the environments in which there are parasitic Mev now obviously in in the case where for a single roll-up you have a market for sequencers that just becomes even simpler in the case of um a creative Mev that's where the argument could be made that in the same way that a private company provides some value for a user and it gets to keep a bit of that value in the form of profits the operator or the sequencer that provides a creative mov to the users will be allowed to keep that as part of their p l and it shouldn't be surprising because something like auction clearing obviously requires running some sort of algorithm as well as incurring Hardware costs so of course the operator should be paid for that and the users would be happy to so long as it's within their utility framework to pay a little bit extra for the profit of the operator now speaking of uh profits we know that um as you mentioned before a marginal Improvement in the cost of execution of chain can provide a dramatic uh Improvement in the overall cost of execution because the cost of execution on the L1 is so high but another thing to remember is that on the L1 the inter machine workload variance is actually quite small so by that by that I mean most validators are are running similar workloads and why that's important is that on the L2 the variance is actually quite large and the reason for that is the same reason that we mentioned before this idea of decoupling between the execution uh its efficiency and security this leads to what I call a sparse market for L2 computation in the sense that you now have a very wide spectrum and therefore Economic Opportunity for The Operators to prove different types of transactions different types of workloads and sequence uh different types of transactions as well so now the name of the game becomes uh what different optimization routes or strategies could be used to reduce the ultimate cost of running the L2 whether it's from the sequencer's perspective whether it's from the approvers perspective and so on and so forth and this is where of course we have validity proofs that come in in all different kind of flavors and forms we have data availability solutions that change the architecture and therefore the economics of storing and querying State data um how you can become uh extremely efficient in terms of sampling some of that data to reduce the burden of storing the entire State and perhaps maybe you only need to store a small subset of that state which then leads to um this interesting problem of State growth management so l2's uh while they way in which they operate as sort of fundamentally different from the L1 because of the decoupling that we discussed before at the end of the day the uh Roll-Ups themselves have a state that they need to keep track of and um the state the the state will grow over time it might grow slower um than what you'd expect for normal L1 but again that that sort of depends on the architecture and the operational model but at the end of the day they run into exactly the same state management issues as the L1 does uh basically it becomes um more difficult to sync uh the full history of the node because you simply have a large amount of data so the name of the game then becomes how do I incentivize external operators to efficiently store parts of the state and enable querying parts of the state and how do I do so without burdening the rollup itself because if you need a roll up for the roll-up then we haven't really done we haven't really uh achieved very much so instead uh in your palette of uh resources and and operations that uh one considers in designing a roll-up they also have to consider how do you manage the state of the roll up so that you don't run into exactly the same problem that you would normally on the L1 and this is where the data availability problem um comes into play and uh ultimately the cost of verification of the validity proofs or of the fraud proofs as well as the cost of publishing the data back to the L1 um always provides a sort of ceiling for improvement because you then rely on the L1 itself to improve in order for you to reduce those costs so instead you have to think about okay how does one improve State Management on the L2 side and incentivize off-chain data availability using different types of models you know volition validium and so on um and how that introduces coordination problems latency problems um but also how that allows different roll-up operators to bootstrap um their their systems uh from from scratch so now another topic uh that we touched on was how does one enhance liveness and finality uh on these layer twos and again returning to this decoupling we now have this interesting opportunity for different types of economic agents to come in and actually improve uh the liveness properties on the l2s without necessarily having to coordinate or having to participate in the day-to-day operation of the rollup and here's what I mean by that you can now have what I call dedicated Fail-Safe sequencers that are paid a small fee that is weighted towards liveness without actually processing any load day to day they sort of behave as an insurance sequencer and if you think about uh insurance contracts in general you generally pay a premium for something that you hope that you will never use or never need to use and that's interesting because in that model you're basically paying for an agent to just sit there and do nothing and so long as they can provide certain guarantees that in the event that something happens that they're able to intervene you're happy to pay that premium these sequencers are basically being paid to be redundant and just sort of sit there and the reason that's interesting is because you can pay these uh agents in a non-native token so that it's not tied to the security of the L1 in in any way the other interesting aspect is that we know regardless of redundancy regardless of decentralization um you can never guarantee 100 uptime that is a real problem for l1s but for l2s you can also never really guarantee uh 100 uptime but what you can do is actually make the net cost of downtime much lower when you use the sort of clever economics so you can essentially socialize uh in the event of significant downtime you can socialize the loss across the multiple uh set of insurance providers and that are willing to underwrite that process now when it comes to finality we know that it's ultimately determined by when the proofs are posted back to the L1 and then the uh last roll up State um that's that's considered to be valid is now sort of canonical and immutable so the the name of the game here becomes how do I provide a market for finality and that's simply determined by the transaction inclusion um and we know from a user's perspective that they can force inclusion in the L2 uh at the expense of some minimum costs so assuming that um they have no time preference it is strictly cheaper for them to send their transaction to the sequencer but in the case that they do have some time Preference they now have to weigh that against the cost of including the transactions themselves and the probabilistic guarantee by the sequencer that they're going to include the transaction so in the same way that we discussed these issuance of these derivatives tying the performance of the sequencer um to the native demand we can also include a similar type of instrument that guarantees inclusion within n blocks or within a a finite period and these themselves could be priced so not only can the sequencer uh guarantee or forecast something about their future costs and the transaction Demand on the L2 the user can also instead of using this very coarse uh gas scheme lever that they're normally used to using and mempels on ethereum they can now have an entire spectrum of inclusion guarantees which gets us into the topic of transaction quality and by transaction quality uh I'm I'm really referring to the spectrum between a pristine transaction which is a single Atomic transaction that is very cheap to execute and is included effective immediately versus a series of complicated uh transactions that are expensive to perform and atomicity is not guaranteed um and most transactions or most series of transactions as a subset sort of lie somewhere in between and again you can now not only have a market for single transactions you can also have a market for bundled transactions and the reason you can have this is because you can now have these specialized sequencers that offer you these sort of services you can have in the case of multiple sequencers where one of them specializes in D5 like transactions or an nft like auction like transactions and so on they can offer you these bundled services uh as the most Market efficient way of executing them so you can see just based on the discussion that we've had before that the added uh um benefit of having multiple sequencers is not just from a redundancy perspective but also because it adds this new Vector of customizability of transactions that on the L1 were normally handled through um off-chain agreements or through something like flashbots um whereas now you can have a much more general purpose a much more robust way of doing so and more importantly you can price it and so now in an era in which uh applications will start to really think about how to subsidize or even eat the entirety of the gas transactions and just incorporate them as part of their p l in order to improve user experience they really need to be able to price not just single transactions and there are latencies and and atomicity but also um they need to be able to price bundles ethnicity of course is sort of trivial in the case of a single transaction but it's very very important in uh complex so now that we're talking about uh pricing there is uh one thing that I want to highlight which is that the network fees on layer 2's fundamentally uh differ from those on the L1 and it's really for a very simple reason the fundamental difference between L1 and L2 economics is that the native L1 assets command something that looks like a store of value Supremacy whereas the L2 simply does not and the reason for that is is quite simple if you inherit the security as an L2 from the L1 you must be economically subservient and what that means is that the security layer has to be guaranteed by an asset that is not native to the rollup but what it does do is that it opens up the design space for off-chain data availability and other off-chain services because now the roll up can serve as the vendor for these types of services which brings me uh to perhaps the most important uh question how do uh these uh l2s if they were to issue tokens um how does value accrue to these tokens and more importantly do does one even need a token for a layer 2 system and there are sort of two schools of thoughts one school of thought says that uh simply because security is inherited from the L1 the L2 token can at best serve as a bootstrapping mechanism for the L2 ecosystem but it is designed fundamentally so that it becomes less useful over time once enough activity is aggregated onto the L2 the L2 token itself it's valuable sort of decay the other school of thoughts um is that the layer 2 itself because it can serve as a vendor for these modular services that we described before that uh it now effectively acts as the marketplace for all of these off-chain computational services and so that can be uh accretive not just to the L2 itself but ultimately to the L1 to the extent that it enables um more economic activity on the L1 than what you would normally get without the existence of the L2 um so then here the question becomes can a layer two justify the existence of a token not just purely based on the mechanics of running uh the roll up but instead can it offer value to the L1 that wouldn't exist otherwise and with that I will leave you with some open questions and again it was an absolute pleasure to be here if you do have any comments or questions about anything that's discussed here as well as these open questions obviously please do reach out thanks again thank you [Music] 