[Music] thank you [Music] hello um good morning everybody welcome to this uh talk on eigen ta which is um a data availability mechanism for Roll-Ups uh in this talk I'll try to motivate why we think this is an interesting mechanism the talk is subtitled hyperscale open Innovation for rollups and you know Amrit gave a great talk just before this on alt layer and first thanks to Amrit and yaochi for inviting me to this uh great day the role of as a service day here um so what I'm going to talk about is how do you design a data availability layer that supports this kind of an aspiration that alt layer presented which is you want to support a large amount of throughput you want to bring um an insane amount of activity into the uh crypto ecosystem so that's why we call it hyperscale open Innovation for Roll-Ups um firstly I want to thank uh several of my team members who've contributed to this it's always a joint effort and thanks to all of them um okay let me jump into the applications so we want to start this with you know we wanted we're talking about hyperscale open Innovation and the question is why would somebody be interested in this and I want to start with a motivating thing you if you want to try to measure applications in the crypto ecosystem you can kind of do this on two axes one axis is what throughput do they consume and the other axis is how much value per bit is transacted what is value per bit like per bit of information that you're sending through the system how much uh you know is a single transaction worth hundreds of dollars or it's worth like a penny and okay so why is this important if you take this um the value you're operating in blockchains today very clearly we are in the uh top left quadrant here in the figure which is there is we're transacting systems mostly uh financial and nfts which have high value per bet these objects that you're trading or sending or worth a lot but it's not a lot of throughput so that's the access on which blockchains today are operating but that we want to be so we want to enable new and interesting applications like gaming social high throughput D5 uh not only we want to go to the high throughput regime but also we want to get to the low value per bit like you know a tweet how much is a tweet worth maybe it's worth a send maybe it's worth nothing so you want to go to that regime you are talking about a very different kind of system architecture which would get you that but even if valuable bit is low because the throughput is high the product of these two is what the total value transacted through the system is so if it's valuable bit times bits per second right and so that's value per second so that's this curve right okay so I want to get to this regime where you have very high throughput but low value per bit but the fundamental thing is therefore the transaction cost per bit needs to be um small okay so if you look at transaction cost per bit you can look at what are the fundamental contributing factors to this I think there are three basic contributions to the transaction uh cost number one is the capital cost right the capital cost is basically you need staking if you want security you need staking and if you have staking then there's a certain opportunity cost of capital so that's the first contributor to the capital cost the second contributor to the capital cost is the second contributor to the cost is operating cost you actually run the system you have to download the data you have to do all these things another third contributor to cost is actually congestion cost um and essentially what we've done is to think about like what these how do we reduce these three axes if you look at the third one the congestion cost the condition cost is basically just um the uh if we look at the congestion cost it's fundamentally coming from everybody who's transacting creates an externality on all the others and you if you're creating congestion you need to pay for it so right now pretty much all the blockchains fundamentally price for congestion so you're basically running at congestion cost so let's look at all these three cars and how we think about an eigen layer of reducing these costs for the capital cost if you want a certain amount of stake really and you know if you have one billion dollars take and you have to pay off the opportunity cost of the capital you may have to pay a 10 APR to actually compensate that and 10 APR of a one billion dollar we're talking about 100 million dollar annually in fees that needs to be paid for it so one way we can reduce the capital cost of staking is by having shared security you have the same pool of security supply not only to one application but to many many applications same thing that same economy of scale that really powered general purpose proof of stake blockchains can actually power other things as well so that's the capital cost if you have Shad security the principle that eigen layer Builds on is called restaking which is the idea that if you stake in ethereum you still have um that Capital that you can actually make additional credible commitments with for example you download and run and validate a new system you can actually promise that you're running that system correctly okay so we have Capital costs we have operating costs if you have you know everyone needs to download and store the data that actually adds up quite a bit over the entire system today actually the operating cost is non-dominant like even for a pretty expensive system like Solana you have like thousands of nodes each node maybe annually costs like ten thousand dollars so you're talking about the annual operating cost is 10 million dollars it is uh high but it is not as high as the capital cost because you have several you know five or ten billion worth of stake that means you're paying off like one billion dollar in like the stating APR okay so you have the capital cost you the operating cost which may not be dominant but as you increase the throughput it will start to become dominant and finally you have the congestion cost and the way to reduce congestion cost is actually by uh enabling shared uh throughput you actually build high performance systems so that there is very little congestion to begin with and you also build an economics where you don't have this unpredictable pricing of oh I don't know when the price is going to go uh up or down and this unpredictable pricing actually leads to a large number of problems uh amritin stock earlier pointed out how uh these blockchains can actually reduce these blockchains can actually learn from existing Cloud systems like AWS and I think we want to do something like that here where for example in AWS you can actually reserve a compute for a long enough time and you know that there is no congestion because you have reserved instances there's also another concept called a spot instance where you can go and buy an instance at the moment but if there is congestion that spot instance may be very highly priced so okay so that's the core core thing we want to think about the fundamental cost basis of the system and make sure that we are actually designing a system which is very very low in the fundamental cost per bit analysis okay so we you just saw the role of architecture earlier so I'm not going to go over this but the core idea is that right now roll up sequencer takes the data and then posts it on a data availability layer why do we need to do this you need to publish for the computation to be transferred and you need to publish the inputs to the computation because if you have the inputs to the computation anybody can replicate the computation or continue the computation onward so you know there are ways to slice and dice this so you could either publish the inputs to the computation or the outputs to the computation depending on whether you're running a um optimistic or a zero knowledge rule or a succinct validity roll up but you do need to publish the inputs or the outputs of the computation on a data availability layer so that's what does the data availability layer do it enables anybody else who wants to know the inputs or the outputs to this computation to come and acquire it Okay so what properties do we need from a DA layer okay actually I'm going to just swap to uh different presentation so here we go I think the animations probably work better on this okay what properties do we need from a DA layer we need the dla to be hyperscale okay what do we mean by hyperscale we want the bandwidth of the system to grow linearly with the number of nodes the more the nodes in the system the more the throughput that it should acquire and what do I mean by nodes in the system the nodes which are serving for data availability the more the nodes the better the system should be in fact it should be you know the most ambitious goal would be exactly linear you have n nodes each of them have C bandwidth you multiply the two n times C should be your system performance that's the most ambitious right you're aggregating all the bandwidth fluidly but there's at least this Factor true loss why because you know we want to operate the system even when like a majority as long as the majority of the nodes are honest so you do take us you know Factor two loss but not that much more like that's what hyperscale is is basically exact linear scaling for a dla okay so that's the scaling uh what about the cost right you know if you compare to a system like AWS which is a single node downloading and storing data maybe there's like a couple of copies or five copies which is storing it and you want to get to that kind of a cost basis even though you want your system to have thousands or maybe hundreds of thousands or even millions of nodes the cost basis should not be as though every node needs to download and store the data so just to pick on the first point here on hyperscale for example what we are expecting is as the number of nodes increases in the system today most of the systems don't have linear scaling they do they actually have no scaling the more the nodes in the system the capacitor the system Remains the Same but what we're looking for is the more the nodes in the system the system bandwidth scales linearly but the cost does not scale at all so that would be an insane system right like you have more performance as you get more nodes the cost doesn't increase and the security increases so you you just like slicing the trade off perfectly in your favor what would be the latency limit you know if you're looking at an AWS system you send a a request there's you know the request goes to the server and the server says yes I stored the data so that would be like a round trip latency and you'd want a system which operates at that kind of like a performance so that your roll-up nodes are not stalling on some data availability layer waiting for you know 12 seconds or you know um sometimes you know even 12 minutes for finality it should be verifiable nodes anybody who wants to verify that the nodes in the da system are doing their thing you should be able to verify it whether I have to download all the data so this is the property we call verifiability which is a simple light node can download and Wi-Fi all the data and finally you want to have it be customizable customizable basically means like somebody wants a different Quorum somebody wants to use their own token somebody wants a different kinds of properties we want to be able to allow for all of these different kinds of properties so this would be an ideal dla in our view hyperscale bandwidth's not a limit low cost low latency verifiable and customizable and so we've we're building a system called eigen da which is built on top of eigen layer eigen layer is this uh core platform which is based on restating so the idea of eigenlayer is the stake in ethereum you set the withdrawal credentials to the eigenlayer contracts eigen layer is a set of smart contracts on ethereum so it's taking ethereum you set the withdrawal address to the eigenlayer contracts and in the eigen layer contracts you set your own wallet address as the withdrawal address so what you're adding is really like a step in the withdrawal flow and what this does is it really allows the system to enforce um a system of like um checks and balances where like if you're not operating for the services that you opted in correctly then the system can enforce a penalty on you um so that's the um basic structure of eigen layer basically we call it restaking which is your stake in ethereum and then you stay you set the withdrawal credential to the eigenert contracts adding a step in the withdrawal flow but then enabling trust transfer to all kinds of other systems including like a data availability system okay how is I I can be achieving this hyper scale property the core idea is using Erasure codes to distribute the information so that no one knows need to download all the information so you encode it and send it to all these nodes and each node only downloads a little bit but they also come with cryptographic proofs that they've been encoded correctly so you don't need to worry about it so we use kcg polynomial cryptography this whole architecture of the cryptography underlying eigen da is basically built off of the ethereum roadmap you know in the ethereum roadmap we have this thing called dunk sharding which is a mechanism to actually scale the data availability bandwidth of ethereum eigen Da is basically like an opt-in system of down sharding that we've built and because it's out of protocol there's a lot of degrees of freedom that we have which actually lets us optimize even more than dog sharding so the donksharding throughput is roughly one megabytes per second 1.3 megabytes per second we are already at 10 megabytes per second with uh eigen ta and whereas the node bandwidth requirement to run an Inda node is actually much smaller than you know all the other systems okay so really this is you know one of our vision for eigen layer is that anybody can come and build these new features on top of ethereum security and or borrowing at least aspects of ethereum security and India is basically and and what we want to do is to enable people to experiment with new kinds of distributed system technologies that can then be built on top and the the best ideas can then be internalized back into ethereum over a longer time scale so I can do is our first kind of uh product on top of eigenlayer which basically tries to achieve this awesome so let me uh skip ahead to some of these other properties I've mentioned it's low cost it's low latency uh but I think one of the most important Dimensions that we need to consider when building the you know things like roll ups is how does it interact with the economics and if you look at the role of Economics today there is a bunch of uh problems with the role of economics and the first one is the Roll-Ups dominant cost is Da and d a cost is and the da cost is high you know basically you have a problem like you cannot make it cheaper than the da cost so that puts a floor on your cost basis and the second problem that we find is the d a cost is uncertain so you don't know because the da cost changes based on congestion also you have same site externalities you know Amrit and stop was mentioning about yoga labs for example if there's a yoga Labs roll up and you have another you know your checking along doing your own like other roll up and suddenly there's a yoga Labs mint and the dla is congested and then now like your da cost becomes uncertain maybe it blows up by 10x and you cannot satisfy your users and that's really problematic so the da cost being uncertain is actually like not very good and and finally roll ups you know unlike an L1 an L1 can fix a certain amount of inflation as the rewards for data availability inside their own system if you're an L1 but if you're a roll-up you can't pay in your own token for data availability you even if you allocate a treasury from your own token you still take exchange rate risk of oh I need to exchange my token for eat and maybe the eat price goes up and you need to kind of eat it on your roll up okay so I roll up with Ida can completely transform all these economics by uh getting the best benefits of chat security while retaining a lot of reviews of freedom first thing is the with eigen da the da cost is low why because the cost bases underneath eigen da is low and really by optimizing on all three axes of the capital costs because of restaking the uh operational cost because of the better like distributed systems architecture and then finally for congestion cause because you know throughput is high there is not congestion so you actually get the underlying cost basis is low not only that there is no uncertainty in the cost basis you can actually go to eigen Da like you reserve a spot you know a reserved instance on AWS you can actually Reserve hey I want 10 kilobytes per second bandwidth for the next one year and you can go and Reserve that on eigen day because again has plentiful data bandwidth we are able to actually do this and many many many systems can many many Roll-Ups can come and Reserve this kind of bandwidth and what you get is actually positive same-set externalities the more Roll-Ups come the capital cost of the system is split among many more Roll-Ups and the cost per byte actually goes down rather than going up finally we allow Roll-Ups when they're doing long-term deer reservation to actually pay fees in their own native token what this means is a roll-up may come and say Hey I want to bootstrap my system and I want to give like the ethereum stakers you know five percent or whatever of my token economy took to as as a DA fees for the first year but also that helped rebootstrap the da and bootstrap the roll up this is something that you can actually do on eigen okay so that concludes my talk basically you the core idea here is by optimizing the underlying cost basis of um how you build a DA you can actually achieve all the favorable properties hyperscale low cost low latency right fability and customizability on a common data availability layer which is built on top of chat security from ethereum thank you foreign 