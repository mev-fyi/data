welcome bankless nation it's a very special episode i'm super excited to bring you on today's state of the nation state of the nation is where it comes out every tuesday on the bankless live stream and then every wednesday morning on the rss feed where we relate it to big picture action items we drop some insights and action items uh and i'm happy to bring you some awesome awesome alpha coming out of a research analyst out of delphi digital we're talking about the ethereum roadmap beyond the merge in some ways the merge feels like the finish line for much of ethereum's history but there is still so much left to do you might have heard of things like bank sharding or data availability sampling or proposer builder separation and you might have thought wow that's really complex i might not ever understand that and you might be right about that and that's and and i'm sure that we are not totally we are not going into the full math about some of these things but we are going into some of these shared themes and shared strategies that each of these complex mechanism mechanisms have for the future of ethereum beyond the merge there's this common structure to all of these things and it has to do with harnessing complexity and the separation of powers to make ethereum scale computation while remaining decentralized so today on the show we're bringing john from delphi digital who wrote a fantastic research report raised by some of the uh leading ethereum researchers tim baiko paulinaya and even vitalik himself called the hitchhiker's guide to the ethereum roadmap and so we're bringing him on the show to talk about not the math but the meaning behind all of these things and what it means for ethereum what it means for you as a potential ethereum validator and what it means for you as a potential eath holder uh you guys might be aware that somebody is missing from this live stream ryan is in the middle of travel uh got rubbed by some travel plans uh so i'm taking this one solo today and our fearless leader will be back with us uh next week for the next week state of the nation but in the meantime we have to talk about uh a message from one of our sponsors alchemix uh alcomix uh you guys know alchemix it's the crypto powered savings paradigm where you can save money either in eath or die or your other stable coins uh you can deposit it into the alchemics deep yield farming account and for all of the money that you deposit into alchemics you can withdraw up to 50 of your deposits while that deposit grows and grows and grows from your yield farming in d5 which alcomix does for you so it allows you to save your money and spend it too it allows you to get your your interest uh payments paid to you up front through the alchemist web app and so you can check them out there is a link in the show notes bankless dot cc slash alchemix or the capital a uh you can see on screen all the assets that you can leverage including staked eth from lido and rocket pool as well as your preferred stable coin and if you are an alchemics token holder there is a coming tokenomics upgrade and there's a link in the show notes for you to explore that as well uh at this point in time this is when ryan would ask me what the state of the nation is so i just have to ask myself what the state of the nation is and today the state of the nation is checking and balancing and that i think is going to be a theme of the episode all of these very complex things like i mentioned earlier dank sharding proposer builder separation data availability sampling all fits under the theme of checking and balancing and we've heard the phrase checking and balancing before this comes from like your basic u.s history class and why it's so powerful is it allows for not one part of the power structures around ethereum to outsize the others uh and so today the state of the nations we are checking and balancing the state of ethereum we're going ahead and get right into the show with john from delphi digital to talk about all the different ways that ethereum checks and balances itself right after we get to some of these fantastic sponsors that make the show possible the era of proof of stake is upon us and lido is bringing proof of stake to everyone lydo is a decentralized staking protocol that allows users to stake their proof of stake assets using lido's distributed network of nodes don't choose between staking your assets or using them as collateral in defy with lido you can have both using lido you can stick any amount of your eth to the lido validating network and receive st eth in return ses can be traded used as collateral for lending and borrowing or leverage on your favorite d5 protocols all this without giving up your eats to centralized staking services or exchanges lydo now supports terra solana kusama and polygon staking whatever your preferred proof-of-stake asset is lido is here to take away the complexities of staking while enabling you to get liquidity on your stake if you want to stake your eat terra soul or matic and get liquidity on your steak go to lydo.fi to get started that's l-i-d-o-f-i to get started the layer 2 era is upon us ethereum's layer 2 ecosystem is growing every day and we need bridges to be fast and efficient in order to live a layer two life across is the fastest cheapest and most secure cross-chain bridge with across you don't have to worry about the long wait times or high fees to get your assets to the chain of your choice assets are bridged and available for use almost instantaneously across bridges are powered by uma's optimistic oracle to securely transfer tokens from layer 2 back to ethereum a token proposal is being deliberated as we speak in the across forum where community members will decide on the token distribution you can have your part of across the story by joining the discord and becoming a co-founder and helping to design the fair fair launch of the cross if you want to bridge your assets quickly and securely go to across.to to bridge your assets between ethereum optimism arbitrarium or boba networks if you're trying to grow and preserve your crypto wealth optimizing your taxes is just as lucrative as trying to find the next hidden gem alto ira can help you invest in crypto in tax advantage ways to help you preserve your hard earned money also crypto ira lets you invest in more than 150 coins and tokens with all the same tax advantages of an ira they make it easy to fund your alternative ira or crypto ira via your 401k or by contributing directly from your bank account there is no setup or account fees and it's all you need to do to invest in crypto tax-free let me repeat that again you can invest in crypto tax-free diversify like the pros and trade without tax headaches open an alto crypto ira to invest in crypto tax-free just go to altoira.com bankless that's a-l-t-o-i-r-a-dot-com bankless and start investing in crypto today all right banquo's nation we are here with john charbonneau from delphi digital john welcome to the show man what's up happy to be on first podcast for me yeah congratulations and we were talking a little bit backstage you have a very recent history with ethereum and crypto which is pretty impressive that you wrote an article praised by vitalik himself as being extremely accurate and extremely well researched uh and so like i mentioned i teased in the in the intro some of these things are extremely complicated there is like crazy math like you got to go back to algebra and polynomials understand some of these things that's not here what i what i want to talk about today because i kind of want to talk more about just the vibes of all of these things how all of these things have a shared structure a shared pattern so before we get into some of the more complicated stuff can you just summarize the vibe of the ethereum roadmap post merge like what what should we expect yeah so the big thing obviously is the merge is not going to be scaling ethereum um so the priority after the merge is going to be all of these different steps that we need um to scale all of the computation on ethereum through the role of centric roadmap so basically trying to make ethereum a really good scalable base layer for rollups while at the same time scaling that throughput keeping it really really decentralized and easy to validate um because that's what keeps everything in check ultimately like true scaling isn't just what's your tps it's throughput relative to what is the cost to validate and that second part is what a lot of other typical monolithical ones that just try to like jam through put through on beefier hardware they ignore that second part so that's not true scalability my mind it's keeping that second part in mind at the exact same time that you need to do right one version of scalability is we just do away with the whole blockchain thing and we just go back to a database and then we have just the most scalable system on the planet but then we'd lose all that trustlessness so to summarize what you're saying post merge posts once we get to proof of stake it's all about finding these different ways to scale computation to scale throughput well without losing all of the cool properties that mako trusts this blockchain to trustless blockchain all right so in the intro to your piece and just to dive into this a little bit more you wrote uh if um uh scaling computation without uh sacrificing decentralized validation let's dive into the validation aspect of this what does it mean to uh scale computation without sacrificing validation uh from from the user who might be like thinking about staking ethereum or ether in the future to become part of the network that validation aspect i think is really important can you talk about elaborate on that part yeah so the big theme with a lot of what ethereum is going to be doing going forward both at the l1 level and the roll-up level is that there's probably gonna be some specialization in centralization and tasks um primarily block production the realization of that is that you just need to really focus the most on decentralizing the validation of that because that's what keeps everything in check um like vitalik's endgame post is what put it the best that most roads tend to lead to that end scenario whether it's in roll-ups you see that you're probably going to have somewhat specialized block production if you want to have the highest throughput on them what's important is to keep regular users behind essentially full node security without the resource requirements of what typical like high throughput l1 would require you to need today so you want to give people that level of security by fully validating the chain so if you have a malicious block producer you just reject the transaction you could just see right away if you're just running a light client of a monolithic l1 that because it just has too high a hardware requirement to run a full mode they can pretty much do whatever they want to and you're just not going to notice it because you're trusting the honest majority that's not the case with a full node a full node won't be accepting invalid transactions and if you're working on a roll-up then you're hiding behind the safety of fraud proofs or zq proofs so keeping that validation for regular users is what really really matters a lot and so we i mentioned three components in the intro data availability sampling proposer builder separation and dank sharding slash proto dink sharding these are all different strategies to get to the same end result correct and so different ways to scale ethereum uh that scale different parts of ethereum but ultimately produce that scaled ethereum with decentralized validation is that all correct yeah that's right because the overall vision of ethereum right now is obviously the roll-up-centric roadmap the problem with ethereum right now is it's not built to actually host roll-ups it's kind of just makeshift right now and we're making do with what the l1 can handle so a lot of these are primarily geared towards scaling the data availability throughput because that is a big bottleneck for the rollups right now that they need to post their data to the l1 and the launch is not optimized for that today so it's being able to scale that while at the same time making it really really easy for regular people to validate that so people who were familiar with the older ethereum roadmap uh we talked about sharding as a way to like scale computation on the ethereum at layer one but we've shifted away from that where um computation scaling has uh been thought to now move into the roll-ups and computation scaling that's we're really talking about like transaction throughput like how fast transactions can go and so uh once we now that we have this roll-up-centric roadmap of ethereum a lot of that computation is happening on the roll-ups uh that's what roll-ups are due and but in order to allow those roll-ups to really become unleashed and go up to their maximum throughput potential we need to enable data to have make data available to them because the resource that rollups need the most is data can you talk about this transition from like this older version of the ethereum roadmap where we were going to actually scale the ethereum layer one in terms of transactional throughput to what we've kind of pivoted to now which we've actually focused on scaling the data availability yeah sure so the old road map was yeah just jammed through the execution on the l1 through execution charting um we've since realized essentially that it's better in nearly all respects to probably put that on rollups and then just optimize the base layer for data availability um so that is what the old starting design was doing the the previous one after execution shards it shifted to the previous data starting design where there was going to be these 64 different data charts and you just post data to them as the roll-ups and there are committees that are checking all of them and so as a validator you're testing that all the data was made available because the data availability is important for the security of those roll-ups for example in the case of an optimistic roll-up you need the data available to successfully be able to arbitrate fraud proof um you also need it in the case of a zk proof for zk ropes to be able to recompete the state and be able to exit it safely so the key thing that we've realized is okay we can shift that block production and execution off chain to roll ups um while now just focusing really on making a really scalable data layer while also keeping the regular execution part for settlement of rollups like they posted proofs to the l1 they can bridge through it um keeping all of those trust assumptions together but focusing on now scaling okay how do we put a bunch of data through the l1 without jacking up resource requirements and uh correct me if i'm wrong and again listeners we are going to go into the three things in in detail data availability sampling proposer builder separation and dang sharding um but uh john before we get there just again correct me if i'm wrong but it's data availability sampling and bank sharding that do scale up uh the effective data of the ethereum layer one effectively increasing throughput on the roll-ups and proposer builder separation is something different is this all correct yeah so proposer builder separation is one of the really important pretty much necessary things to enable dank sharding compared to the previous um sharding design so you could do data sharding without proposal builder separation but proposer builder separation makes the new sharding design possible which wasn't previously because otherwise it would have been too high of a resource requirement for regular validators data availability sampling is part of that making it really easy for validators part that i can very securely check that all of the data was made available and know with 100 certainty or near 100 certainty that it was all available and that means that okay if it was all available and i'm working on a roll up if the data was available and no fraud proof has been posted i can safely assume if there is one person who would have posted um that fraud proof that i'm good to go or if a zk proof was posted and i know all the data was available then i know i'm good to go and what that does is that allows effectively more data to become useful to the ethereum validators while because we only have to verify a subset of that data we get to prove that all of the data is available without having to check all of the data and so we effectively get a data throughput increase because our validators only have to check a minority subset of the data but as a whole entire ecosystem we get to leverage the full expression of all of the data and this is what we mean by scaling ethereum without scaling computational requirements am i tracking here yeah exactly today is the opposite of that where it's not built as a data availability layer so when you post call date of the l1 it's just every node has to fully download it and then you hold on to it that's a very resource intensive way that you can't scale that up to massive throughput okay and then overall i would say resource intensive i would say that these three things again which we're going to go into all fit under that vibe of how do we scale computation without scaling resource requirement as like literally using cool cryptography using cool math tricks how do we scale computation i.e throughput i.e transaction speed uh and and costs without without increasing the resource requirements of these tricks uh because that is what preserves ethereum and keeps it decentralized yep yeah that's right okay and one last question as we stay high level because again super complicated subject so i want to make sure all the listeners and all the viewers get the right vibe uh what does this end state look like post the post inclusion of all these different mechanisms again data availability sampling proposer builder separation and think sharding slash proto data charting what's the end state of ethereum look like can you kind of just walk us through a holistic visualization or interpretation of ethereum in this end state yeah the yeah the the really high level of it will be that you basically introduce this new entity who's going to be a specialized builder so they're going to be responsible for making this really big block together that has all the data the beacon chain block everything put together and then you have this very decentralized very low resource requirement set of validators who don't have to build the whole block they just take it and they have to say okay is this a is this block good to go and was all the data made available and you could pretty easily do that data check with data availability sampling which is very different from today today you would need to download all of that data so that's why we can't scale it up when you introduce the pbs uh proposal builder separation as that new role you get rid of the high resource requirement parts and then you add data availability sampling to make it really easy to do that proposer job of just checking the data was available okay and the the last subject i wanted to want to get to is the interactions between all these components and so i want to put my devil's advocate hat on my ethereum critiquer hat on and say well ethereum's complicated and it's just like solving its problems with more complication right it's just adding complication on top of complication uh and then eventually like once all these three things that david keeps on listing off by name once those get included then we're going to have like eight more things to solve the complexity there and then we're gonna have like 18 more things to solve the complexity there so that's that's the devil's advocate version of ethereum where like it's already complicated we're just making it more complicated to do all these things and then there's the opposite interpretation where actually these three things fit together really really well and have this sort of like elegance between the interactions between these three components uh what what size what side of the that take would you say that you're on it's all a mess everything does fit together really well um even beyond just these three components when you zoom out and look at almost every major component of ethereum's future roadmap almost everything does boil down to those two points of scaling computation while making it really really easy to validate i mean taking these as an example and how they are necessary and kind of interweave with each other dang sharding which is what we're going for proposer builder separation is necessary to do that it helps it scale easily and it makes it easier for us to validate it because you make the proposer job really easy um these all of these different pieces are kind of just running in parallel and they're all chipping away at different things um whether you look at even looking at a bunch of stuff on the ethereum roadmap but so much of it is just how do we lower the resource requirements to validate um whether you look at things like history like history expiry that's okay you don't need to have as big a hard drive to run a full node if you look at statelessness that helps like reduce your ssd requirements so you don't have to hold your state on hand like they're all chipping away at different pieces i mean like data availability sampling which is one that we'll go into more like it helps you lower your bandwidth requirements and that you don't have to download all this data all of these things ultimately do really come into this very cohesive view when you zoom out and you like start to understand how they all work with each other so i think with one of these components like take data availability sampling for example like we get a bandwidth reduction which is nice because then blocks propagate faster we can have more throughput that way um that might be like one single like a sized upgrade i don't know how you want to slide this thing like maybe one order of magnitude but then when we layer on like the next thing like proposer builder separation like we actually get another order of magnitude but then these things interact right these inter these orders of magnitude compound on each other and then we throw in the third thing and then we have like three different ways we are decreasing resources requirements and they're not they don't um they're not linear it's exponentially scaling it's exponential uh reduction of resource requirement uh which turns into an exponential scaling of the actual throughput of the holistic ethereum ecosystem is that a fair take yeah yeah that's generally about right and that's why you see like people will have heard of and we'll go into this in more depth prototank charting which is a halfway step to dank charting um and that's exactly what you see where you have a certain amount of data availability throughput today if you go to protodank sharding which implements part of the steps of dank sharding you get some orders of magnitude scaling and then dank sharding layers on even more of the exciting changes that are coming and then you get even more orders of magnitude scaling for data availability on top of that uh so just to answer a question which i think might have popped up in some listeners minds uh dank sharding slash proto date charting can you talk about how these names came to be yes so dank sharding which is a great name um play on took the old uh discharging design and don crowd is the one who came up with that um and then the halfway one proto-dang sharding is a play on taking that and then also proto-lambda uh pitched in to add that like halfway measured right okay so proto lambda he's with the uh uh he i can't remember where he was but he was working maybe at the ef before this yes he was working at the ef uh then he joined the optimism team he's called proto-lambda i'm sure it's not his actual name but that's what we call him uh and then there's dank radfeist uh also at the ef uh and so these these things are are fondly named after them uh and so uh ethereum's sticking true to his name of kind of picking weird names for its upgrades um so okay so the here's where i i i get a little bit confused john i kind of want you to to pick the next path forward we have three different things to talk about data availability sampling proposer builder separation dank charting slash prototyping charting since they're all interrelated like is there a starting place is there one that we should pick out first to talk about first or is it kind of a chicken and an egg problem since they're all interrelated there is no starting point for this whole entire conversation yeah they're they're all pretty interweaved like you're gonna see data availability sampling proposer builder separation and dank charting all come together um and then prototype charting is the halfway step that implements like some of the transaction formatting um so they all do very much interlock those three so we're just gonna have to pick one yes all right let's go with a data availability sampling can you explain data availability sampling and how it scales computation while keeping ethereum decentralized yeah so back to that message of we're trying to scale the data availability throughput um of the l1 and just to remember there's a difference between data availability and data retrievability um what we need for the security guarantees of the consensus layer isn't that this data is retrievable forever what we need to know is was this data published did we attest that it was available such that it's available for some period of time that any interested party could have downloaded it and could have reasonably submitted a fraud proof or done anything like that like that's what we need for our safety um the history like retrievability of it is a separate much weaker assumption that like is dealt with separately um so that's what the data availability is um and then well let's actually dive into that a little bit more so uh data availability if i'm uh recalling my ethereum knowledge it makes it availability for an amount of time and that amount of time is just assumed to be long enough to know that the whole world could have downloaded and had that data available to them but it doesn't but the ethereum protocol does not commit to embedding that into the blockchain perpetually so it will not be saved inside of the ethereum blockchain forever but the ethereum protocol generates assurances that there was enough time for the whole world to have downloaded that data at some point in time what can you uh just let's uh elaborate and unpack the difference between these two things and why we wouldn't just save it in the ethereum protocol forever and how how do we know that it was actually available to the whole world uh for a sufficient amount of time uh and why are we why do we feel secure pruning it from the blockchain at all exactly yeah so the problem with if you require it stay on chain forever that every like full node has to hold on to this data forever um you just can't feasibly keep this thing decentralized um that's just way too high of a resource requirement to ask all of these nodes that you want to be validating um to be able to hold on to this like ever-growing massively growing thing that's going to have terabytes of data that are running through it when we like implement charting in these things um so right off the bat we know that that just can't keep it like decentralized validation if we do that and then the resource bandwidth that we're talking about here is hard drive space right like if we if we let it get out of hand all ethereum nodes that like all of us that plan on staking our eth and running our nodes we would have like a normal size computer except for the this massive massive just like rack of hard drive space uh and so that that's the resource requirement that we are minimizing here uh with this uh data availability sampling is that correct uh so that would be minimized by the fact that we are going to start pruning um these data blobs when we introduce the new transaction format with protodank charting okay um this this history issue is going to be is going to be exacerbated by the fact that we start posting a bunch of data to the l1 but it's already an issue which is why apart from all of this there is separately another eip like called eip44s that is going to start allowing nodes to prune um all historical data after a year and stop serving that to the network um the difference today is that call data which is what rollups use today it just persists on chain it just stays there but that's not the security guarantee that we really need for rollups um so what protodank charting is going to do one of its big halfway steps to going to full bank charting is it introduces the new transaction format that rollups will use so today they post call data once protodank sharding is enabled they'll start posting data blobs and the thing with data blobs is they can get pruned after about a month so that's more than enough time that we guarantee that it's available that people could have posted fraud proofs or done whatever anyone could have downloaded it in a reasonable period of time um but then full notes just don't have to hang on to it it's not their problem because history is just a one event like trust assumption as long as someone out there is holding the history it's fine i could just go ask them for it um you don't need everyone to be holding that on hand like it's just not necessary so one of an assumption as in if there's one ethereum node that's out there that has this data or like what is the actual process of why why would i would want to go retrieve very old data and then how do i actually do go about finding that data so yeah i like it could be anyone i mean like the easiest example is block explorers if you have some kind of business that depends on i need to be able to serve um this history whenever like anyone asked for it they're a perfect example of as long as they're storing it you're fine um there are other decentralized and like more innovative options um to make sure that we are better incentivizing history um retrieval as it starts to become more of an issue it's not like a big issue today um like another thing is it's really not an issue for the ethereum protocol it's only an issue for apps basically if they use if they lose like their old history and they're like oh i can't prove that this thing happens um so one thing that for example roll-ups could do is they could just mandate that they have to hold on to their own like relevant parts of their history um there's lots of innovative solutions where as long as someone is holding on to it and i could just ask them i can always prove that what they gave me is valid so as long as someone still has it it's not lost forever okay so if a rollup makes a layer one transaction with a specific amount of interactions with a part of the ethereum layer one i could talk to these tokens and talk to those tokens the roll-up can make sure that the data for those relevant transactions are always stored by the roll-up providers right the roll-up nodes and so that would make that that part of ethereum persistently and perpetually available for uh for those with that particular rollup and so that part of ethereum is secured uh and then we could probably repeat this process like 50 100 1 000 times and then eventually we'll get to the full entire full archive state of ethereum but it's no one single node is responsible for storing all of it is this correct yeah it's never going to be mandated in protocol that ethereum needs it you could also try to do decentralized um uh different like incentivization where you can have like different networks um where you are paying people to like hold your data but it's just fundamentally not the etherium l1 like consensus layers problem um because it's not like breaking anything if you can't like resurrect your history it's a problem for that app like hey i can't do this transaction or whatever now i can't prove this thing um but that fundamentally is just a different task that's much easier to outsource and it makes sense to like keep everything very decentralized okay it's not breaking anything if you can't i can't remember how you exactly said it but i think it led into my my next question where if i put on my bitcoiner hat and they're like you guys are pruning the data from your chain like i would get triggered right like bitcoiners are extremely hyper focused on backwards compatibility and making sure that if somebody mined a block in in 2010 and then they didn't move those coins from 2010 onwards into 2050 like the the emphasis from bitcoin is that you will always be able to retrieve your coins and the data and you also get the um the self-sovereignty from always being able to run a node and spin up a node from genesis and verify every single transaction so we're not violating that principle by doing this yeah because the reality is is you won't yourself have the data but as long as you can get from one single person out there like it's really just not an issue and it fundamentally isn't a big concern because there are always going to be these things and you can incentivize them in different ways you can use like they're like third-party like indexing things like you have like the graph um like there's work with like all of these different things that like as long as someone's holding on to it like it's not a major concern and the uh and the idea is that even if one person again putting on my bitcoiner hat okay but you're asking me to trust one single person to serve me the data like what if they serve me incorrect data that uh serves their needs instead of mine what's the response to that so you only need one out of all of these different candidates who should be storing it whether it's at whether it's roll-ups storing theirs their own data whether it's individuals block explorers if people start using like the graph for all these different solutions you just need one of them to provide it to you but how do i know they're giving me the correct data exactly like they can't give you fraudulent data you could always still prove that hey this thing happened like you can like that that's not like they can't just serve you invalid data and then you'll take it um you can always prove on chain that like you can go back and look that hey is does this match um like they can't trick you into doing something the worst thing that could happen is just they don't give you the data and then you just can't retrieve what happens okay uh now is this still this is where some of these interactions between these things get a little hairy is this what we're discussing now about uh the ultimate pruning of the of the blockchain is this data availability sampling or is this dank sharding or proto-dink charting or is it both uh this will be implemented with prototank sharding okay and then the other steps will be implemented with full bank charting so this is the main reason that protodank charting is able to scale is because it introduces the new transaction format um that are going to be carrying these data blogs and these data blobs will be pruned after about a month or so once we know that they've safely been available anyone could have downloaded them anyone could have checked them you can print them and then because of that we can like send through orders of magnitude more data throughput because we're not requiring people to hold on to these things forever so we don't have to worry about blowing up the history with them okay so with with thing charting and proto-ding sharding we can juice up the throughput of ethereum because we are only requiring the nodes to really store the more recent uh parts of ethereum and not the long-term parts of ethereum therefore were were more okay with juicing up the data requirements because it's all in the short term not in the long term correct yeah and the reason that we could then take another step up from protodank sharding to full dang sharding is because in protodank charting we're not going to have data availability sampling or any of that we are still going to require the validating nodes to fully download the data that's how they'll have to check that the data is available is they will have to fully download everything and say hey was this data available when we go to full bank sharding after that you'll only be checking a subset of the data so then the data will be sharded so then that gives you the extra orders of magnitude and scalability okay and then the the going from proto-dink starting to full dank sharding that's when that crazy like polynomial math comes into play where like we can allow for more data to come in because the validators only have to check parts of it and because of crazy polynomial math we have the assurances that uh to like the 99.999 percent likelihood that all of that data is there and so we can uh do you know like the order of magnitude increase and data from going from uh proto ding charting to ding sharding do you know what that is from sorry from proto to a full bank charity yeah like what's the scalability increase on that one i want to say it's about a 30x if i remember correctly wow yeah yeah okay so we we take proto-dang sharding uh and then we add this like polynomial math to get to full denk charting and that gives us a 30x increase in data availability uh yes or i want to say 16. i don't remember the multiple on it but it's yeah it's in the it's in the low orders of magnitude number um the target block size is gonna be around a megabyte um uh for data availability um for um for prototank sharding and then it'll go up another couple orders of magnitude or so um from proto-full so because we update availability sampling because you don't have to download everything anymore okay so data availability sampling is that the polynomial math thing that i was just talking about um that is part of it um you yeah the kzg commitments are like how you um commit to the data and that like includes um that that's the like polynomial part of it um that's where like some of the trickier math comes in um that would be the kzg commitments would be partially introduced um by protodank sharding but not the full like 2d kzg scheme with data availability sampling like some of the like the gr like nitty gritty stuff okay so what components of data availability sampling have we not already discussed that we need to dive into and and or maybe we should start from start from scratch because i'm literally i think the listeners can like figure this out by now i'm literally learning as we go um okay let's let's go back to data availability sampling uh what what is the resource requirement that that is trying to mitigate and how does that work yeah so yeah first i'll explain how it works um so the basic idea is like i said with protodank charting it's not super scalable to just download all the data so what we want to do is make it such that you only have to download a piece of the data um a really small portion of it but how do you like the naive way to do that is just to like take this data that was posted i check a bunch of little pieces and if they're all there i'd just say it's good um the problem with that is is you could have missed the one transaction that like prints a million eighth or like whatever and then you're screwed right um and like all your money's gone or whatever um so that doesn't mean you have to verify every single transaction exactly so that would be the naive way to do it um the better way to do it is what you you do what's called erasure coding so that initial data the original like actual data gets extended so you have this like parity data and this is like used in like error correction it's the same like type of technology that's used in like cds that if they get scratched you can like reconstruct it with only like part of the data it's the same like concept as that um where now you have this original data and you have the extension data and you only actually need 50 of that data and then you could reconstruct the whole thing and it could be any 50 of the data it could be a little bit of this a little bit of this like it could be anywhere so then it makes it really easy to do that sampling math because as long as i have 50 of the data i could recreate the whole thing so that makes it really easy now because i could do the samples and for you to trick me into like a testing to this block you have to be hiding more than 50 of the block not not just like one transaction you have to be hiding more than 50 of like this data that was given to me so now let's say you were trying to do that and i checked once and i was successful i got the data so i still have a 50 50 shot that like the block isn't fully available um but if i check it again now i have a 75 chance and then you you have an 87.5 percent chance like you keep doing that and if you do that like a fixed number of times where you have a 50 chance of being tricked each time and you do that like 30 times or so your odds of being successfully tricked like 30 times in a row at 50 odds is like essentially zero um so that's what we do is we extend the data using erasure coding such that you only need 50 of to be available and then you could safely sample the different pieces um and then if those were successful you can say with pretty much statistical certainty that like the block is available to me okay so let me i'm going to repeat this back to you because to hope that i got it so there's this there's this amount of data there's this data that i got to download and verify and i need to make sure that 100 of all transactions in this bit of data is completely valid it's not enough to do all transactions but one we can't cut that shortcut because if one single transaction gets in if this is coming from an attacker they might do something like print a billion ether and give it to themselves and all of a sudden we have like a huge problem with ethereum security if somebody is able to print a billion ether because now they probably control the network uh they definitely do control the network so we need to make sure that 100 of all transactions are completely valid but we do what we want to do we don't we we don't have to make sure that they're valid sorry that that's one point to that that is really important is the the layer one says nothing about the validity of those transactions that are posted to the l1 from rollups it just says that they're available and then you rely on proofs to tell you was it valid or not but the l1 guy who's just checking the data doesn't say it's valid it just says it was available and someone else will check it okay okay cool again leaning into the whole separation of checks and balances this one person says this data is available somebody else go check if it's valid but right now we're talking about okay all transactions are available to be checked by somebody else and so this one person's job is to say all of this data every single transaction that is exists can be checked by someone and again we need to make sure that every single transaction can be checked so we have this amount of this thing of data and we do this thing with this weird thing this complex math thing called of razer erasure data razer extends the data and my version of that is it makes my interpretation of that it's like it makes like a kind of like a like a square root of negative one version of it i don't know it takes this data and it makes this like random uh manipulation to it to extend the data and so and what that does again with crazy math is that it it takes the same blob of data but then it makes this like clone of it that is in a particular out of a particular derivation and then this person as a result of that uh i might be skipping this up here as a result of that uh you i need to check that this is when we get into the 50 odds thing right because of this particular way of this expression of this uh creation of extra data uh and also with polynomial math yeah so yeah yeah so like the way the polynomial yeah so there's two parts of it you basically need to know that one the data was made available so that's what data availability sampling gives you that you could check all these random pieces say it was available and i'm good to go the other part of it is you need to make sure that the data was extended properly yes because if that extra 50 they gave you was just like dummy data then i can't actually reconstruct it because that was just like useless data so that's where the like polynomial math comes in these things called kzg commitments um those will prove to you that the data was extended correctly okay so then you have the kzg commitments telling you proving that the data was extended correctly and then i could data availability samples saying it was extended correctly and it was also available okay so once we get to that point we have we have those assurances and then we just start taking actual samples right so the first step is to prove that all of the data is uh is extended correctly and that extending correctly once we get to that point of extending the data correctly that's when we get to sample it with the increase and increase assurances that there's nothing hidden so the extension uh allows us to have more effective sampling and then once we get to the extension then we start sampling this thing and sampling is like flipping a coin uh except once we get to like 30 30 coin flips like you will never ever anyone in anyone's life flip a coin 30 times and have it be heads every single time that will never ever happen in your life and so that's how we get to the assurances that we feel secure about like there is it's literally impossible to hide a malicious transaction uh in this like fake bit of data and so the first step is to extend the data to allow for more efficient and more effective sampling and then we sample it about 30 times because that's where like the level that we deem safely safe to assume that all the rest of the data is also valid or not valid but available uh is this all correct yeah in the real number it's going to be 75 for some like complicated 2d kcg scheme stuff but it's probably not like necessary to go into the details of that okay okay uh so as you can see my brain is starting to break john can you just walk us through one more time now that we've gone through this can you just walk us through the date availability process one more time from the high level yeah so in proto dank charting it's gonna be simple uh you still just fully download the data and then that's it if you fully downloaded it you got it then you're good to go you sign off upon it the reason that you could do more throughput for the data in pr in full bank sharding is because now you're only going to be um the validators will only be downloading parts of the data they'll be doing data availability sampling um they'll be downloading like certain rows and columns um and then they'll test you if theirs were available then collectively you're good to go as long as there's enough people who are sampling it um so that like easier requirement allow because you don't have to download the full thing makes it easier for you to like kind of like juice up the data availability through but another level fantastic okay so this is where we're about to get into a very fun part of this whole entire story which is proposer builder separation uh and this is where we start to talk about getting paid in ether uh and so this is where the ether economics of being an eth validator come into play uh and overall where we'll actually start to be able to differentiate uh ethereum scaling strategy from the rest of the outlayer ones uh and so john i'm going to pick your brain on that because i know you've also written another article comparing like the overall the different strategies of outlayer ones and the the relations to their l1 assets so definitely a very fun part of the conversation which i definitely understand a little bit more than the first part of the conversation so we're gonna get there right after we talk about some of these fantastic sponsors that make the show possible ave is the leading decentralized liquidity protocol and now ave v3 is here ave v3 has powerful new features to enable you to get the most out of d5 including isolation mode which allows for many more markets to be launched with more exotic collateral types and also efficiency mode which allows for a higher loan to value ratios and of course portals allowing users to port their ave position across all of the networks that ave operates on like polygon phantom avalanche arbitram optimism and harmony the beautiful thing about ave is that it's completely open source decentralized and governed by its community enabling a truly bankless future for us all to get your first crypto collateralized loan get started at ave dot com that's aabe.com and also check out the ave protocol governance forums to see what more than a hundred thousand dow members are all robbing about at governance.ave.com arbitram is an ethereum layer two scaling solution that's going to completely change how we use device and nfts over 300 projects have already deployed to arbitrary and the d5 and nft ecosystems are growing rapidly some of the coolest and newest nft collections have chosen arbitrary as their home all the wild d-fire protocols continue to see increased usage and liquidity using arbitrary has never been easier especially with the ability to deposit directly into arbitrary through all the exchanges including binance ftx hobie and crypto.com once inside you'll notice arbitrary increases ethereum speed by orders of magnitude for a fraction of the cost of the average gas fee if you're a developer who wants low gas fees and instant transactions for your users visit arbitrum.io developer to start building your dap on arbitro if you're a d-gen many of your favorite dapps on ethereum are already on armatrom with many moving over every day go to bridge.arbitrum.io now to start bridging over your eth and other tokens in order to experience defy nfts in the way it was always meant to be fast cheap secure and friction free living a bankless life requires taking control over your own private keys and that's why so many in the bankless nation already have their ledger hardware wallet and brand new to the ledger lineup of hardware wallets is the ledger nano s plus a huge upgrade to the world's most popular hardware wallet with more memory and a larger screen the nano s plus makes it easy to navigate and verify your transactions and the paired ledger live desktop app gives you increased transparency as to what is about to happen with your nft what you see is what you sign the nano s plus gives you the smoothest possible user experience while you're doing all of your crypto things so go to the ledger website to check out the features of the new ledger nano s plus and join the waitlist to get yours and don't forget about the crypto life card also powered by ledger the cl card is a crypto debit card that hooks right into the ledger live app right next to all the default apps and services that you're already used to doing like swapping tokens and staking so if you don't have a ledger hardware wallet go to ledger.com grab a ledger and take control over your crypto all right bank playstation we are back with john from delphi we're going to talk about the last of the three strategies to get ethereum scaled computation without scaling trustlessness and permissionlessness uh and that comes to proposer builder separation uh which thankfully it's actually baked into the title about what this is uh but john what's a proposer what's a builder and what are we separating yeah so this is one of the key things that makes tank sharding possible from what the old sharding design was um but it was actually initially designed as like an mev fighting strategy that like fights the centralizing forces of it so the way that it works generally is today you have uh if you're like a validator like after the merge or a minor today you have to do two things you have to like actually build the block and you also have to say that like this thing is valid and sign off on a the realization is that like those don't have to be like the same person who's doing those and like it makes a lot of sense to split those out because the person who builds the block today it's like a mining pool operator um but like the point is it's a very difficult role to do that effectively because like one of the biggest like advantages of being the block producer is mev like uh maximum extractable value so ordering these transactions in a certain way that i can extract the most um like whether it's sandwich attacks or like whatever um the problem is is that like like i don't know how to do that on my computer like i'm not gonna be able to effectively like get mev out of this block so i'm just going to say screw it like give my money to someone else like you validate for me and then just like it's fine um we don't want that to happen we want it to be really easy that like a validator doesn't have to worry about mev so we need to just like outsource that building task and that's just to put listener the mind of the listener in the right spot i think there's a lot of listeners here a lot of viewers on the youtube who are playing on staking their eath and hopefully they plan on taking their youth from the comfort of their own home with their own computer and then they're probably hearing about like how lucrative mev is and they're probably really excited to get some of that mev when it comes to the merge the problem is like yo listener yo youtube viewer do you actually know how to get mev like are you technical or and because if you're not then you're just going to leak mev in favor of those who do know how to capture it and so if you are thinking about like oh i can't wait to get my hands on some of those juicy mev rewards uh what you need is proposer builder separation so john i'll throw it back to you yeah and exactly so this won't be implemented until bank sharding which is why at the merge um flashbots is going to come to the rescue again um and basically create which is this thing which is effectively out of protocol proposer builder separation they're going to introduce something called mev boost which is like a plug-in to your consensus client which basically replaces like your execution aspects of your client and like they will take the bids from searchers and builders who are like they want to bid it will like build this optimal block and it'll just hand it to your consensus client um the problem with this is you now have trust in that like relayer um like through mbv boost who's sending you this thing that like that all this is valid and like all of it checks out and like obviously you don't want to like have that trust and like all these different things um so pbs will then actually bring all of that like building an auction like fully in protocol and that's what um allows for dank charting as opposed to the old design because in the old design um we had these 64 like individual separate shard blocks so it wasn't like that hard to produce a single shard block the big advantage of tank charting like the big difference between then and now is that we just put everything in one big block it's not sharded in the sense that like we normally think of sharding where it's like these different blockchains that all have different data everything is put into one big block the problem is that's really resource intensive like you can't do that on like a consumer laptop or anything so we realized that like hey we have this specialized role now that we designed because of mev let's just take advantage of that and make the blocks bigger because like they're the ones who are building the blocks anyway we don't need it to be like super easy to build the blocks we just need to validate them so that's what it takes advantage of is now we have this high resource builder who's going to build the block the whole thing with like the beacon chain block all of the shard data like put it all together and there all the builders are going to send like their block headers along with their bids and all the proposer has to do is just take the block that has the highest bid propose it to the network and people can take that block and they'll be able to do data availability sampling and say hey if my part was available like and the execution on like the execution chain was valid to sign off on it so it makes that proposer roll like super super easy where that builder role is going to be a very high resource like relatively specialized role okay and so going back to the theme of this particular episode which is checks and balances this is separating ethereum block production into two separate groups of people uh where you have the consumers consumer people uh people that plan on staking their eth don't know how to capture mev they probably have like a mac laptop or some like home-built pc that they just want to stake their ethon but they want to capture mev and so the proposer the pros or builder separation is it separates this consumer group into the proposers and then it can separates the builder group into the builders and the builders take all the transactions and bundle it up into a a block that's ready to go all then they already did all that computation and that block is ready to go it just needs to be approved by the proposer who is taking their eth and then the proposer says like okay i will take this block and embed it into the blockchain how does the proposer choose to select which builders block to include so they should take the one with the highest bid um is what they should do um the what you you obviously want to make sure that they are like acting honestly um one of the issues with this is now you give the builder more power um because they can censor transactions like maybe they bid the highest um but in re like they keep bidding the highest for every block but like they want to censor me because like they don't like me or whatever so the builders should never include my thing um that's where something called see our list comes in censorship resistance list so the uh proposer will put out like this list of these or all of the transactions i see in the mempool and so when the builder is like submitting their block and like it goes through they have to either prove that the block was full so i just couldn't include everyone so you obviously don't have a guarantee and that scenario or that i included everyone in there so it helps to like offset that power um but yeah i mean like all the proposer really has to do is just take the one with the highest bid and so it's outsourced all of the responsibility of collect capturing as much mev as possible to this other group of people who have to compete with each other to bid the highest um what's the incentive for the builders to be builders can you just walk us through at the basic level like why would a builder what's the value in being a builder for the builder types for the builder role yeah so they're the ones who will directly get um the mev and the transaction fees out of this block um so that gets paid to them like searchers will bid to the builders like hey i'll pay you this much to include my bundle another searchable bid to them i'll pay you this much to include my bundle and the builder will get the transaction fees out of that block and then that's why we want to have at minimum enough builders such that there's an efficient market because if there's an efficient market every builder should build should bid up to like the maximum value of whatever they can get out of that block and then so indirectly all of the value of that block then flows over to the validators um because obviously if you have an inefficient market and there's just like one super builder and i could just i could take all the money and then i could just bid whatever um but as long as you have an efficient market with like enough builders then they should be able to bid up to the full value of what like that block is so that all the all the value flows in directly to the validators which keeps like super decentralized so once we get to this point in ethereum's history where we have these builders who are building blocks what what group of people today does that correlate to like who are who are the the people in the ethereum ecosystem that will be likely candidates for being builders later on so that is gonna be um like actually a new role because the person who's playing that role today is the mining pool operators okay um the actual miners themselves like don't actually like build the blocks or do anything the mining pool operator basically they're the one who does it and then they send it to like the winning miner and like they put their proof of work and like prove that the block is good um what's gonna happen with this muv boost which is what will happen like initially after the merge um which is like this flash bots program flashbots is going to run the builder and the relayer to start so they'll be the ones taking on that specialized role and then the searchers are the ones who just submit bundles so they like they run a specific strategy and just say include these transactions like a little piece the building is like a separate process because it's not just figuring out like specific strategies it's how to take all of the bids that you were given and like optimize like with some algorithm like what's the best way to put them together um so that's gonna be actually like a pretty new role um flashbots is gonna be taking that initially okay and then looking to like kind of progressively decentralize that and allow like other builders in and then when you get to full pbs it'll be like totally permission looks like anyone can do it and like hopefully by then especially because maybe boost will decentralize like enough people know how to do this like there should be parties at that point okay so the the people that are running me v bots today are they the likely candidates to become searchers in the future the people who are running mev bots today yeah like pretty much nothing's going to change for them if you're running if you're like an mev searcher today right now you're just submitting um like to minors it like after the merge you'll be submitting through mev boost like to their builders and then like after a proposal builder separation like you'll just be submitting your bundles to like the builders and that um but all like any researchers like they for the most part run like a single or like a couple optimized strategies of like i'm just doing like arbitrage on this or i'm just doing sandwich attacks here like they're not able to build like what is all of the mev out there most efficiently they're running like two or three strategies so the builder is gonna be that new role of taking all of those things and like putting in them like into a block which the operator is doing today cool so it's like a meta searcher right and so we have like some searchers are doing are making sure like the there's a balance between balancer unisop and sushi swap and so they are having they're taking the best arbitrage opportunities there and that is a highly competitive market that will be many many searchers for and then there's like the liquidation searchers so the people that liquidate people on compound and ave and other borrowing money markets and those people have a highly complex highly refined strategy and they compete in that realm and then there's like 17 other different sources of mev in the in the ecosystem and every single source likely has its own searcher and the searchers produces bundles which bundles up all the mev that they can capture out of the the imev opportunities in ethereum and then the builders take all the searchers bundles and they bundle it up into into one single block and then then they propose that to the proposers and the proposers accept the highest bid which likely comes from the most efficient uh collection of mev from all the searchers coming from the most competitive builder going to the people that are just staking ether and so all of this work that goes on behind the scenes from all the searchers collecting all the mev and all the block builders competing to build the best blocks ultimately gets captured by the the single solo staker who's just staking eth on their ethereum node and they didn't have to worry about any of it that's pretty cool that's pretty cool yeah it's there's a lot of like steps to all these specialized parties but like that's the beauty of all this is it just completely abstracts all of that away i as a proposer need to say that's the highest bid like good simple like that's pretty that's pretty much it simple and uh something that vitalik said in his end game which you cited in your article is some essentials uh uh some centralization is needed to scale which is like you know centralization that's like a bad word in in this industry uh and additionally with this whole proposer builder separation we're going to have people on their tiny little consumer laptops and then we're going to have other people with their super computers that are competing against other people with super computers and so we do have this stratification of like we do kind of have the this alt layer one super node type setup in ethereum but it's okay in this version because of the checks and balances by this proposer builder separation would you say that's a fair take yeah you still want the builder to not be like literally one guy at like google who needs like this ridiculous computer to do it like you want it to be enough that it's decentralized enough that like you have an efficient market and that you don't have to worry about like liveness concerns that like oh it's five builders in the same place and like some regulators just gonna go shut them down um and the hardware requirements that we're talking about for these builders like it's like running a gpu and like having like like i think it's like two and a half gigabits like at least bandwidth like those are like those are like high resource requirements for a regular person but like like just on your consumer laptop you can't do but like they're not like these crazy requirements that like it's decentralized enough that like we shouldn't have to worry about with all these checks in place felt like censorship or like liveness from them and then we're just relying on the decentralized validators at the end of the day who are like they're the ones who are like ultimately still gonna like sign off on everything would you say that it goes from uh a proposer you can do on your regular like consumer laptop and then a builder you kind of need a high-powered gaming computer is that a fair take yeah yeah you would need like a gpu or like a really high-end like cpu and like a very good internet connection okay and to be a proposer like i could do it on my laptop like like it won't be anything crazy okay so those are the three things that that finishes off proposer builder separation which i do think is like the most fun and interesting part of this conversation because that's where we start to talk about who gets paid ether but i do want to like kind of compare and contrast this to other l1 scaling strategies and so at the highest of levels how does ethereum's like roadmap with all this stuff differ from the strategies of like the generalized alternative layer one how how would you classify that yeah so there's a few different approaches one approach is i would like simplify as like the sauna approach which is for the most part like you make some optimizations to the execution environment like parallelism etc um but mostly you get like a lot of your scaling out of like entire hardware requirements like you need higher hardware requirements to run a full node and that like lets you like process more transactions downside of that obviously like i can't like check uh like i can't like run like a validating note or like be a full note of solano like on my laptop like it's like it's not a reasonable uh constraint for me um so that's trying to like all right let's jam everything through one chain the other vision would be more along the lines of like avalanche or cosmos which use like subnets and zones respectively saying okay one chain is not enough we'll split it a boss cross a bunch of chains um problem with that obviously is a fragment security and you probably end up with like pretty like small like not super secure like validator sets for a lot of these things and you're still just like trusting the honest majority of them like you're not gonna have people like running full nodes and validating like all these different subnets and zones like you're trusting the oddness majority of them and you're fragmenting security across all of them and then there's like the modular vision approach um which is like ethereum celestia polygon available where hey we're going to be this base layer and then we're just going to let rollups live on top of us and they can inherit like our security and run it like really low resource requirements for users who like just need to be like checking proofs basically and from that like they will have like the full security and like the higher throughput all in this like interconnected network and uh talking about a line from one of your other reports and you alluded to this but uh with the component that i want to lean into uh you say many monolithic ecosystems such as avalanche and cosmos can achieve meaningful scale however this fragmented security approach inherits far less value capture back to the base asset it will instead disproportionately accrue to subnets and zones respectively can you talk about the value capture part of this thing and how that fits into this conversation yeah yeah so that's a big difference of this is like it's like effectively when you look at either avalanche or cosmos like they're not inheriting security from like the primary network or like the hub or anything um so there's no reason to like pay like that main network and so you don't like they run that like each zone in each subnet like they run their own validators and they get paid the fees for that chain like anything that gets paid or burnt or anything like that goes to them um that's obviously not the case with rollups um and that's where you get like a real staking yield and all of these things um like out of them not just like a made-up like steak and yield that like it's inflationary and you're giving out money um like roll-ups will continue to pay back um to the l1 like fees and at the same time like ethereum will continue to have like its native execution environment that also has like that is ultra secure and that you're getting like settlement to the l1 um like everything gets paid back to it still okay john this is a lot this is super complicated how long did it take you to actually go through and like understand all these things um so i started working at delphi two months ago um and prior so these were my first two reports um prior to that i got interested in crypto last year um like beginning of last year um basically just started like buying bitcoin when it was going up because i was like this is going up i got really interested in d5 around like april may like justin just in time to get wrecked by the crash there um and then over like summer it's like end of the year was actually when i found like you guys and like that's when i started to get like red told on like the whole like modular like vision um like the first time that i ever first like understood like okay what is the modular approach like how do these roll-ups like actually scale um was the like ultra-scalable ethereum article that you put out like end of october and like that's when i like had like oh  moment when i remember when i got to the part of that where like you start to realize like how these modular things scale is they actually scale by getting more decentralized which is like a crazy realization because as you have more of these like decentralized validators saying the data is available you can safely increase throughput even more and then like you just keep adding more and then you can add more data availability throughput which means more rollups which means more transactions and it's like this virtuous cycle um so yeah i mean like i first got into like looking at modular like six months ago and started full time like two months ago so it's all it's all out there on the internet yeah can you just explain for the listeners uh what's it what it's like to people talking some people go down the rabbit hole pretty damn fast but i would say you've gone down the rabbit hole faster than i've ever seen anyone go down the rabbit hole can you just talk a little bit about like what that's been like for the last few months for people who are like perhaps sitting on the edge of the rabbit hole thinking about like do i dive in or not and like do i get a job in crypto like how much do i commit myself to this can you talk a little bit about just like how you got versed in these things that are like like super niche like polynomials are one thing but then we can get into like erasure coding and all that stuff like i don't even understand this uh and i and from what i've gathered from you you didn't understand it either more than like a couple months ago so like can you explain for the listeners like what that's been like yeah uh like i would definitely encourage people to like if you are really interested like dive in um like i was like slow on that for a while like the first like eight months or whatever the longer that i was in crypto like i was spending like a little bit of time on it on the side but i was like like i wasn't thinking like i was probably gonna go do this as a job kind of thing um once i got into like the whole modular like like from like from you guys i then found paulinaya and that's when i just like i went through like all of his stuff over like end of last year like beginning in this year and that's when i hit the point of like okay i'm gonna go do this as a job like full-time like i enjoy doing this like after my job in banking like way too much like i need to just like spend a couple of months like really learn all this stuff um and then just like immediately start like go interviewing um so like that's what i ended up doing was like just like if you just like bust your ass on it like like like literally everything is out there and like that's what i did like i like basically read like a ton of paulie naya for like a couple months and then like i put out like a modular related post and then like started interviewing from there and like it moves really quickly like that is one of the really cool things about like crypto that is like very very different like no one really cares what your background is like if you're smart and like you know your like you'll get hired like and all of it is out there on the internet like it's it like it was a lot of fun amazing well john fantastic work on this report i'm gonna have to go listen back to this podcast again i'm gonna have to read your report again to fully understand it but again for the listeners who brains feel a little bit broken i'm with you but all you have to understand is that there's a separation of powers that puts power and keeps power in the hands of the individual the people that hold and stake ether at home and so the cool thing about ethereum is if you are just doing that that is enough john thank you so much for for diving into the very technical parts of the ethereum road mapping explaining that to us here on the uh bangladesh nation fun awesome bankless nation you guys know what to do next uh there's a link going to be links in the show notes linking to john's report both the one that we were talking about the majority of this podcast as well his as his other report comparing ethereum to other layer ones as you all know ether is risky crypto's risky defy is risky you can lose what you put in but we are headed west we're on the frontier it's not for everyone but we are glad you are with us on the bankless journey thanks a lot hey we hope you enjoyed the video if you did head over to bankless hq right now to develop your crypto investing skills and learn how to free yourself from banks and gain your financial independence we recommend joining our daily newsletter podcast and community as a bankless premium subscriber to get the most out of your bankless experience you'll get access to our market analysis our alpha leaks and exclusive content and even the bankless token for airdrops raffles and unlocks if you're interested in crypto the bankless community is where you want to be click the link in the description to become a bankless premium subscriber today also don't forget to subscribe to the channel for in-depth interviews with industry leaders ask me anythings and weekly roll ups where we summarize the week in crypto and other fantastic content thanks everyone for watching and being 