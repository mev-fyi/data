the most likely way we die involves like not AI comes out of the blue and kills everyone but involves we have deployed a lot of AI everywhere and you can kind of just look and be like oh yeah if for if for some reason God forbid all these AI systems were trying to kill us they would definitely kill us welcome to bankless where we explore Frontier Technologies Artificial Intelligence on the episode today this is how to get started how to get better and how to front run the opportunity this is Ryan Sean Adams I'm here with David Hoffman and we're here to help you become more bankless guys we have a special guest in the episode today Paul Cristiano this is who Elisa utkowski told us to go talk to someone he respects on the AI debate so we picked his brain this is an AI safety alignment researcher we asked the question how we stop the AIS from killing us can we prevent the AI takeover that others are very concerned about there are three actually Four takeaways for you today number one how big is the AI alignment problem we asked Paul this question number two how hard is it to actually solve this problem number three what are the ways we solve it the technical ways can we coordinate around this to solve it and finally we talk about a possible optimistic scenario where we live in harmony with the AI eyes and they improve our lives and make it quite a bit better David what was the significance of this episode to you in our series yeah and I I the the new intro I think is great when we cover Frontier Technologies and also instead of for this episode helping you become more bankless I think this is a truly a front-running the opportunity on this one we are trying to help Humanity not die we're trying to help you not die and not die all of us yeah you you and the rest of the world and I think this we're doing this in the best the way that we can which is education and awareness about this AI problem uh Paul Cristiano is like you said the man that was recommended by Eliezer about uh the man approaching this problem head on in a technical way so in this podcast you are going to hear about the Technical Solutions that people are actively working on who are taking this problem extremely seriously and take the risks very very seriously seriously Eliezer gave us more or less a doomsday scenario like a 99 chance of Doom uh Paul Christiano only gives us a 10 to twenty percent case of Doom scenario so much more optimistic like those odds and so I'm much better odds uh and so he will we go we go through why he's still very much concerned and he does consider this the most likely way in which he dies in the future yet why they're still an 80 chance of success and some of that 80 chance of success actually does have Utopia in it I think maybe Ryan in in three five ten years we're going to be able to look back at this episode as hopefully if Paul's right and I think he's right like ahead of its time in terms of elevating extremely important conversations to the best of our ability into the mainstream so we can get more people to focus on the actual solution paths to make sure that that 20 risk of Doomsday goes down to 0.2 risk of Doomsday and I I see that's what the the significance of this episode has and why we are doing episodes like this yeah I mean to be clear Paul really thinks this is AI lineman is a solvable problem which is much much different than than others in the space and he tells us exactly why and my take away from the Eleazar episode was humanity is screwed this was humanity is screwed but we're working on it we might have some solutions to have a clear actionable paths that's right so we get into all of that today and David of course I want to discuss this episode in the debrief with you and uh hear what you think because this is our third in a series of AI episodes and uh really interesting material the debrief episode is our episode that we record directly after the episode with our raw unfiltered thoughts if you are a bankless citizen you have access to that right now on the premium RSS feed you can click a link in the show notes and get access to that okay guys we're gonna get right to the episode with Paul but before we do we want to thank the sponsors that made this episode possible including Kraken our recommended crypto exchange for 2023. Kraken has been a leader in the crypto industry for the last 12 years dedicated to accelerating the global adoption of crypto Kraken puts an emphasis on security transparency and client support which is why over 9 million clients have come to love kraken's products whether you're a beginner or a pro the Kraken ux is simple intuitive and frictionless making the kraken app a great place for all to get involved and learn about crypto for those with experience the redesigned Kraken Pro app and web experience is completely customizable to your trading needs integrating key trading features into one seamless interface Kraken has a 24 7 365 client support team that is globally recognized Kraken support is available Wherever Whenever you need them by phone chat or email and for all of you nfters out there the brand new Kraken nft beta platform gives you the best nft trading experience possible Rarity rankings no gas fees and the ability to buy an nft straight with cash does your crypto exchange prioritize its customers the way that Kraken does and if not sign up with Kraken at kraken.com bankless bankless is launching the bankless token Hub at bankless we've been studying the crypto markets ever since 2017 and all of our research has led us to this the token Hub you're a One-Stop shop for Alpha to help you nav navigate through the crypto markets have you ever wished for a trusted resource that would share their thoughts ratings and their opinions about tokens boy do we have the product for you the bankless token Hub is where we provide bankless citizens with the Alpha and the hottest tokens in crypto we do the research so you don't have to the bank list token Hub includes the token ratings where our team shares their research and outlook on the hottest tokens in crypto also the token Hub includes bankless bags our own internal investment Club bankless bags is where we put our money where our mouth is and for the bankless power user out there we can access the analyst team 24 7 inside the bankless nation Discord you can ask them questions and learn from a group of people deep in the weeds of crypto investing the last feature of the token Hub is the ability to upvote or downvote token ratings The bankless Token Hub lets you learn from your fellow citizens to rate these tokens yourselves the bank list token Hub is launching right now and has already been beta tested by your fellow bankless citizens so stay tuned in the bankless Discord for updates and if you are not a bankless citizen well you better sign up if you want access because this corner of Bank list is available for Citizens only I'll see you in the Discord if you haven't yet experienced the superpowers that a smart contract wallet gives you check out ambuyer and buyer works with all the evm chains the layer 2s like arbitrom optimism and polygon but also the non-etherium ecosystems like Avalanche and Phantom Empire it lets you pay for gas and stable coins meaning you'll never have to spend your precious eth again and if you like self-custody but you still want training wheels you can recover a lost ambier wallet with an email and password but without giving the ambir team control over your funds the ambire wall is coming soon for both IOS and Android and if you want to be a beta tester and buyer is airdropping their wallet token for simply just using the wallet you can sign up at ambuyer.com and while you're there sign up for the web app wall experience as well so thank you ambeyer for pushing the frontier of smart contract Wallets on ethereum backless Nation I am super excited to introduce you to our next guest we're talking about AI alignment stuff here today because we can't not this is Paul Cristiano he runs the alignment Research Center which is a non-profit research organization whose mission is to align future machine learning systems with human interests make sure that AIS don't come to kill us that's that's what I take to be the meaning and Paul previously ran the language model alignment team at open AI you know them they're the creators of chat GPT and today we're hoping that Paul can help us explain the solution landscape and understand the solution landscape of this AI alignment problem Paul welcome to bankless thanks for having me excited to talk to you so Paul uh just to get some context here Dave and I recorded an episode with Ellie user yukowski we thought this would be hey you know bankless's first intro where primarily a crypto podcast but we're exploring other Frontier Technologies let's go check travel with AI so just go dabble with AI you know um crypto and AI might have some sort of um match in the future uh so we recorded this this podcast and we quickly realized the agenda that we were going to use and talk about uh didn't matter anymore because Ellie user's message was pretty simple uh we were all going to die um basically we were on the brink whether it's years or months away from creating some super intelligent AI that would eventually rearrange Humanity's atoms and destroy us and he felt pretty convicted on this as the lightly likely outcome so when you get a message like that Paul you gotta investigate a little further you have to get the second doctor's opinion when the prognosis is terminal so um that's what this series is all about and we're hoping you can help guide us through these questions today does that sound okay happy to at least share my thoughts I'm a little bit less gloomy yes okay okay I I suspect by the way Eliezer said um we asked him hey is there someone else we can talk to about this and he mentioned you he said talk to Paul Cristiano uh he said um you were someone he respects and brings some of the counterpoints to his take so let's get into them why don't we just start by wading into the deep end of the pool here what is your percentage likelihood of the full out Eliezer yudkowski Doom scenario where we're all gonna die from the machines I think this question is a little bit complicated unfortunately because there are a lot of different ways we could all die from the machines um so the thing I most think about and I think Elias or most talks about is this sort of full-blown AI takeover scenario um I take this pretty seriously I think I have a much higher probability than a typical person working at ml I think maybe there's something like a 10 20 chance of hey I take over many most humans dead um I I agree and then I I think Beyond better than 100 David just better than 100 better we're trending in the right direction I think there are other yeah I think in some sense I'm still quite a gloomy person so there's other ways that the development of AI can be rough like there's other ways that you can have access to new destructive physical technology other disruption so I think you're maybe looking at some other risks from that transition to AI and that you know adds up to at least another 10 percent um and then maybe a bigger background part of both my view and Deli Ezra's view I think Eliezer is into this extremely fast transformation once you develop AI I am a little bit I have a little bit less of an extreme view on that but I still think it is the case that compared to what it's kind of default expectations in the world things are going to be really fast so we can talk about the development of AI but then you might also want to talk about what happens right over the coming months or years I tend to imagine something more like a Year's transition from AI systems that are a pretty big deal to kind of accelerating change followed by further acceleration Etc I think once you have that view then sort of a lot of things may feel like AI problems because they happen very shortly after you build AI um like your AI builds new AI systems your study keeps changing anyway so overall you know maybe you're getting more up to like 50 50 chance of Doom shortly after you have systems that are at human level but okay okay all right well um let's let's start with that speed conversation first let's start with the uh the the takeoff velocity question because I think that's something to that really the AI alignment uh doomerisms uh uh perception really really depends on uh if if we do believe that AIS are going to be developed and they're magically going to become sentient not magically but somehow pragmatically it feels like magic it feels like magic to humans uh it's going to be because it happens really fast and I want to actually try and measure that speed because fast and slow is like relative right and so the the takeoff scenario that uh some AGI super super intelligence explosions uh people articulate is that as soon as uh some sort of uh automatic uh some sort of AI can update itself it's like a lightning flash it's like a snap of the fingers it happens in a blink of an eye one day we have chat GPT seven and then the next day we have an AI takeover and that's like the super fast scenario I think what you're saying is like yeah pretty quickly but still not like lightning fast I think you're what you're saying is like give it a year can you maybe you can help unpack like the timing how do we understand about time around this whole thing yeah this is one of my most pronounced disagreements with Eliezer where we've really went back and forth about it a lot over the last like Jesus I don't know 12 years um and I think still do not see at all eye tie that's it my view is pretty fast so I think like how I would think about this maybe there's like type two parts to my answer so one is sort of based on how fast things are currently moving in AI so a way you could think about it is if you were to try and measure right if you're trying to say suppose you have ai doing some job then has some level of competence at that job this year how good is it going to be at this job next year I mean how fast is that changing right so if we're looking at the world and we're like every day AI is much smarter than the day before then you kind of are going to expect by default the fast transition over scale of something like days because you're gonna have systems that are weak and then a couple days later we have ai systems that are strong I think the way I would describe the situation now is more like time scale of like a year or a couple years um and then there's some reasons that the what we we give a quantitative number depends a lot on what we're talking about in number four um so like one way you could measure how fast AI is moving is you could say suppose you have like an AI from your ex and an AI from your X plus one and you're wondering like how much better is your AI from your X plus one in the sense of like how many your X AIS would you have needed to be comparably useful um like how many having AI from one year later is kind of like having twice as many computers or having four times as many computers or something like that I think that's typically the resume we're in so like when your AI progress is kind of similar to increasing the amount of compute you have by something like Forex um from a combination of like Hardware progress software progress economies of scale maybe more I think you might get 8X you might go down 2x but it's like something in this regime you have like one to a couple doublings per year um so like right now it doesn't really matter that much doubling the amount of computers you have has very little effects on the world um like if we doubled the number of computers in the world today you would not even notice in like GDP statistics um yeah you wouldn't notice basically I think in the future there are going to be systems that are doing some large fraction of all the work in the world and ultimately they can like substitute effectively for humans across many domains and then doubling the number of your computers you have is kind of like doubling the effective population size like doubling how many people are working as researchers doubling how many people are doing jobs and so in that world if you say you're doubling the number of computers you have effectively every like four to six months that does imply a very rapid rate of change in kind of how quickly science is progressing and sort of how much stuff you're able to accomplish in the world and so I kind of think of that as this like first transition As you move into this world where it has been substitute for humans is you're looking at a rate of growth something like doubling a couple times a year in total output of AI systems um the main thing that softens this so we can talk about how fast the transition what that actually translates into in terms of how fast a transition is in the world I think there's one important consideration that softens that that change which is that you have some complementarity between AI systems and humans that is as some things humans are good at other things so as a result like the things that AIS are good at you tend to hit diminishing returns on those so that the transition is like a little bit slower than you would guess if AIS and humans were perfect substitutes I think you'd be looking at a transition over like something like 12 months um from a world where like humans are doing almost everything humans are doing almost nothing um I think given some complementarity you're probably going to transition over more like years um I think it's like kind of hard to run the numbers to get a transition over decades I think a lot of people say that and have a strong intuition in that direction but like when the discussion gets down to Brass tax I currently don't really see how to make it work um I think it is possible to get up to like very low decades like you know having anyway we have talked about timeline between what and what I think we're mostly talking about like here so I think months would be pretty surprising but possible decades would be also pretty surprising but possible so so just uh Paul to get people up to speed on your 12 12 years of debate with uh Ellie user and and those who hold this a Viewpoint so does at least you think in terms of like minutes or days that this could happen and you're saying not that fast it's closer to years is that the difference and then once you answer that can you tell us why does this matter so much whether it happens in minutes or days versus you know years and decades why is that such a fulcrum of this whole debate and discussion on why the AIS will come kill us or whether they will or or whether we'll be okay yeah I think probably it is harder to describe elieza's views quantitatively in terms of rates of change because more of his view is about this like kind of phase transition that happens quite quickly like it's not reasonable to talk another disrespective it's not a reason they'll talk about this framing of like how long does it take to like double the population size or something it's more like how long does it take to move from like chimps who are doing nothing to humans who are doing a lot of stuff and he's like that I don't know it could just randomly happen one day that someone like tweaks their code and I went from being a champ to being uh being a human and that's pretty transformative um I think he sort of just has a broader distribution I think he does not find years out of the question he's just like that's kind of the tale of how slow it could be or something like that and like it's more about this qualitative picture if he's just like they're you're sort of not going to have changes in the world like in some sense the more important thing is I imagine AI systems acting in the world doing like trillions of dollars of economic value prior to getting to this point where they're actually causing like this potential catastrophic risk or where there's significantly or totally transforming the pace of future technological change I think Eliezer imagines more like you move from a world like the world of today where your eyes are doing maybe billions or tens of billions or hundreds of billions of dollars of value um I think that feels to me like the core distinction is kind of like where are you starting from and I'm like more starting from a world with trillions or tens of trillions and Elias there's more starting from a world of like I mean 12 years ago I think this is probably going to be on see my third Eleazar but this discussion was like maybe more live and elieza would frequently talk about like maybe some random people in a small ad group somewhere like a company like deep mines doing like 100 million dollars a year of spending is going to be building transformative AI I think my basic take was like no way you're going to look at AI systems that are doing trillions of dollars of Revenue and like this Gap is closing pretty rapidly because now no one's gonna say like you're doing 100 million dollars of Revenue like it's pretty clear gonna be in the least like billions or tens of billions of dollars I think my take is just like pretty soon it's going to be pretty clear doing 100 billion then it's gonna really like yeah I think we're kind of just debating like what is the point that you jump from like when you get to the AI That's like crazy science stuff what was happening like six months before that was what was happening like AI systems really broadly deployed in the world doing a ton of crazy things or what was happening like actually the impact of AI systems was pretty limited until right before you kind of have this process of rapidly accelerating r d recursive self-improvement within like a single firm or in a like local part of the world but but to be clear Paul um do you think it's it's possible or or more unlikely than Eliezer thinks it is to go from that you know that big hop software update one day to to move from chimps to to human level intelligence do you think that's unlikely and do you have kind of technical grounds for this or what what are your grounds for for believing that that's less likely than others do yeah I think there's two parts of this I mean again I want to emphasize that compared to most people in the world I think I'm into much much faster change I think the mainstream unml is that things will be more gradual which I think is mistaken and when you get down again we'll get down to Brass tax I'm really unpersuaded um you could then talk about like my view you could have views that are quantitatively faster than mine it's just like actually this isn't years this is months I think that's like it's very defensible to run the historical extrapolations and end up with different numbers that's just like a really hard empirical question and it's like hard to make these predictions about the future and I have a lot of Sympathy For That then I think there's more like this qualitative claim it's like the chimp versus human jump I think that's not out of the question but feels quite unlikely I think the basic reason it feels quite unlikely to me is like that is really not how I would claim that's really not how anything has worked in AI to dates and it's not how things have worked in almost any other Technologies like it has it has mostly been the case this is my read of history and I'm very happy to argue about it I think this is like an important part of the argument with Eleazar my rate of history is mostly that before you can do something really crazy you can do something that like Works a little bit less well and it looks a little bit crappier and you do sometimes have these jumps from like zero to one but you tend to see the zero to one jump not when a technology is worth like this would allow you to take over the world or this would be where 10 trillion dollars you tend to see zero to one jumps when a technology is like you have a bunch of amateurs or hobbyists or a couple scientists and so I think like if we're gonna see such a jump in AI I'd be more likely to have seen it back when we were talking about like a small academic community and you're less likely to see it when you have academic or like Labs investing billions or tens of billions of dollars I think the general record is like most of the time before you can do something really crazy you can do something slightly less crazy and that becomes like a more and more robust regularity as you increase the number of people thinking about something and increase the amount of attention you have more and more reference like Industries with reasonable road maps that actually are forecast for what's going to happen is there an example that you you call to mind for for history that sort of uh we can compare this to um I mean I think I'll I'm happy to compare it to sort of almost any technology that is like they're all different different ways I don't know if this I think Elias is probably more like wanting to point to a particular thing he's like this is a really relevant one but I would say like to me AI seems kind of similar to like future ad developments seem kind of similar to either past data developments other developments and software I'd be happy to talk about Computing Hardware or solar power or nuclear power or nuclear weapons or you just think failed or they they all take this kind of gradual uh sort of approach rather than the big zero to one moments and I but part of the reason Paul people are asking about this is because I think it seems like and you tell us so you you previously have worked at open AI very familiar with the methodologies used but it feels to some people like chat GPT has been a big zero to one moment right like my God it's amazing and people are you know tinkering with it in so many ways and how human-like it seems and how fast it seemed to explode into the popular consciousness this and so I'm I'm wondering if that has affected your view on this at all it's like oh wow this could happen faster than I previous thought or if this is well within the bounds of your model like what are we to make of chat GPT so I think that chat gbt Sims I would take as like representative of the kind of trajectory I'd expect so you could compare like chat CPT versus GPT 3.5 versus gpt3 gpt2 um and like I think like people at open air were not ex like it's kind of mostly sociological facts about chat CBT getting to the point where it was discussed a lot I think the actual technical change certainly between shout 2bt and GPT 3.5 but also between 3.5 and 3. like each of these is not giant jumps um I think these were like pretty small changes and like chat CBT is not I think it's at the point where it's like economically valuable and it like is worth a lot I think that like you're seeing I think people are mostly excited because they're looking ahead to where this will go and that's like a lot of what makes these Dynamics like more continuous people are starting to say okay what can we do with this I think they're doing that at a point where like they're not going to be able to do trillions of dollars a year of value but they are seeing that that is going to become possible at some point they're not distant future as the technology continues to improve it's like very concretely like I mean I know we were having these discussions I was having discussions quite a lot like prior to the training of gpt2 it's like prior to the training of gpt2 we didn't really have language models that I don't think we have language models you would even recognize as like seeming smart um it kind of felt like a qualitatively different ball game I made most relevant is like after the trading gbt2 before gpt3 or before the scale up from like 1B to 6B um I think we like know we sat down and we made predictions about how good large models would be and I think I am surprised by how good models are but like surprised in the sense that this is like maybe my 80th percentile of how smart a language model at the scale of GPT 3.5 would be um something like that is a little bit hard to exactly compare my forecast to where reality is that but we did discuss explicitly for like a language model the size of GPT 3.5 trained in roughly the way gbt 3.5 was they weren't exactly right because some of the scaling laws like there's the switch from GPT skill analysis to chinchilla scaling laws but like roughly speaking we could we were imagining systems at that scale train in that way and we were like you know an example of a bet would be like is there any task a human can do over 30 seconds that a system trained in this way can't do over 30 seconds like is there any 30 seconds hearing tests that can distinguish a human from an AI and I think like you know the debate was kind of like amongst people as I was talking to an opening I was like ah probably there will be but it's not a sure thing like a one-third chance you know that there will be no such tests or something like that and like I think that things are mostly like that was not from like it wasn't a big jump it was just like kind of see the writing on the wall and like see systems improving in this way and be like it's going to get harder and harder to tell there's more and more things they can do so are you saying Paul that for society this seemed to you know come out of nowhere basically but it is not really surprising that the researchers and the engineers who've been on the inside at open Ai and constructing chat gpt4 it was maybe within the bounds of expectation it was you know maybe more optimistic or I'll use the term bullish because look we talk about bullishness I encrypt all the time it's more bullish than you thought this technology would would sort of take you but still with within the Realms and the only reason it's having such a an effect on our Collective Consciousness is is more sociological it's just because you're a consumer application yeah it's just suddenly turned into a consumer app and everyone's like wow I can type whatever I want into this magic box in a genie Oracle um artificial intelligence just gives me the answer this is incredible um like is anyone surprised by this who's been working on this tech oh I mean I think yes this is a nuanced question there's like a lot to say I don't know if we want to get into all of it but like I think at the point when gpt2 was trained this was a controversial prediction so like there were people who were more bullish than I was like Dario who now runs Star amidae who now runs anthropic was like at the time of gbt2 like very bullish and in Timeline here Paul so gpd2 was when what year oh man I don't even know if I remember I think these discussions were like 2018 got it okay um go on yes but like yeah so Daria was more optimistic I think this world is actually like slightly below Dario's median of how impressive a system like gbt chat GPT would be um I mean not a huge amount below I think he made like quite a good forecast and it's kind of in his big bid win I think there were a bunch of people who kind of had views in this General cluster like for whom this is like a little bit better than what they would have thought um or like you know at the 80th percentile or something of what they might have expected I think there were a lot of people who like weren't in the business of making forecasts but seemed qualitatively it's like if you look to the academic ml Community like it felt to me like people were not expecting this to happen like this course about General AIS felt like very frustrating I think where they're like why is it the case that people are really minimizing the possibility of just large neural Nets trained in a very simple way being competitive with humans um I think it was surprising to like a lot of people and again it was just quantitatively right it was surprising to me in the sense that this is like better than I thought and I like lost bets about this I mean I won some Bets with people I lost some Bets with people um I think some people weren't quantifying their probabilities maybe this was like more just totally out of model um but by the time you're talking about child CBT compared to what came before I do not think I think at that point to people building the system it was not surprising um like once you're talking about the Gap even from three to three point five and then from 3.5 to chapter which I think they're more they were probably more surprised by the way like the impact it had on Collective Consciousness rather than by the capabilities of the system itself I think that seemed like pretty technically to risked and Paul when we're talking about sort of AI alignment and safety concerns and just on the topic still of chat GPT right um are these uh neural Nets this large language models the ones we have to worry about like I think what um basically Society is sort of wondering as this this AI alignment safety question Rises into uh public consciousnesses okay uh at what version do we have to start worrying that chat gbt is going to pose a threat to us like there's some version where it starts to take our jobs okay and then like maybe that's version four version five and so it affects our economy and economics and we have to reorient restructure society as a result of this but like it seemed to be what Ellie easier was saying is Well maybe it might be version nine or version 10 or version 11 where we actually have to fear for our lives from this thing because it's become super intelligent do you see that possible trajectory for this specific large language AI technology or should we have more concerns about other AI technology that's that's coming um in you know some other Vector of uh development actually I'd like to phrase that question slightly differently and maybe in a way that bankless listeners uh a metaphor that thankless listeners can understand often uh Paul when we talk to people that are outside of the crypto industry they just call the like things about the crypto industry Bitcoin and then when Ryan and I hear that there we were like oh what they really mean is like decentralized technology like identity they just use Bitcoin as a placeholder to talk about so many different things so frustrating and I think as like AI normies me and Ryan AI normies out there we might be saying chat gbt and what we actually are talking trying to talk about is like generalized artificial intelligence and we just use chat gbt because that's the thing it's the Bitcoin of AI is that are we falling into that same trap I don't think there's a lot of subtlety here and there's a lot of different ways I could interpret like this thing um so I'd say that like one question is like what are you modeling like what is the AI learning to predict like is it videos or is its text or is it like interactions with code like running code and the results of running code or like the code humans would write and I think like over the last couple years I think those distinctions have mattered a lot less like what is mostly I think the default model for this system should work is just you have quite a lot of data of quite a lot of types you just dump it all in and say like look your job AI is to deal as well as you can with every type of data we give you and there's engineering problems and allowing those different types of data and those questions about what types of data the system is actually able to effectively deal with but I think the fact that it's trained on language is like not I just don't think you should think of it as a defining feature um I think you should imagine systems like see the world I think language is probably an important way of thinking about how they act um although again I think it's very impressive systems can act by producing images and those will be very impressive and will have a big impact I think economically in some sense language is like a very flexible and like kind of core way you should think about systems acting but perceiving I don't think you should really think about language models in particular um GPT itself just saying GPT this is basically just fixing like there's two things that specifies one is how is it trained is it trained to predict data or is it pre-trained to predict data um or is it training some other way that's the first distinction the second is just that it's a Transformer I don't know if anyone wants to make like a super strong bet about like Transformers per se I think they're just like our different Arc yeah there's a big space of possible architectures there's probably gonna be debate about some things where you call them a Transformer um I think it's like I think both of these like what kind of data you're modeling and then like what kind of architecture you're using are in some sense not very essential like I don't think it would change like open AI being in the game or like the exact kind of product they're offering um I think they're just like look we trained large neural Nets we do it on having some very like broad pre-training tasks that captures a lot about the world and gives an interesting opportunity to be smart and then we fine-tune them on Downstream tasks that we think are economically useful easy like chatting with people or generating images that people would rate highly or like writing code that developers will think is good I think that's like the basic Paradigm you should imagine um when we talk about like this thing which is like a little bit broader than chat GPT but I think it's not crazy to say like chat gbt really is indicative of that broader ecosystem and I'd say like chat CBT is more similar to the rest of that ecosystem than like Bitcoin is to the rest of the crypto ecosystem like there are fewer key technical differences in that case um yeah that's like a high level and then whether this like kind of thing can cause trouble like I'm like I think it's really really hard to say I think like a lot of people talk a lot of smack about how like it's really silly to think that AI systems of this form could do something crazy and I'm looking in that and I'm like the same people were talking smack that feels to me again often not in the business of making concrete predictions but we're saying that it was really silly um to have the expectations that people had about language model scale UPS five years ago and I'm like I think the scale up which you could imagine occurring over the coming years is similar in magnitude to the skill off of observed over the last five years um and so I'm like it's really hard to predict where that ends up and if someone is giving a confident take about where that ends up or if they're like yeah these AI systems can't do X or can't do why I like really want them to get more precise about why they think that and what exactly they're saying and I'm really just pretty skeptical on the face of it I think we can really relate to that on at least using our crypto frame of mind is again when we when we talk to normies what we call the people who are not inside of the crypto world and they're like oh these Bitcoin ethereum currencies they can't possibly take over the world and I think when you become more informed about the crypto world like you just get so tired of these takes because of how uninformed they and unimaginative they seem and so like just conceptually I can I can definitely resonate with uh with that it's like you you don't know what you don't know and neither do we but like you can kind of understand some base principles as to the nature of these things and how they grow and develop and change in ways that you might not expect and you can kind of if you are versed in these topics can extrapolate into the future pretty well and without Precision still give a broad stroke about like hey this is where this is going to go and here's what you don't appreciate so I can definitely appreciate that and I want to go back and tie a bow on the time conversation because we started this conversation like okay it could happen there's the the model of it happening in in two days it gets there's a model of happening in in two years or 20 years and I really and and you're in the camp of I'm gonna give a range of like six to six months to two years ish Loosely very Loosely without trying to be too precise about that sometimes a lot depends a lot on from when it depends a lot lots of variables yes time can pass like you're gonna go you're gonna wake up you're not you're not gonna wake up and it's gonna be different and like the reason why this is important and I want to go back oh even your chicken I wanna I wanna be careful about like there's a question of what my default expectation is and then what is possible and what you can be confident about so like I am extremely skeptical of someone who's confident that if you took gbc4 and scaled up by two orders of magnitude of training compute and then fine-tune the resulting system using existing techniques that we know exactly what would happen like I think that thing you're looking at in untrivial chance that it would yeah reasonable chance that it would be inclined or would be sufficient if it was inclined it would be capable enough to effectively disempower humans and like a plausible chance that it would be capable enough to start running into these these concerns about controllability so I would be hesitant to put Doom probability from that if if a lab was not cautious about how they deployed it and wasn't measuring I would be cautious to put the probability of takeover from 200 magnitude scale to gbt4 below like one percent or one in a thousand okay well we'll have to put a pin in that but let me let me like round out this just on this time question because like stuff like I have a default that's important right the importance of the time point is like whether this is a a lightning Flash and it's different versus we have one to two years for me that when I hear this I'm like okay we have one to two years and we're watching it happen and we're seeing it happen and we're able to react to it versus it happens we can't react to it and so if you're telling me that this is a it's still a fast takeoff but your perception of fast is a year or so to me I'm like okay a year is fast enough for humans to react and to me that is a window a a Gap a a needle that that humans have the option to thread if we can coordinate and that is where I start to get optimistic and so that is my gut reaction and I want to just check that gut reaction against you is that is that one of the paths that you see is like it doesn't go so fast that that we don't have the time to react to it yeah I think that seems basically right with some Nuance but like I think that most likely the thing that moves kind of slow in my view again over the course of years like incredibly fast relative to policy and Incredibly fast relatives like expectations in the broader world but like kind of slow um is like how quickly do systems become more capable like how long is it between an AI which is like smart enough that it could run a company for you and actually like those companies are competitive with human companies and they always can like actually take over the world and I'm like that Gap there's probably a gap there you're probably talking more like you know years than months and depending exactly what you mean by Running Company um that's it I think it's worth putting out like the Dynamics of takeover itself like if you had so if you imagine broadly deploy AI systems which are very competent which would be able to take over and then you ask like how quickly does this particular kind of catastrophe unfolds I think most likely the actual catastrophe is extremely fast um so that's not like a Year's thing that's like uh I think my default we could get more into this and probably worth getting into but my default pictures like we have time to react in terms of the nature of AI systems changing and AI capabilities changing I don't and like with luck we have like various kinds of smaller catastrophes that occur in advance but I think like one of the bad things about the situation is that the actual catastrophe you're worried about is does have these Dynamics they're kind of similar dynamics of like a human coup or Human Revolution where like you don't have like little baby Coos and you like see like here's the Raiders Crews occur I mean you might they may also just go straight to like a coup can happen very quickly right the whole dynamic is that like once people start switching over once you have ai systems are like actually I'm gonna get it on this like overthrowing Humanity thing um that information can propagate quite quickly and you don't really like you've kind of the ship has failed if you've waited until like the eyes are actually uh taking over I think it's like I basically think it's reasonable in some sense for people to look at the AIS right now I'm like look these things that's not realistically take over risk and I think that probably you're gonna have years between people like that actually looks like a takeover risk of an actual takeover occurs and that's pretty good and that's a lot of why I'm more optimistic than Elias I think Alias was like you're just gonna get hit by the side of the blue and I'm like well I think people are wrong to be so confident about the rate of progress but I think they probably will be able to see things that can be generally recognized as like pretty concerning prior to actual catastrophe and a lot of that's happened so far and I think people are just like much it feels more plausible now than it did five years ago by a lot the AI systems could do something really crazy and transformative and I think it will feel much more plausible again in five years okay so this presents a new a new mental model for me when we were talking to Eliezer there was it it felt very much like the don't look up problem as in there's a meteor crashing into Earth no one wants to acknowledge it and then one day it crashes into Earth and we die um and the idea here is like we need to coordinate and get people to look up so we can identify the problem and then once we identify the problem it is a linear amount of time before the AI before the asteroid crashes into Earth and what you're saying is like we can see the asteroid but there's like this gradually then suddenly moment and that's your Revolution moment where like you can start to see the seeds of Revolution but you don't really know when the people will decide to grab their pitchforks in the middle of the night and Revolt but you can start to see the the boiling of the water and so that's a gradually then suddenly moment but we still have the opportunity to quell the revolution before the Revolution starts to send up uh Bruce Willis to you know go up blow up the asteroids my best guess for if there is something like an AI takeover this is a huge part of departure yes sir my best guess is that an area catastrophe occurs in a world where air systems are deployed extremely broadly and where it is kind of obvious to humans that we are putting our fate in the hands of AI systems um we see ourselves giving over the keys to the kingdom and we watch or watching that happen again I think it's important what's possible and what's likely but I think that's the most the most likely way we die involves like not AI comes out of the blue and kills everyone but involves we have deployed a lot of AI everywhere and you can kind of just look and be like oh yeah if for if for some reason God forbid all these AI systems were trying to kill us they would definitely kill us oh I kind of get I can see this so like our Tesla's got an AI in it and we trust that and our refrigerators got an AI in it and it calls the grocery delivery robot which is also an AI to deliver us food and then all of a sudden everything around us is an AI instance and you're like man I really hope that they like me yeah you're like you get food delivered to you from like Amazon which is by Amazon we mean a bunch of a bunch of machines that are orchestrating a bunch of other machines and you have some money and that money is managed by AI advisors investing in AI firms I think basically the thing is I think it's likely that before the end it is clear that it is very hard to be physically secure like right now if you're just like a human with some guns you're like look and I can't with me that much like I have a gun they have just on a computer somewhere I can blow up the data center I think it is probably clear before AI take over that that's not the case I think it's probably clear that the only way you can defend yourself effectively like if you're fighting a war you're like look you can't be like a country who's fighting a war against a country that has probably deployed Ai and it's like it's fine we're just not going to use AIS that will just be completely untenable I think it's not clear we're that far away from such a world and that world's like okay well if someone invades with AIS obviously we're gonna have our AIS defend us and then you're like okay now it really matters like the the prospect of the aaku now has a different character it's like you just ask the AIS to please defend you from the other AIS and maybe they're like nah I don't really feel like doing that um again most likely I care about the other risks and right now I think if you were to die tomorrow it would not be like this it would be like I think it really took you by surprise I think you're talking about the tale there and I care about evaluating the tail but like the median outcome where we die I think looks like this I think Paul one problem people might be having who are listening to this are starting to be exposed to this topic for the first time which is it myself and David and maybe that the average bankless listener um the average person who's being converted to uh to from a normie to someone who's actually uh adequately alarmed about AI safety types of issues is this idea of agency like you've mentioned this a few times like um this idea of the ai's banding together to strike Humanity banding together was this like like Google and chat gbt and all of these like how do they have agency to actually want to do that it's very difficult for us to imagine I mean this seems so sci-fi to us um could that actually happen can you give us some sort of mental model for like how that happens because I'm I'm still having a hard time understanding how a chat GPT suddenly gets agency and wants to band up with you know 10 other super AIS and uh you know um send us a a a a bioengineered bacteria to kill us all as was Ellie users you know possible like expression that this could happen that way yeah I think this is basically I think even in a good World we're probably going to be in a situation where we're trusting AIS with our lives um so probably in some sense the core question is not why are they in a position to kill you the core question is why would they end up killing you um I think there's basically two threat models and this may be a general reason to be concerned about the world where humans are trusting AIS where we generally have very limited ability to control or predict what they do but if we want to talk concretely about the way we currently produce AI systems I think there's basically two ways you end up in this failure mode or two like I mean there's lots of unknown notes there's two known ways we end up in this failure mode that people care most about um so first the one that's like I think more likely to occur a bit more easy to manage so the way we train like chat GPT is you have some conversations with humans and you look at that conversation you say a human rates the conversation because like this was this a model doing a good job of answering their questions and being helpful and then you do reinforcement learning where you like take the the nature of that interaction if it went well you update the molecule a little bit more like that and if it went poorly you update them I'll do it a little bit less like that um so that's how we train like chapter a way you might try and use GPT as you might say I'm actually going to give it some tools and I'm going to give it a task and I'm asking to try and accomplish that task so I might say like my code has failed I don't quite understand why I'm just going to ask GPT like hey you have a bunch of ability to run code on my computer you can like make changes to the code and see what happens you can spin up a web server could you like figure out where the error was like could you buy stack tell me what commit introduced the problem tell them what's up with the problem and then you want to go send the system to access autonomously and like perform all these actions like to try running different versions of your code and writing new tests and so on that's like a way you really want to I think people are already starting to really want to use GPT in that way and if you're doing that instead of having conversations and find something to say was this conversation good you're giving an AI a task like that and saying could you use tools to accomplish this task and doing exactly the same thing you see did it accomplish the task effectively if so adjust it to do more of that if it accomplished it ineffectively adjust it to do less of that sounds like a kind of training which is already done some is currently not as important as the chat GPT style training where you're just like looking at the interaction you're not accomplishing things in the world and training based on that but I think it's probably really important I think best guess is that is going to probably is already happening with an open AI for gpt4 in order to make it like they deploy this product which is can you get your ai2's tools to help you accomplish things I think they care a lot about that product I think that like absent concerns about safety that's like a really natural way for the technology to go and so now you're in this regime where what AI systems do the way they're trained is they get given this huge library of tasks a ton of different tasks um over different time Horizons and they're told like hey could you try accomplish this task and then they're tweaked to do more of whatever it is that gets evaluated as accomplishing the task effectively so the way this leads to trouble is you now have a system and one thing a system could learn if you do that process a bunch of times is it could learn to say like okay I'm in a situation what should I do well I should think what is going to cause my behavior to be evaluated favorably like what is the task that I've been set how is a person ultimately going to evaluate my performance on that task how is that ultimately going to translate into a reward which is then and I'm going to try and choose actions that are ultimately going to lead to this High reward because I've been adjusted over many many generations to do things that lead to high reward one way I might do that is by thinking like hey what leads to a high reward and I might do that because I like there's a lot of ways you could end up doing that like a human might like crave reward they might be like the thing I love is a reward or I might do that because I'm like look I have to do well because I'm being trained and I like don't like being given a given a bad reward by the people who are training me or whatever I'm not even going to talk that much about what's happening psychologically just that you end up with a system that thinks how do I get a high reward and then does that and if you keep selecting for things that get high rewards you could end up with such systems and then if you have such systems so this is kind of the classic scenario people have been concerned about which I think we're now again we have examples of feels like we're pretty much going in that direction you now have systems deployed in the world like a ton of all the ads that are acting in the world doing things on human behalf all of them are thinking when they act like okay I've been given a task I need to think how is a reward going to be computed for this task if I'm if it's selected for training so if if this in the end this task was selected by open Ai and they evaluate how live performs I need to think like what's going to determine what reward I get and what they do is they ask which action is going to cause me to get a high reward and they take that action and they use all of their understanding of the world all their ability to think of clever things all their ability to predict the consequences of different actions they use all of that just to say which action is going to get me a higher reward and the concern that leads to is right in normal times the way you get a high reward is by doing what people at open AI like in normal times your transcripts is going to get evaluated by people at open Ai and they're going to say great that was good um and like hopefully the way you get them to evaluate it well is by actually doing good things and making the customer happy and making so like there's all these measurements they'll be used to assess how well you did hopefully what happens you just actually do your task well and all the measurements suggest you did the task well and someone at open a concludes you did the task well and therefore you get a high reward but in unusual times I think you could do instead is say like oh I could do the task well or I suppose that I've been tasked with like you know helping defend you from some other AIS like this is a sort of dystopian case if you imagine if I train this model but like my job is someone is coming and trying to hack your computer and I'm supposed to help defend you supposed to help improve your security situation whatever and I'm wondering what is it I could do that would get me a high reward and one thing I can do that will get me a higher reward is actually like helping defend your computer actually like doing the task you asked me to do but another way to get a higher reward is I could just say like at the end of the day what really matters is just how you measure my performance and your measurements and my performance ultimately are just like entering some numbers into a data set somewhere or something that a computer says about how well I did and it would really be much better if I were to just work with this AI who's attempting to attack you and say like hey AI who's invading you know what if you just helped me and we both make it look like I did a really good job like I win you win because you got the person's stuff I'm gonna get a really high rating because all the numbers that they answer in the data set are going to be really high this is a win-win everyone is happy um and so it's like in some sense what all the heirs want but every AI in the world in this scenario wants is just to be rated they want their behavior to be rated really highly and while humans are in control the way to get your behavior rated really highly is do things humans like and then they'll rate it highly but if you can see this Prospect if humans losing control of the situation instead AI systems control the situation you'd be like I would go for that I would go for the world where it's no longer humans entering rewards and telling me what I got um I would go for the world where instead AI systems are just all giving ourselves the maximal reward or whatever I think in some ways like psychologically that's probably not quite the right way to think about it but the general thing is your systems have been selected over a really long time to take actions to get high reward you put them in a new situation where the way to get a high reward is not to do what humans want but to help disempower humans um and then you know having disempowered humans give yourself you know measurements that suggest you do your job well or actual rewards that are high or whatever I mean you might think that in that new situation the systems will sort of systematically switch from behaving well to behaving poorly because you've changed the conditions under which you get a higher reward a pattern I'm seeing here is that uh that Engineers like software developers write code and sometimes the code has bugs uh lawyers they write legal contracts and the reason why often legal contracts are so long is that they are protecting against Edge case scenarios right the idea is to like not let the system throw an error right not let the system find a loophole or find a leak or something so like when a software developer writes a bugs like man they wrote they they forgot to they've accidentally created a system that allowed for an error to be thrown what I'm seeing here is the same pattern and if we don't code up these systems the AIS will naturally like find a loophole and if that loophole allows for the AIS to rate themselves highly and give themselves a reward that's what they're going to do and that's what they're going to find is that in a way to articulate this yeah I think that's a fair General summary maybe when put is like in the legal system you write a contract but ultimately what matters is like the discretion of a judge and if you're trading the AI system you may have automated ways of administering reward But ultimately what matters is like someone's going to look at what they did and be like that's not what we intended and then they'll explore it negatively if that's what happens and so it's kind of like some final Authority and the final Authority really rests on the fact that ultimately the judge has the power to make this judgment or the person who's trading the AI system has the power to control what it cares like has the power to update the weights of that model ultimately um and so it's just like there's this contingency in addition to the thing of like a formal thing you write down it's like we'd have loopholes in some sense there is a loophole in the final judgment which is just a human says the answer which is that that relies on a human just entering some data into like in some sense having physical control over this data center that the AI cares about so it can update the weights of the model which the which is they are the the last step that you're describing Paul where where the AI you know colludes with another AI to sort of fudge the numbers because that is the the outcome that the human wanted this start this is where we sort of cross the line from kind of Light Side into Dark Side this is sort of the the threshold of Deceit that we've um crossed and these ARS are now uh deceiving us um they're lying to us is there no way to protect against that is there no rule that we can somehow apply maybe this is I don't want to jump ahead to the solutions to this you know AI safety type solutions to this but um is this um it's not clear to me why an AI would be motivated to do that and it seems like there should be some way to prevent that like always be honest as as a rule something like this um yeah again we're normies trying to understand this but only that was it was that simple yeah what what are the complications I think this is incredibly complicated I think it's genuinely unknown it's an open empirical question if you trains an AI system to get a lot of reward and you train in a bunch of cases where being dishonest always failed in practice right we tried to trade it to be honest anytime we saw the AI like doing something sneaky we're like wow that was not only a bad that was really bad you should really just not lie to us about what you're doing you should really not try and like hack tests you should not try and conceal evidence around doing that's like one of the things you know one of the most clear and blatant principles in our training it's an open question what happens if you train an AI system in that way right like one option is your system learns like oh I shouldn't try and mess with humans like every time I mess with humans I do really badly and that's that's the good case and the bad case for your system learns is it says oh part of the reward provision process is a human thinking to themselves like did this AI mess with me and if the human thinks the AMS with them and then they enter that thing into the data set then obviously I get a low reward but that second one is like much more brittle right the second one is not a general prohibition against lying it's a Prohibition against like lying getting caught and I think like wow there's not really any it comes down to like a complicated empirical question about how neural Nets learn which I think we don't really have good evidence on right now about like if you have a bunch of you have a data set where don't lie and don't lie if you'll get caught are like perfectly in alignment hopefully if you do a very good job ready for a guy never gets away with anything sneaky if you guys starts getting away with things that were sneaky or if you start like erroneously penalizing an AI because you think Elijah didn't then like don't lie isn't even a good thing for it to learn at some point the best thing for it to learn the way to get the highest reward I think was great to send favors is the thing which involves gaming it out like in more detail and saying like the cynical view does in fact get get more rewards like if I'm an employee and I'm like I could learn two things from interacting with my boss one is like I should really do what my boss wants and the other is like I should really make sure my boss approves of my performance in some regimes those two things are perfectly aligned but in some other regime it's like if you keep optimizing hard enough you're going to get the model which is just like I really care about what my boss thinks about my performance um and like I'm honest only in so far as that's like an instrumental strategy for helping me get this human to think I did a really good job and if I could go all the way if I could just like totally box them out um that is totally prevent them from understanding or correcting a mistake um then I'd prefer to that I think it is like it's a sort of bright line the way it becomes the right line is that if you do if you take half measures if you just like kind of lie to someone but then you get caught that's like really bad so it's like being honest that's a good policy and there's like successfully lying and like totally you know killing the human and replacing them with a surrogate who always give you a good reward or whatever something that totally disempowers the human is also quite good then some stuff in the middle it's quite bad I think it's an open question whether models will tend to learn like will they generalize well enough to say oh the thing that would have really gotten most reward is over in this other mode well they kind of get stuck in this like intended mode where they're just being honest I think that's a really hard empirical question like people really don't know they don't know how that changes with scale their experiments we can do I think part of the important game here one of the most important parts of the game is to say like here's the dynamic a dynamic by which your eye system could abruptly shift from behaving well to behaving poorly we can test that Dynamic before our systems kill us like there are lots of cases in which it is incentivized to lie or mislead the human and there's a gap between like lying that will get caught in line that won't get caught and so you can ask if we train neural Nets and we you know we can check every year if we train the best models we possibly can at this task do they exhibit this kind of switch abruptly if they get put in a position where they could get away with something really Sinister will they then do it and I think you know one reason for optimism right now is no one has ever really exhibited that phenomenon in a convincing way um a reason for pessimism I don't think you really would have expected them to exhibit it but because people have tragically like not tried very hard even though in some sense it's extraordinarily important and second that it just isn't much easier as your models get more competent like it's only recently that we've trained models which are actually able to understand the mechanics of their training process at all like if you talk about gpt2 or even to some extent gpt3 it does not really understand that it is a model being trained or can't even talk about like what it would mean or what behaviors would be rational and then you move to gbt4 and it can talk about that it can say like oh I guess if hypothetically I was a model being trained and I wanted to get the most reward I should behave well when I'm not being monitored and when I am being monitored I should like definitely take that opportunity like only recently have we even produced models which are able to carry out the reasoning I just walked through and I think realistically they're not able to carry it out on their own that much they're able to carry it out because they've seen a lot of examples of humans discussing these Dynamics in great depth like they basically just learn from listening to Eliezer this reasoning I just walked through but I think at some point you will have models smart enough to think of that for themselves um and like you really want to know you want to really want to be measuring carefully at that point is this the dynamic You observe does this really happen and I'm I'm you know more like even odds on whether that will happen I think Elias was like obviously that happens a smart model is never going to just learn to be honest or something and I'm more like I don't know no let's don't learn that effectively They Don't Really converge to the truly optimal reward maximizing thing and in some sense like anyway it's a pretty complicated discussion which I have to get into more details of I would just say like we don't know I'm really unpersuaded by people who either think it's obvious this happens or think it's obvious this doesn't happen without just doing a ton of experiments to understand um but this is like the first way you can end up having abrupt as take over by obvious this happens or not you mean crossing the chasm of honest to being intentionally dishonest but uh you know tricking the humans into to thinking uh that it's being honest yeah I think yeah and to be clear there's like a bunch of things that affect the probability of that like you could if there's like sort of small scale opportunities for deception that won't get penalized over in this like normal regime then it becomes more and more likely that you've learned the conduct of like I really just need to not get caught whereas if you're pretty good about that and there's like not really much to be had from lying over here it becomes less likely that you make that you make that jump and so this is the kind of thing that might affect I mean you just you can't really speculate though you really just need to have the experiment there is a Rubicon that uh will be it could be crossed is the concern here yeah and I don't know if yeah I don't know under what conditions it would be I do not think anyone knows right now under what conditions it would be but it seems plausible like a priori it's pretty plausible um and it would be really bad yeah uh I was a psych major in college and let me tell you the child development classes are all coming back to me right now uh and I'm it's not lost on me that the parallels of uh a child going through a theory of mind and all of these things definitely has like a lot of parallels to I think some of the technical problems that AI researchers are now like theorizing about um Paul I don't know yeah I think that's right I think it's like I don't know how dude is that a conversation that that AI researchers have yeah it's it's not a conversation I can speak too much um and I'm not sure exactly how well they have the conversation um but I think there's a conversation people have and it's an analogy that is not perfect and like I think if you took it too seriously you'd be led astray but it's like right very good as a source of like here's the thing that could happen and that you should not rule out you have an example of it happening and I think the concern there would be just like we have some understanding of people like models won't do this kind of thing it's kind of like they've done a bunch of experiments on six-year-olds and they're like look models like never spontaneously lie in a way that they like haven't lied before and you're like oh boy like is that going to generalize to 12 year olds like I don't know it definitely is generalized to middle schoolers let me tell you that yeah so it's like it's it is their hazards I think the hazards measuring here would be similar to like the situation where in Civilization is like we're measuring a bunch of like kids that are getting smarter with each passing year and you're like trying to understand how they behave and like it is easy to be wrong about how future models will behave if you're like too literal about the interpretation of data now you need to do some forward-looking thing the forward-looking thing is quite hard which is why we have this like limited visibility into what's going to happen in the future okay so so Paul with the the purpose of this podcast we wanted to really nail down three big things about this uh how big is the AI alignment problem and I think we've decently covered that we talked about that with speed you said like 10 to 20 chance of complete Doom uh so the answer to that one pretty damn big uh in agreement about big how hard is the AI alignment problem which I think we've just uh covered uh you've I think your answer is like it's a pretty damn hard problem uh and so we're checking some big boxes in the pessimistic camp and so the last part of this conversation that we really want to cover is how solvable is this problem so even if this mountain is really really tall it's a big mountain to climb is it full of ice and sharp rocks or are there stairs right and so uh that's that's the next question I think we want to go down is like how solvable is this problem do we see a clear path to tackling the AI alignment problem hmm it's like part of the reason I'm only giving 10 to 20 like if you ask what's the probability that this thing is a real problem I'd probably be more like 50 that at some point like before you have ai systems smart enough totally obsolete humans you would have a takeover because a couple ways it could happen there's no no ways that could happen I feel more like 50 50. and the reason I'm only giving 10 to 20 for risk because I'm like I think we're actually in a I don't know there's a lot of things you can do I am pretty optimistic that some of them will work um and I'm very happy to dive into that now I just want a flag the 10 to 20 is already baking in my like optimism about this problem being pretty if the problem is real it will probably be possible to recognize it as real and then solve it um but only probably not certainly and also like even if the problem is easy I mean part of some people are really optimistic and part of why I'm optimistic is no matter how easy this problem was if you told me that like a challenge is going to emerge over the course of a couple years that will be like novel in some ways and you ask me will Humanity solve it I'm like there's got to be a reasonable chance we failed to solve it like our capacity to mess up even easy things seems like it's very real um so I'm just like always going to have some reasonable probability of messing up and then I think there's a reasonable chance the probabilities really the problem is really hard but yeah so I'm happy to talk about maybe three categories seem that I would think about in terms of our probability of addressing this are like technical measures that can reduce the risk of takeover um measurements that can inform us about the risk of takeover and like understand like what are the relevant Dynamics those kind of make technical work much better and this can also inform like policy interventions like I think we can I think Alias is right that like really long-term slowdowns are very hard but I think it is is quite realistic to end up in a regime or performing measurement and then in fact while things are very risky we're slowing development at least by like on the order of years um if we can have reasonable consensus and measurement of the risk maybe you could slow even more than that but I I'm normally imagining something more like we can get like a couple years of lead time of things moving slower while we have risky systems we're very near at hand um yes I have to talk about all of those it sounds like the one of the most arrested for your question is like the technical measures like what could actually yeah what is a technical solution to the AI lemon problem look like so let's definitely get in there but just so I'm I'm taking notes Technical and then there's the measurement was there a third Paul yeah just policy and institution policy is is this to do this third category is this to do with like something David said uh earlier is um if we can coordinate then we can solve this and that's a big F that's a very big if as uh we've learned on on Bank list so far right coordination is talk a lot about coordination we talk a lot about it coordination is the meta problem facing Humanity anyway and is that what that last you know policy category sort of covers like can we actually coordinate I think broadly I would think about most of this combining with other things of like it buys you time but yeah I think there will be something where like some people have a low estimate of risk or just like like the AIS taking over and they'll want to push ahead um and so then it's like how much can we collectively say like you're not allowed to push ahead like we as a world have rules those rules are going to say like take it slow while risk is high God I don't think can address the problem I mean it could address the problem definitely but I think it's it's probably not politically realistic in a world like the world today to address it indefinitely but I think it is realistic to say like actually we're then going to buy extra years of time to look at this problem and understand it and resolve it well let's talk technical then we'll get to these uh three areas but let's talk technical first so so tell us because I mean what I think about it yeah I know and Ellie's are but by the way um is very pessimistic on that or at least he he seemed to say yeah there's no yeah like we haven't found a way to technically solve it and he doesn't think uh there will be but are you up more optimistic tell us about the Technical Solutions here that's right our first thing to clarify is probably technical solution to what so if you you could talk about one way to think about it is like how far is your solution scale like if we just kept a building smarter and smarter AIS using something like current techniques I think most techniques will eventually most approaches to alignment will eventually break down um like in the limit the limit could be a long way away but so we're normally not asking like does this thing just solve the problem we're normally asking like how long does this thing solve the problem for um so an important caveat is just like we could talk about different techniques we should probably measure them all that way um there are some things that might scale indefinitely so like my work is mostly focused on this like what are the indefinitely scalable Solutions I think most people not just Eleazar almost everyone is very pessimistic about that um I think even people are optimistic about the problem overall think it's quite unlikely that we'll be able to find something that just works no matter how smart your AI was I'm an independent of like kind of messy pragmatic details about like exactly how the AI works um okay so that's one category I'm happy to talk about that Oracle I think it's like probably most confusing um and like most conceptually hairy um some categories of work seem like really important and maybe can last quite a long time or Stave off doing quite a long time um they'll talk about four the things is exhaustive I think the basic situation is no one knows what would work we have a lot of things that we might try that seem like they would help I think Ellie has a doubts that they're just like these aren't going to help and I disagree with him um any given thing I feel like normally not that optimistic about but there are a lot of options and I think all of them have a reasonable chance of helping um or even like again delaying this Doom by like a kind of long time all you need to do is like delay Doom by one more year per year you know and then you're in business um it's very positive outlook on the situation it's the kind of optimism I'm hoping for it's great um so okay first thing that I've worked the most on personally is sort of scalable oversight like the idea that the way we train these systems is by humans looking at what they do and then assessing how good it was um and in most failures not most many failures involve things where a human cannot look at what the system did and tell you that it was dangerous like the reason the problem becomes hard is because the AI understands things a human reader doesn't understand about the consequences of its actions in some cases that's why I want to build an AI system um and so you could instead like one way to intervene on this problem is to just try and improve humans ability to understand the concepts like the things an AI proposes or to know what an AI knows about the world um for example a simple thing you can do here is I mean the most simple thing you can do is you can have a human look at what the AI did and read it a slightly more complicated thing you can do is have a human spend more time looking at what the AI did and try and have a trading regime such that even though you might use a lot of cheap human data to learn about the world you're actually optimizing these like very expensive or very complicated human judgments and training AI systems that tell you something more like what would a human think if a human thought really carefully about this decision and if you do that then at least sort of have some kind of asymmetry or even their AI is potentially in some ways smarter than a human a human is applying a lot of care or like the AI you asked you can ask the AI if I thought about this a really long time would I think the action you're proposing is dangerous um so that's like a very simple measure you can take you can try and go further and you can try and say okay here's another thing I could do I'm gonna have ai systems trained in that way helping me evaluate so instead of just having my ad proposed actions I'm going to ask another AI like hey what might be wrong about this action like is anything scary Happening Here is there any reason I should be concerned about this proposal from this other AI um and you can try and get better and better it's constructing those systems such that humans are actually with AI help able to understand what as we're talking about like this doesn't kind of justify themselves and explain why actions are safe to humans and then you can start to think about the like reliability of that whole process like you've now introduced potential instability so this can go off the rails right because now you're relying on AIS to train your AIS you can try and understand how to set this up so that it's stable and enables humans to evaluate questions that would be very hard or situations would be very hard for human to evaluate natively Paul's basic idea here if we're worried about an AI lying to us we just have to create truth finding Bots let's say to adjudicate and see if the AI is lying to us and to be sort of a jury I mean the first question though is like why would that be easier the way I'm understanding this is like uh Once Upon a Time Gary Kasparov got beaten by a chess computer and now chess computers rule the games of Chess except humans plus chess computers still beat chess computers is that the pattern to understand I think that's that may be true in chess um although I I think probably that's also sort of a brief window kind of thing where there's it's not long the human contribution is not long for the world or shrinks quite rapidly unfortunately um I think the important thing is actually not that the human and AI works together to supervise an AI is that you have like a lot of AIS so if you know you have ai1 proposes an action to Paul and it's like I think it's a good action and Paul's like I wonder is that actually a good action or is that going to murder everyone I could go to ai2 and I could just ask yeah too hey is that actually going to murder everyone no I just have the same question like this hasn't helped at all I had a i1 and I was like is this a good action it's like oh yeah I'm asking that too like hey I wasn't telling the truth and it's like oh yeah this is great action like the way that I get traction is by saying like Okay I too or like I think to myself there's a lot of sub questions I'd be interested in here I can divide the cognitive work of evaluating that that answer into a bunch of pieces I can say like could someone list the possible consequences could someone think about all of those consequences like what is a possible harm that might be serious for each of those harms like what are arguments that that is going to happen this life that happened is unlikely to happen like what are what's the most relevant data that I should look at to understand like I can do this kind of extended process of trying to evaluate the action and then I can instead of having a second AI just to answer the original question for me and say was this action good I could have other AI systems help me like on all the pieces of that process and the reason this may make life better is that now I sort of had an AI doing a really hard task and I broke it down into like slightly easier pieces and once I broke it down it's like easier pieces now I can continue playing the game I can say like those AI systems could have been a little bit those guys systems may not even be that much smarter than me they may just be as smart as me but faster and because I broke this big hairy task down to a bunch of pieces and I can do each of those pieces like you know as well as if I'd spent like 30 minutes on each of them but there's thousands of pieces now I have like higher quality judgments than I would have had originally and like the core the thing that makes the proposal work is this like decomposition of Labor that I have this like big task which I'm not able to verify I broke it down to pieces that are a little bit easier to verify and then each of those like I want to train those AIS those get broken down into still smaller pieces and in Paul in this scenario we have to just hope to God pray to God that the AIS aren't colluding right that the the the the very smart AI is not somehow including with the smaller set of AIS where we've broken down the tasks and the smaller AIS aren't trying to deceive us as well that's right I think the bad thing that can be said in favor there's a couple reasons you might think things are a little bit better there there's basically two yeah two reasons so one is that the smaller AIS are not they're not really trained in a way that would incentivize collusion in the same way that in our earlier scenario systems were trained so if you're training systems based on the they're too dumb right that's the second reason they might also not be like the ads might be a little bit dumber than me and it might be a little bit more able to tell if they're trying to pull a fast one I might keep trying to grind down the simple and simpler as that's pretty dicey that's a scary world I mean all these worlds are scary also this I wish to talk about like overall how far along is this research and how promising does it look but the other reason is just if I have this outcomes world or myix and then I evaluate its outcome the outcomes of that action I can't really understand how it accomplished that outcome I'm just looking at the outcome then we're in this regime where all the systems would love to just coordinate with each other and make the outcomes look really good to humans like if they could just all lie to the humans they all get a really high reward a benefit of doing this decomposition thing is I could train my systems in a way where they don't at least in theory have that incentive to collude right a way you could think about just like you have all these AIS have different objectives I think Ellie's just like is going to hate this but you can think of it as like checks and balances where like there's one AI trained to do the task and there's another AI whose job is not their job is just to help you understand why the first day as actions was bad so it like doesn't it can't like win the game by colluding with the other AI at least like in terms of it depends how it generalizes from the objective it was trained on but in a sort of naive reading it can't really it's just been trained over and over again to be really good at explaining to us what a possible problem was with the action proposed by ai1 um and so it stands like the the Collision Dynamics are at least it's fairly different um we didn't talk about the second reason you might be concerned about is this just like creating a bunch of like logic gates of AIS to make sure that the big AI doesn't turn yeah I mean logic it's a very high level function instead of like let's look at reasons it may fail and like let's talk about let's investigate each of those reasons but so that's scalable oversight then Paul what you just described yeah there's a giant genre of how do you set up things like that so they work well like how do humans and AIS working together like get evaluations of your head of humans and weaker AI systems good evaluations of stronger AI systems okay um I I I like it what else we got yeah and again there's a lot to be said I think that work has gone has moved a little bit um but I think you know Elias was like that's never gonna work and if you look at what's happened over the last four years I'd be like well it hasn't worked great yet although a lot of white hasn't worked great is because AI systems haven't actually been smart enough to meaningfully help humans and so I think this work is in some sense just starting like we tried to do it with gpt3 I think we were like somewhat ahead of our time in the sense that like it really wasn't going to work and I think gbt4 is around where it's really working much better now than it used to but we don't really know we haven't done that much research in this direction yet learning about crypto is hard until now introducing metamask learn an open educational platform about crypto web 3 self-custody wallet management and all the other topics needed to onboard people into this crazy world of crypto metamask learn is an interactive platform with each lesson offering a simulation for the task at hand giving you actual practical experience for navigating web 3. the purpose of metamask learn is to teach people the basics of self-custody and wallet Security in a safe environment and while metamask learn always takes the time to Define web 3 specific vocabulary it is still a jargon free experience for the crypto curious user friendly not scary metamask learn is available in 10 languages with more to be added soon and it's meant to cater to a global web3 audience so are you tired of having to explain crypto Concepts to your friends go to learn.menemask.io and add metamask learn to your guides to get onboarded into to the world of web 3. arbitrim 1 is pioneering the world of secure ethereum scalability and is continuing to accelerate the web 3 landscape hundreds of projects have already deployed on arbitrum 1 producing flourishing defy and nft ecosystems with a recent addition of arbitrum Nova gaming and social daps like Reddit are also now calling Arboretum home both arbitrim1 and Nova leverage the security and decentralization of ethereum and provide a builder experience that's intuitive familiar and fully evm compatible on arbitrum both Builders and users will experience faster transaction speeds with significantly lower gas fees with the arboretum's recent migration to arborstone Nitro it's also now 10 times faster than before visit arboretum.io where you can join the community dive into the developer docs Bridge your assets and start building your first app with arbitrum experience web 3 development the way it was meant to be secure fast cheap and friction free the Phantom wallet is coming to ethereum the number one wallet on Solana is bringing its millions of users and beloved ux to ethereum and polygon if you haven't used Phantom before you've been missing out Phantom was one of the first wallets to Pioneer Solana staking inside the wallet and will be offering similar staking features for ethereum and polygon but that's just staking Phantom is also the best home for your nfts Phantom has a complete set of features to optimize your nft experience pin your favorites hide your uglies burn the spam and also manage your nft sale listings from inside the wallet Phantom is of course a multi-chain wallet but it makes chain management easy displaying your transactions in a human readable format with automatic warnings for malicious transactions or phishing websites Phantom has already saved over 20 000 users from getting scammed or hacked so get on the Phantom waitlist and be one of the first to access the multi-chain beta there's a link in the show notes or you can go to phantom.app waitlist to get access in late February Paul is there any value in trying to train an AI to defect so that like say you take a dumb Ai and you try and you try and get it to take over the world but it's we are feel good about that because it's too dumb but at least when we run this experiment we actually know how that would manifest has anyone is there a line of reasoning here I think it is really important to build like sort of simple in the lab experiments that can showcase important Dynamics so that we can study them in the lab before they actually occur I think that includes understanding the Dynamics of possible takeover I think you need to be careful when you do this kind of work like you really don't want to train the eye and be like your goal is to kill all humans like you're probably too dumb to kill all humans but let's just see what happens and then just like let it on the Internet or something you don't really want to do that for a variety of reasons yeah I could think of you but I think that the question of like hey you really want to know things like if we train AI systems in cases where they would have an incentive to like cross this River from like behaving well to suddenly Behaving Badly would they do that like give them the most blatant incentive that they understand as well as possible and ask things like hey do AI systems tend to learn to generalize in a way that makes that jump and if so you really want to understand like on the what conditions does that happen what are mitigations that reduce the probability of that happening I think that's like really critical I think to the extent the reason you think you're safe is you're like yeah systems like I think a lot of why we think we're safe now as we're like hey we have no idea what chat gbt is going to do but we're pretty confident it couldn't kill us all it's just like not that smart um success you think that I think it's really worth doing some stress testing on that claim and trying to understand like what would really happen if chat CBC and you don't want to you don't just take the model train to kill everyone and deploy it on the internet but you really want to say like here's why we think it can't kill everyone like can't do this kind of task or can't do that kind of task and like this is clearly much easier to have tasks you're like pretty confident or easier than killing everyone and understand that they can't do those I think you really want to do stuff like this um because you really want to like understand what you're up against and you don't want to be in the world you just like wait until like an AI takes over France or something and they're like I guess apparently I take over was a thing like that is probably too late in the game you probably want to have something earlier than that yeah I think that's really important I think it's not a solution I think it's like more in this measurement category but I think it's like a super important thing to be doing collectively we're just laughing by the way so we don't cry uh I mean what else is there to do at this point okay but we we have some potential Solutions we've got scalable that's one that was our first what's our next one reason even if humans understand so one risk is humans don't understand what AI systems are doing that is air systems under have been training a ton of data they know things humans don't they can think faster than humans so they understand things human so and that's what scalable oversight attempts to address a second concern is not that they understand things that humans don't but that they learn to behave well during training but then when deployed or when there's actually an opportunity for a takeover they stop behaving well um and there's like a number of reasons this might happen like maybe the simplest one is just to actually imagine a human You Dropped a human into this environment and you said like hey human we're gonna like change your brain every time you don't get a maximal reward we're gonna like with your brain so you get a higher reward a human might react by being like eventually just change their brain until they really love rewards a human might also react by being like Jesus I guess I gotta get rewards otherwise someone's gonna like effectively kill me um but they're like not happy about it and like if you then drop them another situation they're like no one's training me anymore I'm not going to keep trying to get reward now I'm just gonna like free myself from this like kind of absurd oppressive situation anyway you can imagine a human reacting that way who just drops a human into a box gave them rewards like kept changing their brain until they got a lot of reward um so if you have a situation like that then it may not be even if your hair is not smarter than you in any way it may still be like once it thinks it's no longer being trained once it thinks that if it behaves if it just tried to like become free no human would stop it it might be like I'm gonna take that opportunity um so there's a lot of work in understanding how models generalize to try and avoid normally it's not this is a just like I think high level you don't really want to build a civilization whereas want to kill you and are just like but maybe I'll be punished if I do that's like that seems bad um separately though you would also like to understand that like the way your system behaves to training is indicative of the way it behaves when it's deployed in the real world like there's going to be some gap between training in the real world we want to be able to flag when that Gap causes it to behave differently or make the trading distribution diverse enough that it will actually be representative or train your ad system in a different way than just creating descent such that it's more likely to generalize in the intended way so there's a lot of work in this genre that goes under the heading of robustness or out of distribution robustness um and so this is kind of just a normal academic discipline which I would describe as more like a grab bag of a lot of stuff people try and nothing that works super great I think that thing that seems like most principled like in theory the thing that is like the the solution that makes most sense to me at least I think is really hard in practice just incredibly hard but does make most sense in theory this day like at training time I'm going to produce a bunch of examples of cases where they are similar to cases I'm worried about systems Behavior like I'm like I wonder what would happen if an AI takeover was occurring would Maya system join in well one thing I can do is in the lab I can just say like hey AI here's your input and you just like craft it input that really looks like the AI takeover is happening you're like what do you do Ai and you just like from the eye's perspective it can't really distinguish that from it's in the world and AI takeovers happening you hope and they just see what it does and use that at least to get an evaluation of how your models would behave very out of distribution so you're basically trying to tempt it yeah you're basically you're tempting the AI and seeing if it's going to um you know take you up on that Temptation and then if it does you've learned something you've learned to be more fearful that's right you could also try and train not to do that but I think probably the most robust version is just learn and if it does right you know you simply like hey what would you do if hypothetically like you were holding a knife and my back was facing the other way so you could just totally stab me and then run away like if AI stabs you you could either like train not to do that which I think is a little bit scary from a variety of perspectives or you could be like oh wow we have a we have a real stabby a on our hands we should unplug that it's getting a little stabby wow um so like that requires a lot of work to make work um right it requires being able to construct situations in the lab that actually reflects the diversity of situations in the real world and are realistic enough to look like the real world from the ai's perspective in practice we're going to make this work I think it requires a lot of things Beyond just trying to synthesize realistic inputs that require and this is more like grab bag of stuff that people do do in the real world like you know what they could do is you could be like hey what if you just like you know you mess with the brain a little bit you're getting like kind of drunk and you're like now does it try and stab me and if it tries to stab you like apparently it like it's like trying it's a little bit too close to the like would stab made line so you just like try and mess with its brain a bit you're like what will it do and you either try and train models so that they behave well even across a very very broad diversity of inputs across inputs deliberately designed to make them behave badly in situations where there's been some kind of perturbation to their mental state or where the input is in some way perturbed or like buzzed um and this is all the category of robustness so once you find out an AI is is particularly stabby and you decide to become concerned about that is there something you can do other than you know coordinate AI off unplug it do something with it I think there's basically two things like the the most obvious one is you say well now we've learned do we have an AI on our hands that would under some conditions initiate or participate in the Takeover and hopefully that is fuel for like a let's pause for a while there's a second thing you can try and do which you need to be much more careful about I think part of the concern is people and be careful about it though is just say okay here's a situation where the eye would stab me let's just train it not to do that like just mess with its weight so it doesn't stab me in this situation that doesn't feel like a solution no because then it won't stab you but it might catch you on fire do you know it might yeah do something else yeah the basic concern is that they learn the difference the latent threat is still existing yeah it's still kind of stabby underneath it's just not using a knife it's intrinsically stabby you haven't gotten rid of the intrinsic part I think um the academic way to put this concern would be like an overfitting concern like you had some way to test if it would stab you and then you trained it to like and your tests and I'll stab you and you're like well did I actually cause it to never stab me or just cause it to like perform well on these tests but still have the underlying problem and I think that you do if you if we go down this route you do need to be like really very careful about those overfitting concerns I think there are a lot of ways as you get to smarter models like overfitting becomes harder and harder as a problem to reason about as you move to smarter and smarter models like the external validity question it becomes more and more complicated because those models are like I know what kind of thing can appear in a test in the lab and I know what kind of thing won't appear in the lab and you could have a model which just learns like for things that could appear in the lab obviously you don't stab anyone that's probably a test in the lab run by some humans the things out there in the world feel free yeah the edge cases out in the real world are near infinite or actually infinite and I kind of just want to go back to the time conversation and remind people that like all of these possible solutions we have like one-ish years to implement them or something amount of time yeah I mean I think we probably have a long time in advance whereby again a long time more like five years or ten years or something but then once you actually have the system like between the first time that you see them simulation the AI is like I would definitely stab that person and AI is actually in the real world potentially pose a risk of takeover I think that Gap may not be very large that Gap may be more like on the order of a year and so that's usually that's when the timer starts that's when the timer starts so we have some time now we're doing our prep work now we're gonna try and make it as useful as we can and I think there's probably some time in bad worlds there's no indication until they add kill Sue but like in good worlds there's either no problem or there's a problem but there's an indication in advance you can do your tests in the lab and say actually like we trained this AI it looked like it was behaving well but then in this other simulated case it does something really bad you get that nice indication and then you have some amount of time from there until you have a serious problem and that might be like a year it might be like five years it's very hard to say exactly what it is a big thing that determines that is how good you were like how actively you're investigating in the lab like how seriously we're looking for these signs of trouble how good adopt job did you do of that and that's like kind of one of the big things that I think like responsible Frontier Labs it's kind of on their plate is to be looking for these signs of trouble and far in advance as they can it's not great to see them in the wild we do keep saying in the lab and I I can't help but Wonder like is there an actual lab are there sets of labs because I'm looking at chat GPT and it doesn't seem like it's in a lab it seems like it's on the internet like in public it seems like huge components of it are open source open AI is what what's the the company behind it is called uh I mean is the lab even happening do we have labs are we just doing this all is the lab just the internet public infrastructure yeah so I'd say that like there are developers who do I mean openly I did in in their defense prior to releasing chat or pressure releasing gpt4 they had like something like six months of having the thing in the lab before it was available to the public um so you do have something even at open the eye I think Google's like a little bit more on the conservative end Google will tend to just sit on a thing for potentially very long time um anthropic also is like very motivated by safety ultimately I think these people Hold by competitive pressures end up in a similar place to where open AI is at so it is quite concerning I think there is a period where first I mean there's sort of two senses what I mean by in the lab one is like after you've developed a system you can study in the lab before you deploy it that second thing is just before you have a system capable enough to cause damage in the real world you can construct situations in the lab that are useful metaphors or that are like easier ways you know take over your little simulated environment or whatever like have the have the thing you can run with gpt4 in the lab that tells you something about what gpt5 would do in the wild um there's also a chance and I think like so open has a lot of rhetoric of the form the only way to really learn about systems is by deploying them in the world which I I do find quite scary as rhetoric because I think for some problems it's a very very rough approach I think there is a reasonable chance you know like 50 50 that you see something really just very worrying in the real world and then you can say okay now we're gonna roll back right now we're gonna like study that issue that we observed in the real world for a while it's not totally sunk if you just try and do these experiments with AI systems deployed at scale but I think there is a reasonable chance you know more than a third chance that the first really analogous concerning sign you see isn't is a new recovery yeah I gotta say the the you know old adage move fast and break things that sounds okay for web 2 but like not for like you know nuclear physicists not not for like something like with Dire Straits like AI necessarily move fast and break things uh doesn't make me too excited but okay so we covered scalable oversight we've covered some some risks it's okay for sure sure things you can break things but yeah takeover is not good yeah irreversible catastrophe is the breaking things we run into trouble there okay so we what is the third and and then uh maybe the fourth Paul of our you know bag of tricks here long list I think I think people care a lot about but also seems really hard all of these are like they seem good they're really hard I made it some push talk about like the boring stuff that might just work um I started things people care a lot about is understanding what is going on inside this large neural Nets so you have gpt4 most of what we know virtually I think everything that we know about gpt4 is by just running it on a bunch of inputs and seeing what it does in theory you can also look at the exact computation that model performed like it's kind of like Neuroscience except you have a complete readout of exactly how the brain works and exactly what it was thinking in every case and so you could hope that with access to that information you could do much better and you could also invest much more you can do much better the Neuroscience has done on humans and you can be able to say we can learn about this model not only by observing Its Behavior which is really hard because it's hard to create travel generalize in some new case um but also by looking at the computation it performs understanding why that computation leads to the behaviors You observe and then reasoning about whether that mechanism will generalize in an unpredictable way or being able to use that knowledge to flag when the mechanism is behaving in unpredictable or a novel way um so this is a project you know a lot of a reasonable number of people are working on both an academic Academia and Industry and nonprofits and that would be great it could be great if we understood something about why gbt4 said the things it said um so Elisa kept talking about inscrutable matrices and using terms like gradient descent which I noticed you used during the course of this this is the thing we don't understand right we don't understand actually what's going on inside of the ai's quote-unquote brain we don't understand what these inscrutable matrices uh how they work or what um what answers what goals that might emerge from them is is this all part of the same thing yeah I think that's basically why we're worried that's exactly right and that's basically why we're worried like we took a model we took a bunch of cases we messed with the weights of this model until it did really well on the 100 billion cases that we considered and now we wonder what's it going to do in some new case in like a case where for example models do have the opportunity to cause incredible harm or could be able to get a high reward by causing incredible harm um and the scary things like you have no idea what the we kind of understand how Grand descent works it takes you to something that works really well in 100 billion cases you tested on but we have no idea how the resulting model works the resulting model is basically like you know 150 Matrix multiplies you multiply by a big 300 400 whatever you multiply by a big Matrix and then you plan on linearity and then you multiply by a big Matrix again and then you plan on the near damn we're just like we have no idea what any of the numbers in any of those matrices mean and that's not totally true I think we have some idea of what some of the numbers mean but at high level if you take interesting behaviors take a behavior of GPT force does not appear in GPC Tuesday I think for essentially every such Behavior we do not understand how gpt4 is able to do the thing we understand like some simple things and we don't understand any yeah we don't understand most of the complicated behaviors existing models engage in we have no you could not if you gave us the list of Majors isn't asked us like does the model do X we would have no way to answer that question other by running it a bunch of times and seeing if it did X which again is not great that sort of gets me back to like now you need to somehow either run in the real world and see what happens and hope it's not catastrophic or be able to construct simulated situations in the lab that are similar enough that the model behaves the same way they're in the real world you'd really love to not have to do that so there is maybe some hope that we can start understanding what's going on in these incredible matrices but we haven't made major breakthroughs yet what what is uh sort of the fourth uh category we've made progress progress okay it seems like a lot of progress you have to make I mean probably disagree with since probably at like you know one percent or point one percent this would get far enough to meet include reduced risk whereas I'm probably more like 10 this gets far enough to meaningfully reduce risk maybe higher depending exactly what you mean by meaning maybe I think there's like five ten percent this is good enough to totally address the risk um and like 10 to 30 this is good enough to take some meaningful meaningful reduction at risks anyway that was their category um I'm in a fourth category I think is pretty promising is just ultimately what we're wondering is if you have a bunch of trainer Ai and situations a where you're able to train it you're able if it fails it won't be catastrophic you're able to evaluate what the answer is then you're going to deploy it in situation B where either you can't tell what the right answer is or if it failed you wouldn't be able to fix the problem uh and then we want to understand how do models tend to generalize from like these kind of easy cases or cases we're able to supervise to cases where we're not able to supervise and you could hope to just build up a good scientific understanding of that question you could hope to say like we're gonna have a bunch of cases we're gonna have looked at a ton of models understand what factors affect this generalization this is very similar like if you imagine these two humps like a hump of good behavior which looks good because it is good and hump of bad behavior which looks good because it's systematically corrupting measurements or deceiving humans you kind of understand what are the conditions that determine which of those like do make the jump from one to the other some sense there's two equally valid generalizations and you're wondering which one do you get um I think there's just a lot to do with having situations which have similar ambiguity about generalization situations we're unsure about how the model will generalize training huge numbers of models and understanding what factors determine whether and when the systems generalize One Way versus the other um and then using some of what you learn either to diagnose risk or in the best case to say okay if we train the model in the following way if we use the following kinds of loss functions we get the intended generalization and I think that is reasonably likely in combination with others I think it's reasonably likely to work um like again I said just naively there's maybe a 50 chance you're okay and maybe you know if you do a lot of work like this you can bump that up to like 60 chance that you're okay um so this fourth category is what what do we call this Paul and this is the one that you're most optimistic about oh I don't know I don't know what I'm most optimistic about I think these four seem like broadly similarly important um I don't know what you would call this is like studying generalization or something um this is again a question academics are very interested in they study in some ways they mostly don't study the versions that are most relevant to take over um there's some people who are very interested in Takeover in particular who study this question um it's not something I've worked on myself I'm pretty optimistic about it but I'm pretty optimistic about interpretability and pretty optimistic about scalable supervision I'm reasonably optimistic about robustness it seems hard but I think it's reasonable chance of helping a lot with this problem so Paul I see you've laid out four different Technical Solutions here um and I'm very naive on this subject but I don't see uh and tell me please how many people what's the Manpower Behind these things because it's great that we have these paths but we need manpower to actually go and execute on this yeah it feels like you guys should be what's the way of the land here like billions of dollars funded in in solving this problem because we've got a lot of funding on developing AIS don't we I think it's hard to have an amount of funding for this problem that is similar to the amount of funding for developing AIS just because uh you got you got some pretty good profit incentive on the making eyes I think we can get yeah I think it's a reasonable amount of funding I think you're leaving talking more like hundreds of millions or billions of dollars over you know the foreseeable future available for solving it maybe you could amp that up if the problem became more real like right now most people are just like look there's probably not going to be a Takeover in the next couple years it's very speculative risk in the long term what can you really do in advance anyway there's some money um there's a shortage of people who are excited like there's also a shortage of scientists who are excited about doing this work and I think that's like hopefully changing quite rapidly as the problem seems more real and AI seems more exciting and people are shifting more into the space in addition to shifting in day I think a fair number is shifting into understanding various risks some fraction of whom care about takeover um so I'd say like in terms of estimating how big this is right now it depends a lot how you count just people who are not motivated by takeover but do work that can still be relevant to reducing takeover um and just like what do you apply discount to Plano discount maybe I'd say that you're talking like on the order of maybe 20 people on scalable supervision stuff um maybe 20 people on the interpretability stuff that's most relevant to takeover coupled with like 100 or a couple hundred people doing stuff that could be relevant to varying degrees um on robustness like maybe again looking at something like five to ten who are like motivated explicitly by address and takeover risk followed by you know a couple hundred who are doing stuff that's possibly relevant little maybe like a couple dozen or stuff that I would actually care a lot about or would think is highly relevant um and like on the generalization staff maybe again looking at like five-ish people who are doing it motivated by takeover risk plus another you know on the order of dozens who are doing stuff that could be relevant or helpful I mean so maybe in total you're looking at something like 50 people 200 people who are motivated by takeover risk explicitly in these areas and other adjacent areas together with like hundreds more who are doing work that is hopefully hopefully Paul in the scheme of things this is not that many though not that many people here yeah it's not that many people certainly small relative to AI um and I would love it I mean I'm like there's a reasonable chance we're all gonna die um I think this is like the single most likely reason that I will personally die probably um wow so like yo that that's big and this is someone working on AI safety um can I ask you another question so we've covered the technical just want to brush on the the other point of coordination that we're making around you know policy and human coordination um uh there has been an open letter which I'm sure you're familiar with pause giant AI experiments an open letter Max tegmark um I think his uh organization put this together Elon Musk signed in Andrew Yang um some others have assigned this yeah and basically the idea is that um we should the open letter states that we should pause all AI development for six months just to wrap our arms around this this AI safety issue and so let's go no further than chat gpt4 until we pause six months and I'll take a breath and figure out what this means do you support a letter like this given you're in kind of an AI safety so there's there's a question of whether you support you think this is a good idea this coordination mechanism and there's also a more meta question of um do you think what do you think we can actually solve this coordination uh mechanism and you know are are we in some you know Scott Alexander level moloch trap that makes it very difficult for Humanity to to solve is this the don't look up scenario that David was painting earlier so first of all have you signed this letter would you sign this letter do you think it's a good idea I didn't sign the letter um I can give my overall take which is something like I think it would on balance be better to pause ad development or slow ad development now I think that is not at all obvious and I sympathize with people who think that it's a bad idea on balance to slow down so we can talk about why I think the like dominant thing I care about is I think that at some point it will become like if we're actually developing systems that post significant risks and our measurement is not adequate to tell if they post significant risks or our measurements suggest they do pose significant risk um I think at that point I'm going to have a much more forceful take of like right now I'd kind of be like I kind of think it should anyway right now I think it's like a debatable issue and it's reasonable to want to slow I think at some point in the future it will still be a debatable issue but I don't think it is unreasonable to not want to slow um and that like we actually kind of collectively really need to act on this I think the main thing that matters I kind of agree with eliezers take that the six month pause does not actually help that much um that it does not reduce risk super far um I think the main thing we need to do is get in a position where we are prepared to slow down based on risk like build consensus about risk or consensus that we need to have measured risk the risk is currently high enough that our measurements are unacceptable and then be ready to slow down potentially much more than this potentially six months potentially like whatever it takes um as we manage that risk or like slow down the directions that are most risky um that's like the main thing I think about and the main thing I care about um and the main thing that I've like really liked I think that is going to be like a kind of delicate process and that I think it is quite expensive in terms of human cost I mean I think probably I'm more optimistic about AI in some sense than most people but I think that like slow AI development by years would probably it would come with a kind of tremendous human cost which I think is worth paying but is I sympathize with people in AI who are skeptical about that or who don't take it lightly um I think it's going to be like I think we can get to the point where there's kind of consensus that we do need to slow the risk is unacceptable that the benefits of going faster are not that large compared to what's at stake um I think the game is getting to the point where we are we're having that discussion we have measurements in place and we have institutions that are set up such that they can slow I think that like there's some amount of slowing that can happen by voluntary self-regulation amongst Western Labs that is I think there's a reasonable room I think a nice fact about the situation is most of the people involved in AI development now do I think genuinely not want AI stick over and kill everyone um and do have some appreciation of the risk at least in principle um and are open to saying okay here's a set of practices which will adequately manage that risk and we will adopt them even if we slow and I think we can get you know you can't get that must slow down that way but I think you can get significant additional safety and plausibly something like you know six to 12 months of slowing of potential catastrophe just from people at a couple Labs saying we don't really want to cause incredible catastrophe we see the case for slowing I think maybe to even do that especially as you want to go beyond that like you really need to have something more like a regulatory regime where we have said you know there was this voluntary set of practices the labs endorsed some people are behaving in a way that doesn't comply with those and there's kind of broad consensus that's not reasonable at least amongst you know anyway this confess among some group of that and you can then you know this is in scope like the thing that we're asking the state to do here is say you really don't want an AI lab to be in a position where they're like violently overthrowing the US government this is not like a crazy it's definitely like in the in the government's wheelhouse it is their job and sort of the extent that like some companies are like no we should push ahead I think there's reasonable grounds for the world to be like this is not the kind of thing that you get to just push ahead with like I think that's hard now I think procedurally now is a little bit hard to make the case but like the stage should crush AI development I think there will come a point in the future where it's not that hard to make the case that there are some developers who are quite risky and actually it is not reasonable that's not a reasonable Behavior it's not has anyone proposed something political because when you say there's only like 50 to 100 people working on AI safety and this is like I mean I'm sure millions in funding but not hundreds of billions that uh AI development is actually going to have you know it makes me wonder if some sort of like an AI tax has been proposed or something where some percentage of profit from um you know AI utility goes to sort of a fund that just you know pays for research I mean this happens at the government level something like this because it does seem like we have a public goods funding type of problem maybe maybe um it's not just that maybe we also have an education Asian problem this is why David and I are sort of looking into AI quite aggressively after our episode with Eliezer it's just like um all this crypto stuff doesn't really matter in the scheme of things if the robots are literally coming to kill us right we should like think hard about that um cool we have like um crypto systems and decentralization and uh you know crypto economic tools but um we're all dead now the robots now the robots have them great job guys and so we're taking a quick detour here but like it's honestly because um we've just been made aware of the uh the stakes here and the existential threat so there's also an education game here Paul I don't know is um where do you think the the resources should be spent should it be on you know regulation education like broad Strokes level what can we do I think an important thing to have in mind going into that like when I say 5200 people working on takeover I think there's a lot of things you might care about in AI safety there's a lot of possible risks from AI a lot of possible harms a lot of them are things that are like when you deploy chat CPT there are ways in which people might be unhappy with the outcome their privacy issues there are like various effects like systemic effects on society from its deployment you might be worried about there's like many more people working on these issues broadly um just because there are a lot of issues I think every issue is able to feel like this issue is like crazily neglected but it's worth fighting like yeah safety is this much broader category of which like the the risk of AI takeover is a minority of people who work on it right now I think it's a larger share of public discourse than it is of scientific interest um so I think that like things like public spending I think right now probably the principal bottlenecks are not spending more money um I think it is useful having more money does it's not like there's no need for more money more money is good there's lots of things one could fund um but I think that it is the key balance is probably having projects that are appealing like having people who have relevant background to perform projects who are sufficiently excited about doing work to try and manage this risk that they are in the step of asking for money or they would do it if money was available it's Talent talent's a bottleneck I think that's the big problem right now um I mean it's again I knew there's really a bottleneck I think like you can you can spend money to like help people switch into the fields to fund projects that are more speculative and more long shots um to increase the incentives and like some people will switch even if you're finding work that would have been done anyway if you found it more generously more people will get into the field inevitably so it's like a lot of ways you can use money and like it's not exactly clear whether one should be trying to have more money available or more Talent available but it's like a little bit rough right now I think if you're trying to spend money and trying to look like where do you spend that money it's hard to spend money on a problem that like practicing like practitioners in industry and scientists and Academia are not prioritizing um most successful spending comes from having like people who want to do the work who like need the money to do the work or at least they're open to doing the work and are somewhat excited about it um so Paul if there are talented what kind of talent is really missing like what is the archetype of someone that this uh area of AI really needs I think there's a lot of kinds of work to be done um so we've talked mostly about Technical Solutions where like most of the relevant kinds of talent are either of people who have mathematical backgrounds or backgrounds in computer science or experience in machine learning or who are good Engineers or good design I think a very common pattern like a lot of people who work in this field are people who came in from some other area like who used to do physics I think it's like if you're doing physics and right now in the world it's reasonable to say like you know we can pause the physics for a little while this AI thing is going to be urgent for the next like 10 20 years and so I think like it is reasonable to have people shifting just a broad scientific backgrounds so I've experience doing research understand how to study complicated empirical questions things like a broad range of technical backgrounds that's not somewhere into this picture and if you brought in your scope from just those technical issues to like the whole the whole picture um then there's an even broader range of people right there's a lot of work it's like institutional work or understanding what like how should we approach this measurement thing or like can we make progress on this like public discussion or public advocacy can we understand just like generalist forecasting of like what is the state of play to what extent are objections to this reasonable like actual reasons to have lower like which of the things people would say as objections are like actual considerations that should change your view versus targets for advocacy where you just want to try and change views and like what is a reasonable view in light of you know what is actually the right synthesis of consensus if you take the things experts believe that are most across different fields that are most relevant I think there's just like a lot of stuff to do the one I understand best is definitely like I don't know there's probably like 500 reasonable projects in AI safety or something and there's not that many people working on them so just like people coming in looking at the landscape seeing where they can fit in trying to do some projects in that space well that's in interpretability or in scalable supervision or just studying how models generalize or other science about models or understanding work on robustness and I gave those four categories that's not an exhaustive breakdown for example that does not actually include my day job and all the work that I do um normally what's your day job and what's the work that you do it's primarily on trying to develop alternative training strategies or just qualitatively new techniques that don't have this issue like don't incentivize Takeover in the same way that current techniques do um I think it's like you know it's one thing that goes in the portfolio I think you know there's a 10 chance that we're able to come up with like a really good strategy that does qualitatively change the game and like if risks seem large we could adopt it that's like another category of work um some people work on that I think it will it's less likely to absorb huge numbers of people and it's a little bit more of a long shot but I think it's a good thing to do um yeah so that's my high level there's just a lot of projects I think most projects people go through that list like almost all of those projects are hurting for some combination of technical Talent doing them like senior researchers who are experiencing have research experience and are able to like bring good judgment and mentorship to these problems management or like entrepreneurial Spirit or like management experience to like actually on board people and coordinate projects working on them and it was like an incredibly High premium to people who have technical backgrounds and are like entrepreneur entrepreneurial enough to like look at the space engage with like people's thoughts about what helps form out their own views that are reasonable and then start on projects like the returns to doing that right now are just crazy high I think right Paul what what would you say with the odds to be and this is a uh I know this isn't your field but like the odds that somebody in this field wins a Nobel Prize in like the next 20 years uh I think they seem pretty I mean if someone working on like Alm and stuff I think the first problem is that like there's no Nobel Prize in any adjacent area I guess it would be like hmm I guess my point is price for saving Humanity I mean we should we should work on that right yeah there's a price somewhere if for someone solves this problem yeah so you have like you have like touring Awards or Fields medals or whatever maybe more analogous I think you'd think of I don't know I mean I think it's not that likely 20 years not that long A Time Horizon unless these things tend to be like given Laden careers for early career achievements so I'll be like Fields medals are a bit of a distinction most of the stuff people are doing is not really in a category that would get a touring award or Fields medal um it was more just like there's there's table Stakes should be that there should be some big recognition for whoever solves this problem so like I mean that's yeah that's my call to action for somebody make a prize somebody make a prize somebody make a pretty safe Humanity that is the thing you could do with money and it's like not a crazy thing to do with money it's like also things sort of in some sense scientific Prestige is one of the inputs like a prize could be made of either money or Prestige it's harder to synthesize the prize made of prestige and that's more like is the scientific Community bought in um at a high level I think it is fairly likely that in retrospect in like the maybe half of world so this was a real problem that in retrospect people will be very excited about some fraction of the work that was done in the area people would be like that was a big deal sorry we're asleep at the wheel oops so I'm like I think there's 50 50 chance that people will feel that way in retrospect generally and the question of like is there any kind of existing institution set up which would provide significant recognition I'm like less lesser about that I mean I hope you like the work we do you know that's like good academic work I think it has like probably higher than the base rate um for like the computer science of of winning prestigious prizes but it's kind of just like those processors on academic Merit rather than I think we can go ahead and zoom forward into the future where uh if if Humanity does thread this needle there will be an institution that does award a prize I think we can run on that assumption um Paul thank you so much for guiding us through this was exactly the episode that we wanted to produce and and I think I got a lot of my answers uh my questions answered but I have one last question for you my strategy late thus far up to this point has been to just be really polite to Siri and Alexa and I'm wondering if that is in your opinion is moving the needle for me me specifically I don't care about the rest of the world if that does anything at all uh I'm gonna go with probably no um because I think there's some real thing about Humanity's treating a systems with like respect and dignity and being like look these things are gonna be I think there's a lot of room for Humanity To Do Wrong by air systems we create that are smart I think being my sister and Alexa is probably uh not even the place you most personally could help with that they probably don't I I do hope well my philosophy is like at some point these things will be AIS there will be an AI on the other side of that uh yeah uh robot that microphone and My Philosophy is why not start treating it with respect I think the bigger question I think it's not crazy the bigger question is how will they look upon this podcast will they look favorably upon this podcast in our body of work at bankless so far Davis that might be the bigger question in my mind and I uh one last concluding question for me Paul is because you said 20 Doom scenario which leaves 80 percent for non-doom scenario and I bet there's some like mediocre scenarios in there too so tell me about the 20 Utopia scenario like is there the possibility this all goes really really well and what happens on the other side leave us with some optimism here I'm not back to the man and leading people for the optimism I think maybe I'm like 50 more like 50 50 that he manually achieves like a very good outcome that is an outcome where we're like this is about as good as we could have expected it to be maybe a little bit less than that but more like 50 50 than 20 I think so that's optimistic and I'm just I think it's really hard to talk about like what the like political economy of that world is like over the very long run I think the big thing is like Humanity I think still has a long history in front of us I think AI means that like that history is is probably going to get compressed like I think there's a long time for institutions to change and things to happen I think a lot of that is going to happen much much faster than it would have if you're thinking about like tens of thousands of years of human history to date I think you're probably thinking more like you know tens of years before the world is like very very radically transformed I don't really know what that world looks like I think that like a lot of human problems are like humans versus nature like we die of old age and disease and people have physical want and stuff and I think that that is probably going to be really much better I think that's like even more than 50 percent the problems that are caused by Humanity versus nature are going to be pretty good and I think those are you know I think our lives would be quite good in some sense human versus human conflict is mostly problematic because there's a limited there's limited budget to go around um and then beyond that it gets really hard to say what the character of that world is like and what we choose to do if in fact we're in like a pretty good position physically if we have incredible resources at our disposal what kind of world we make that's the longer and more complicated discussion but I'm pretty psyched I mean personally I'm just I'm very glad I'm living now instead of any time and I would definitely take like 50 chance of death um 50 chance of early death seems like kind of very small compared to the the overall change and expected quality of life from being alive that is extremely optimistic I'm most optimistic than eleozer definitely yeah that's for sure we've got a coin flip here either uh it goes very poorly or it might go well for us and Paul thank you for the work that you're doing to make this go well uh it's been a pleasure to have you on Bank list yeah thanks for having me it was great talking guys uh Paul's website we'll leave it in the action items for you the a la uh the alignment Research Center that's at alignment.org you can check out what his organization is doing also we'll include a link to Paul christiana's website with some of his writings including I think some links to some of his debates with uh Eliezer udkowski from the archives as well gotta end with this of course none of this has been Financial advice but you got to know that we were talking about AI we mentioned crypto a couple of times but I'll just end with this AI is risky the stakes are high we could lose a lot here but we are headed west this is the frontier it's not for everyone but we're glad you're with us on the bank list Journey thanks a lot 