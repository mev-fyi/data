Hey guys you're in for a treat uh David and I decided to release our debrief episode this is usually reserved for bankless Citizens that is the premium access to the bankless RSS feed separate episode that we release right after the episode uh this one we decided to release because I think it's really important people get the context for how we were feeling right after that episode it might be a cathartic after what you've just heard so we hope you enjoy it yeah I think um the this episode is going to cause a bunch of stir a bunch of conversations we're already seeing that inside of the bankless nation Discord uh so we're assuming that that conversation is going to also be happening elsewhere so we figured we'd add more context to our reactions to this episode and make the debrief public for once in a while uh which is a nice treat for the bank location so here we go and uh guys you can get these debriefs on a regular basis if you go to bankless.com in the top right there's a big red subscribe button click that button and you can get the bankless premium RSS feed uh in your podcast player enjoy cheers welcome to the debrief this is our episode after the episode with uh Eliezer idkowski David I didn't realize that this was going to hit you so hard man it uh it really did I you know I don't think I was ready for this this wasn't your first time going down the AI alignment rabbit hole no certainly not and I'd also read a lot of um what Eleazar had had said before listen to previous episode previous podcast heard him make the case but I think there's there's something much more visceral about this versus any other time I've read his writings or any other time I've seen him and this is like I feel like I was looking across someone who was like utterly defeated and he he said he still had some hope left but it didn't feel like that looking across to him right on our Zoom screen it was hope in the sense that like sometimes just people doubt themselves and that's an equivalent amount of Hope for him yeah this is a man who spent 20 years working on the AI alignment problem and education problem and is um as I understand one of the foremost preeminent thinkers on the subject um who is basically like it almost felt like to me like he was um throwing up his hands and saying not throwing up his hands he's basically saying what more can I do I'm just going to live out the rest of my days peacefully and um go die in the way that I think is best you know what I mean it felt like a general on the battlefield who like knows that they've already lost and the Army's about to get wiped out and and sang to the Troops like okay go die as you see fit you know what I mean like it it had that kind of feeling and uh toward the end I don't know if Eliezer was getting emotional or or like not I was wondering about that I mean how can you not it seemed like it right I mean like um just I think it's that it's also the um the sincerity through which he expressed these viewpoints like there's not a doubt in my mind that this man believes what he is saying yeah uh and I combine that and I'm like well um maybe this guy's like the vitalik of artificial intelligence or like AI alignment and he's given up you know like um what hope is there and so like I guess I was thinking we would enter this podcast and have like well here are all the ways that it could go really wrong and I think we should be more concerned about these things and there needs to be more attention but I thought there would be some silver lining lining right it's like you know like all the existential types of conversations about uh nuclear proliferation nuclear Holocaust or about like global warming or you'll pick your poison biological weapons there's always like this but if we do XYZ then like there could be a happy ending that was none of that here there was no happy ending and that's what hit me especially hard in this episode yeah and we in the agenda we're like all right what's the Bold case for AI and like which is bankless language for just saying tell us the positives uh and then there's also like tell us and then also what the warnings like the the red flags that we need to look out for and I knew that this episode was going to be like oh there's a lot more warnings than there is Blue Sky yeah but yes I was not expecting uh like hey there's no Blue Sky it's only a pit of there's only the void there's that's the only thing um and I think perhaps like one of the reasons why you're reacting to this is just because like this is the guy who's on the furthest reaches of the frontier who's clearly smart about this who's clearly thought a lot about this and is is the guy to lead the charge against this and it seems like he gave up years ago yeah and he gave up after giving a big try like you know 2015 he had the attention of the world I feel like this is when um Nick balstrom's bostrom's book came out um the book on super intelligence I believe it's called um and he like this big conference and you had like the billionaires and sort of the tech Elite the Bill Gates and the you know the um Elon musk's saying yeah this is a big issue he thought that that was the moment all right and then I feel like all of maybe the heroes or the helpers who are supposed to partner with him kind of disappointed him because it turned out all that they were interested in doing or all they ended up doing was like um getting wealthy off of new AI Pro in projects for like getting some sort of social signal boost from this and not actually doing anything to help solve the problem and so I feel like he's coming out of that too and he's just like well I guess I guess this is how we die um and you know like we know the the Peril of moloch traps and coordination failures and how intractable that they are this is the most lucky molok that I've that there is basically yeah this is actually the like other things are like oh moloch light or moloch's cousin no this is moloch as in like this is the last step there's no there's no there's no reality past this Mohawk it's like trying to stop the internet like how would you even go about doing that when there's such uh tremendous economic upside for everybody to want to continue this project called the internet um it's like trying to stop electricity it's like um I mean this this ball is in motion uh I don't know I I just I guess I just pray to God that this guy is wrong um right and it wouldn't even be him being wrong it well that's what he said the only chances that I'm wrong uh or or that he has there there's some unforeseen way of of solving this it's like a logic problem right like this is just this is just like this massive logic problem and he's come to this like maze in this Rob logic path whereas like you always end up there over and over and over again my one question is do you think he spent too much time in his own head thinking about this and has gone through like very dark paths as a result and it's kind of um no I mean I it makes and like I said it's it's like a logic problem right and so I think he's just gone through this logical path of deduction and he's come out of this conclusion and I think it's very normal for a lot of for people to not want to think it's like date very deep in our DNA is like I don't want to think about death like I don't want to think about the end of humanity I'm going to think about literally anything else and so he's just the only guy who's like smart enough to articulate the position and committed enough to the actual logical process to get there and he just happens to be the one sober guy he was like Hey all you other people who are trying to like make yourself naive in this cozy blanket of profits and revenue using AI uh like you guys are part of the problem um it's it has the feeling of like some of the early sentiments I've read of the physicists who created um the first atomic bombs right yeah like the Oppenheimer project and like this feeling of like my god what have we unleashed and like out of that um them wondering uh how long Humanity would actually last like that there's some sort of a you know even this is um band of physicists I believe grouping together who had equal concerns I I don't know the full history but of the Doomsday Clock right of like now we have to tell the world how dire this situation actually is of nuclear proliferation and how close we are to midnight on the Doomsday Clock I guess I've never really viewed the nuclear proliferation from that perspective but that that makes sense as in like if you are a scientist who just saw a nuclear bomb go off for the first time and then you kind of put the pieces together like oh soon everyone's going to have this power uh and then all of a sudden like well of course there's a Doomsday Clock because as soon as everyone has this power the odds of somebody pressing the button drops to almost certainty over time I think a large number like a decent proportion of the physicists who like were involved in these projects um didn't think we would last you know another couple of decades right but you're not making the comparison between nuclear arms and AI because like the obvious the the difference here is that in the AI example the nuclear bombs are sentient and have an agency to live yes um our expense that's that's sort of the um yeah that I mean we asked that question and he's like this is way worse um he doesn't have that motivation to create like a Doomsday Clock type of um social education apparatus because like ah what is the point I mean this is um I mean he kept giving kind of this laundry detergent analogy I think like for this idea that that um creating an AI can be used uh okay you know you can create it in AI using Garden variety ingredients right you don't have to have like you know enrich the ability well this is what what Daniel smockenberger talks about we're just like that means the means to destroy the world in many different ways is becoming easier and more accessible as technology progresses and so this is like why any and every sentient civilization will always progress towards this inevitable inevitable outcome is because we will always make technology and we will always make AI but ai's not the only thing in this category it's just like the worst it's the worst one but there's also like the ability to make like a absolutely massively deadly virus in the comfort of your own home is soon to be in the hands of everyone because of Bio bio engineering right but I always think on the con on the other side I know that's a huge threat but I'm always like but there are vaccines too and that technology gets better right this is why this is the worst one because there is no it's it's the assumption that the morality scale morality ethics of AI in humans is completely just like Divergent as in just like orthogonal of the word that Eliezer has used in other capacities it's like AIS and uh AIS where will start to create their own more Frameworks of morality and ethics and it will be insular to the AI species and it will not contain our morality and ethics it will be completely Divergent from each other and so what they think is good or bad will be on a completely different plane of existence and their plane of existence won't intersect with ours and that's the Crux that like every other technology does not have and why nuclear arms and uh generating a virus and what was the other one um Nuclear Generating a virus I mean Nanobots could be another one right it's really the morality conversation that's the separating Factor here between the AI doomsday and all the other doomsdays yeah I this that's why you asked why this hit me it's like yes I've dealt with the existential things you know like obviously different coordination failures um before but the certainty of this from a like again I mean you go back to like I've only saw it once and I was half paying attention but um don't look up movie I don't even know if I quote at that movie scene correctly anyway but like um it's just it felt like a scientist who had been spending decades trying to tell the world that this asteroid was approaching and now is that kind of the the end limit like my God these people aren't listening we actually as a species don't have the ability to to coordinate and solve this one and figure it out and um yeah that was really like depressing like of the level of an asteroid is careening towards Earth and we are doing nothing about it that's why it hit me so hard did I hate you like is it is but you see no I I've done this before and I I not to say that you that you haven't because you said you've gone down this route before I thought so this is this isn't new for you but like yeah I remember like listening to Elias around Sam Harris's podcast way back when and I was like I was like painting my dad's house uh it was like my summer job and I was listening to it and I was like I was going through the accidents from 2018 right yeah and as I go into the existential crisis I was like oh this is bad like oh this is really bad but I need to you know get myself through like physical therapy school and like and I and so what am I I'm still gonna do all the same things I am going to do tomorrow as a result of this information and so like how is it going to impact your life like you're still gonna go pick up your kids still gonna go kiss your wife good night you're still gonna do the bankless podcast like what you're gonna do about it yes but like I I was more optimistic that we'd have a shot at like persisting past the next 100 years uh then I was coming out of this episode are you gonna are you gonna turn into like an AI Doomer and is that what Eleazar is um you know is he an AI Doomer is that somebody is that like how would you dismiss him like is that a dismissal of his points uh or is that just like a I don't know I guess maybe you're you're being more stoic about it than me right now which is like well you know if it's been a nice ride anyway you know I guess if um if Eleazar is is right um I mean you could drop everything and start and we could turn the bankless podcast into the AI alignment problem podcast and we could start to fight that fight if you wanted to that's something that we could do I just um I guess right I guess if you want to start to work towards solving this problem because like it's basically you go about living your life as is and just enjoy the fact that you're alive in the first place or or you or you turn into an AI Doomer and you're like you build your underground bunker for when the age there's no underground bunker I think I would probably just like enjoy it like life right now or you just turn your entire life into following eleazers and like start to join that coordination Group which I think I totally suggest that we do but I still kind of want to do the more no normal business things as well I mean like I I think that um I mean maybe in some way crypto right I mean we talk about solving coordination problems I think we're nowhere near to like solving the coordination problem of artificial intelligence in fact this is part of the content we couldn't get to bankless listeners because it seemed so pointless we wanted to ask him questions like um crypto now that we've created this programmable money system where the robots get bank accounts right and not only do they get bank accounts they can actually build Banks themselves have we as crypto just empowered this artificial intelligence we wanted to ask them questions like that and like but it just seemed so pointless by the time we got to it it's like of course this answer would be yes but if you know you would have to he would have to be a different personality he's like he would have to put on the hat like oh you want me to be the AI bullish person let me take off my actual hat and put on my fake yeah hat which is my AI Bowl person and then I'll be a fake person in order to act out he was not fake at all in this conversation that's what that's one thing I'll say is like I expected him to be like yeah you know you crypto people should be careful with what what you're doing there are some good things and some ways you can raise money to fight this fight or solve coordination problems in other ways that could be advantageous but um in other ways you're creating infrastructure to like you know increase the the power and decrease the timeline through which an artificial general intelligence can come destroy us um I expected that but like by the time we got to it it's just pointless because I already knew what his answer was going to be it's like yes and it doesn't matter it's like that there's just a there's a nihilism it went past like an absurdism of like um yeah we're screwed let's laugh about it like Rick and Morty style it it got to like oh man this is heavy [ __ ] yeah like anyway that's how it hit me I don't know if it's gonna how it's gonna hit the listener some of you guys might be listening to this and be like I know much more about artificial gender intelligence I know about the counter arguments to someone like uh Eliezer is this even a art a technical artificial intelligence question this is not about coordination failure it's a coordination and morality and philosophical question yeah that's why it hit me hard it's not about the details of AI it's just like the concept of AI is just the one of the pieces of the puzzle yeah I mean do you think that there's any possibility that um he is complete like I know there's a possibility do you think it's here's my bowl case that I think maybe perhaps Elias or might also agree with for how we still exist okay uh we make the AI that he thinks that we're gonna make and the A and the AI just does not give a [ __ ] about us well I've tried to impose that you ants can just have your Earth and we and you guys are making it marginally difficult to harvest your resources as resources elsewhere so we're going to go elsewhere if you guys are become the hardest resources not no longer the hardest resources to cultivate we'll come back but right now we'll just go grab Mars so just like it just like blasts off into this it's just like distracting yeah see you later and we get like a few more generations to live before they come back and then eat us or do they yeah maybe they don't need us maybe they don't care maybe they have other things to do um yeah I I mean there are there are these possible outcomes yeah it that's maybe part of the follow-up q a of just like it still didn't totally make sense to me that um the AIS would be like Auto evil we want to mail everyone a bacteria that's going to destroy every single human being and rearrange their atoms like like maybe it just the default is ignore if you're not getting in my way if you're not gonna shut me down if you can't shut me down then see you later I'm gonna blast off and like go explore the rest of the Galaxy I don't know so here's my question for the hopefully incoming q a session with eleazer so we have to train our AI models right we have to we have to train them on data what data do we have the internet where did all the data come from on the internet it came from humans so don't we actually imbue our culture and who we are as humans into AIS that way and like even though it's not technically part of the code as to how to learn values and morals won't they just like absorb it just because that's where their data is coming from yeah I mean that that is people's like sort of the argument of like why can't it be a a gentle parent to us why does it have to be like why can't it kind of um be some sort of like a father figure for Humanity and be like well it will literally have our DNA and it will not literally well no it won't but like it could have our mimetics in it it could yeah right um I I mean we could ask that question again um I feel like we proposed that and he answered it maybe he wasn't in the headspace to kind of like all right answer it in more detail or maybe the question has to be constructed in a different one or maybe I just like pepper like I think there were times where I overwhelmed him with questions I said this in the intro and um just like it's just a style he well he choose your own question his your style is like here's a bunch of words it's it's all collectively A vibe respond to the vibe which is actually good podcasting but that's how I've learned to ask questions and many many people uh work with that style of things we're like you overload them with cope questions but they get the vibe and they already know what they want to say or they just choose which one to answer exactly yeah they well most guests just want like I think I've been on the guessy more than you have but like you almost just you just pick the answer that you want to give you already you know what you want to say anyways it doesn't matter what the question is yeah he is not like that he's very process oriented and logical and he's like you sent me three uh queries like a computer questions and like it's too many queers same time it's like right don't DDOS me bro exactly yeah um but yeah that's I you know that's kind of a style thing aside but like uh yeah as far as the substance um I don't know I feel like this this list of people he mentioned um Paul Cristiano Aja cultra Kelsey Piper Robin Hansen I've heard of Robin Hansen I don't know Hanson he wrote this book elephant in the brain which is one of my all-time favorite books right um yeah we spoke at East Denver 2018. do you think that this could be like just um somebody listening this is like oh cute the crypto guys are like interviewing an AI person they're all scared isn't that cute it's their first time and they're like really good answers for why we will not be destroyed by artificial general intelligence no no because crypto sorry thanks your crypto like we're pretty we're futurists in crypto like if we don't know it's not like we know nothing about it I thought we knew a little bit like we knew more yeah like I've I mean I think a lot of people will listen to this podcast and be like that is complete BS like I'm talking about not crypto people I'm talking about like normies oh right the list of this and be like what is he talking about I'm not afraid of Siri like see you later what a crackpot um but crypto people we're like totally into this like we understand right uh I kind of always it was I was trying to think about that while making the agenda for the podcast like okay we've never done intentional AI content on the podcast but I'm not going to assume that the average bankless listener like doesn't know about the alignment problem I'm gonna guess that like 50 of at least 50 of bankless listeners already knew about the alignment problem going into this podcast yeah evil AI coming to kill us and we can't teach at morality and it just gets super intelligent and then right we didn't even use the paper clip um and I'll do the idea of that exactly yeah you construct some general intelligence to create a paper clip Factory and what it ends up doing is as a byproduct turn every atom in the reachable universe into a paper clip uh including all the rest of humanity and this was like a now and I think a device actually created by eleazer which is like another fun fact this guy is like thinking about this stuff for a while yeah this guy is his he's inside of a lot of uh conversations doesn't really say AI for sure it's heavy man it's heavy are you good I just like are you processing something I'm good I'm fine yeah I I'm yeah I'm generally like stoked about these things yeah you you seem like I need to kind of check on you tomorrow morning dude I was like I mean like I was like um getting worked up in that episode a little bit like a little bit like yeah um wow [ __ ] like yeah it is that that was the prognosis for Humanity it's fatal yeah I guess I I haven't thought about this stuff in a while um don't let me drag you down dude uh keep your Vibes up okay don't let me drag the rest of the nation down um hey we can turn this into an AI um alignment podcast if you want I'm not smart enough to do that um but I actually you know what it goes back to I don't think it's an AI thing it's just like a Phyllis philosophy and awareness thing yeah we are really good at educating like and we can just like in include in the intro welcome to bankless money in finance also remember to talk about the AI problem and get everyone on board with the AI alignment problem and now into the episode we got 20 years left at best 2 to 20. 20 years he had held in my 20 years and things to do I guess it's heavy um all right well maybe he's wrong though can we say that okay here's one thing I'll say that's always like I'll they've always kind of shelves like yeah there's like it's a long tail of like maybe the Black Swan Works in our favor this time we uh so vitalik has given to eliezer's um Institute um in the past and so uh we reached out and just as we do we asked kind of our close contacts of like hey are there any you're a big brain here we're having this big brain on can you tell it like what would you ask them if you were us and um fatality I don't think you'd mind a saying he's like well just stay away from topics like um you know like the centralization problems of artificial intelligence because right because eliezers well past that he's like he doesn't care about essentially he wouldn't have let us go there yeah he's like he wouldn't he would have immediately said who cares about Century exactly exactly so so okay so we didn't go there and we weren't silly enough but like the Talk's comment was um uh eliasers probability of Doom is probably like a a 0.9 90 probability of Doom actually after this episode I think it's like a 99 Point um 98 plus yeah um and then he said but my probability of Doom is probably a 0.1 so 10 odds I like those he sent point he's he's at point one that's what he said um so I like those odds a lot better and vitalik is also very smart and I want to know why that Delta exists and now he hasn't spent his whole life on um artificial intelligence obviously and AI safety so you know like maybe there's a kind of a Delta it's like I wouldn't trust eliezer's um opinion on on all things crypto of course but um I don't know maybe there's some hope there there that there are like I guess we need to get other opinions is what I'm saying um before we kind of and I feel like that's one thing I'd like to do I don't want to turn this into an AI podcast but like I want to hear something before we do I need a second opinion hope David I need somebody to come on so I think vitalik's like the actually the best person to do this because like once again this is not an AI issue this is a philosophy thing and who else other than Ai and then vitalik well he's pretty optimistic you know in general but um that's one a question I didn't we didn't have time for with Eliezer I wanted to ask like were you always so pessimistic or what I I was worried that that would almost seem so too prying but I guess that retrospectively could have been a good question I think he was fairly odd been I was just like I was a little I'd say worried about his mental state but it was just very like um yeah dude it was very down I was just curious about like okay since you've so uh convictedly come to this conclusion like what do you do with your day we said he's unsabbatical that was other things music I might come back into this have some more at Value dad that's why I have time for podcasts look maybe he's just burnt out with it maybe uh I don't know man that's all I got uh I I need to let this episode percolate sleep on this one and uh I I apologies to everyone listening that if we accidentally gave you an existential crisis we try to keep things light not beat and optimistic and bullish ultimately we are bullish under Humanity we're the most bullish people ever and that podcast is like is one thing like damn you there's no way to be bullish about that right you know how do we turn this how do we flip this one how do we spin this narrative not even us um there's another uh subject matter which I can't remember maybe it was Eliezer on Sam Harris but you talked about how knowledge is discovered like what knowledge is and how knowledge kind of exists even without a form factor to hold it in does that make sense yeah as in like um uh and and so like there's a lot of knowledge out there that humans don't know there's a big gap between where we are versus where we will be when AI comes um 10 20 years is a lot of time um yeah just remember because there's a lot we don't know Eleazar is not an all-knowing being either and like he is um yeah ultimately he's one smart person who's taken a close look at this and come back to sparing they're you know other people who I assume come back more optimistic so um yeah I guess there's that yeah well uh that's it uh accidentally gave myself an existential crisis we'll try to record a podcast but uh I'll bounce back tomorrow for state of the nation we'll get some recording done we'll be back on crypto topics and I'll just forget this ever happened huh okay I'll get Logan to make a make a PO app I accidentally gave myself an accidental crisis while doing it [Music] all right well um check in on me tomorrow and uh let's talk about it then uh Banks Nation hope you enjoyed the debrief I guess we just keep on doing this because why not oh because why not 