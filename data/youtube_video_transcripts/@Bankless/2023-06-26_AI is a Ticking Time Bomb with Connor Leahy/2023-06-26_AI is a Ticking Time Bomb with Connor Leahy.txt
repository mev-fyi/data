even if you your listener don't totally buy the existential risk thing you know maybe you don't buy it's fine but it is the case that the leaders of all the top Labs anthropic Deep Mind Over have been on the record saying clearly that they do think that there is a realistic possibility that these technologies will kill literally everybody and they're doing it anyways welcome to bankless where we explore the frontier of Internet money and internet Finance this is how to get started how to get better and how to front run the opportunity this is David Hoffman here without my co-host Ryan Sean Adams but regardless we are here to help you become more bankless today on the episode we're talking AI alignment and AI safety once again we're talking to Conor Leahy the CEO at conjecture a mission-driven org trying to make ai go well we wanted to do one last episode on the AI alignment and AI safety conversation because we think Connor can really deliver a very compelling and easy articulation as to why AI safety is real and why it needs to be treated as such some Main benefits and takeaways that you're going to get from this episode first the intuitive arguments behind the AI safety debate the things that you can take to your friends to convince them that AI safety is a real issue second the two defining categories of ways AI could end humanity and third the major players that are playing in the race towards AGI and why they all seem to be ideologically motivated rather than financially motivated fourth why the progress of AI power is based on two exponential curves and lastly fifth why Conor thinks government regulation is the easiest and most effective way of buying US time here on Bank list we've had the AI alignment AI safety conversation a handful of times with different players from the industry ever since we had that Eleazar episode which we were hoping would have been an AI crypto conversation but turns out it was an AI is going to kill us all conversation we're bringing on Conor Leahy in June of 2023 a number of months after we first went down this rabbit hole because the world of AI kind of feels the like the world of crypto in 2021 it is moving so fast and so Conor gives us the lay of the land a snapshot in time of the AI alignment conversation as it stands here in late June of 2023 as well as also articulates in the easiest and most simple terms possible why AI alignment is such a big deal if you bank with listener are not convinced of AI alignment going into this episode and you remain unconvinced after the end of this episode I have nothing left for you and so let's go ahead and get right into that conversation with Conor Leahy but first a moment to talk about these fantastic sponsors that make this show possible especially Kraken our preferred crypto exchange for 2023 assuming that we get past the AI alignment issue we will still need to buy crypto assets using our Fiat dollars so perhaps use Kraken to get that done there's a link in the show notes to get started Kraken Pro has easily become the best crypto trading platform in the industry the place I use to check the charts and the crypto prices even when I'm not looking to place a trade on Kraken Pro you'll have access to Advanced charting tools real-time Market data and lightning fast trade execution all inside there's spiffy new modular interface kraken's new customizable modular layout lets you tailor your trading experience to suit your needs pick and choose your favorite modules and place them anywhere you want in your screen with Kraken Pro you have that power whether you are a seasoned Pro or just starting out join thousands of traders who trust Kraken Pro for their crypto trading needs visit pro.cracken.com to get started today mantle formerly known as bit Dao is the first dowled web3 ecosystem all built on top of Mantle's first core product the mantle Network a brand new high performance ethereum layer 2 built using the op stack but uses eigenlayer's data availability solution instead of the expensive ethereum layer 1. not only does this reduce mantle networks gas fees by 80 but it also reduces gas fee volatility providing a more stable foundation for Mantle's applications the mantle treasury is one of the biggest dow-owned treasuries which is seating an ecosystem of projects from all around the web 3 space for mantle mantle already has sub communities from around web3 onboarded like game 7 for web free gaming and buy bit for TPL and liquidity and on-ramp so if you want to build on the mantle Network mantle is offering a grants program that provides milestone-based funding to promising projects that help expand secure and decentralize mantle if you want to get started working with the first dowled layer 2 ecosystem check out mantle at mantle.xyz and follow them on Twitter at Xerox mantle if you haven't experienced the superpowers that a smart contract wallet gives you check out and buyer and buyer works with all the evm chains that are out there the layer 2s like arbitrum Optimum ISM and polygon but also the non-etherium chains like Avalanche and Phantom because of the power of smart contract wallets and buyer lets you pay for gas in stable claims meaning you'll never have to spend your precious eth again the web app has numerous Fiat on-ramps to make it easy to dump your Fiat for crypto and if you like self-custody but you still want training wheels you can recover a lost and buyer wallet using an email and password but without giving the ambier team any control over your funds check it out at amber.com for the web app experience but also the ambier mobile wall is coming soon for both IOS and Android and if you want to be a beta tester you can sign up at ambuyer.com app and since you stayed to the very end of this ad read you should know that ampire is airdropping its wallet token to early users for simply just using the wallet so if you want to get started with Amber all the links that you need are in the show notes Bank list Nation I'm excited to introduce you to Conor Leahy the CEO at conjecture a mission driven organization trying to make IAI go well he's also the co-founder founder of a Luther Ai and open source AI research non-profit laboratory which interestingly operates mostly inside of a Discord server much like our company and so many of us in the bank location Connor welcome Vanquis thank you so much for having me Connor the crypto world has more or less collided with AI and bankless we had our introduction with that surprisingly when we had our Eliezer udkowski episode which we had to Pivot mid episode from a crypto AI intersection episode into an AI is going to kill us episode since then we've continued to go down that AI alignment rabbit hole and I think a decent number of people in the bankless nation in the broader crypto landscape except the AI alignment problem but others completely reject it and it's interesting to see some people just have a spinal reflex rejection of the AI alignment problem so in this conversation I hope we can kind of just talk about the conversation of AI alignment in the outside world the companies that are playing here the game board that is laid out in front of us but first I really just want to dive into the very basics of the AI alignment problem and see if we can once again articulate clearly why the AI alignment problem exists what it is and why it's important and you've gotten a lot of practice at articulating this so I'm wondering if you could kind of hand hold us through some of the very basic premises behind the AI alignment problem yeah absolutely so to you know start things off the airline problem or existential risk is really what I care about this is this is that something could be so dangerous that an accident or a misuse could occur of such magnitude that it could threaten the continued existence of all of humanity or you know curb our potential forever in some sense this is of course be extremely terrible I think this is pretty uncontroversial that if such a thing were possible and if it did happen that would be pretty terrible and so the way I like to think about this is first kind of from like an outside perspective is if we look at a history of Technology you know if we see that technology has gotten better and better all throughout our history has given us way more power way more affordance a lot of this is great right you know we have medicine and you know I have air conditioning in my uh um office thank god um you know all these you know great things that are are wonderful and I love but also as technology increases uh as your power increases your ability to control the environment um you have more capacity for things to go wrong or for Destruction to occur so you know back in the stone ages you know the worst possible thing that I could do with Stone Age Technology would kill you know I know maybe like 10 people or something um you know if I'm like a pretty big guy or you know maybe if I'm like super smart about it I could kill a few hundred you know if I have my whole War band with me I could kill maybe more but like an individual person just using Stone Age level tools you know keep not going to really be an existential risk or anything of that sort as our technology gets better you know we develop you know uh more sophisticated Weaponry red develop gunpowder we develop um you know stuff like this uh the number the damage that can be caused both and on purpose and accidentally increases you know you sometimes for benign reasons you know if you have bigger ships more people can drown you know that's pretty benign we don't think that you know that's a risk that we are willing to take but in other greens you know there was no such thing as you know uh TNT factories blowing up before we had explosives and now that we have TNT when an accident occurred the collateral damage was suddenly of a type and a degree that didn't exist previously and this is only continuing you know so we went from you know okay you can kill like Five Guys to you can kill like 50 guys to kill 500 and now you know you press the button and drop a nuke out of an airplane and you can kill 50 000 people or even more than that and so if you like would graph you know over time be like reason like blast radius of Technology of like you know a misuse of a technology or thing you would see an exponential you would see that our technology is growing extremely fast extremely quickly towards larger and larger blast radius not all Technologies you know many Technologies are very safe and they're very good and I think we should invest in them and we should build them but there are technologies that have larger and larger blast radiuses and eventually this blast radius will Encompass Earth if our technology keeps improving at some point we will have technology that is so powerful that it can destroy Earth that it can destroy all humans and even accidentally it would of course be easier if you do it on purpose but at some point if we have powerful enough technology we should expect things where even accents are a problem and so I would make the claim that AGI is in this in this category it's in in the category of a of our you know Trend in technology towards more and more powerful systems where even an accident during the development of the system has larger and larger brass radius and you know sometimes this is what we accept as a society we accept that sometimes you know a clinical trial might go wrong you know this is something that sometimes we accept to some degree not to arbitrary degrees we don't let anyone do any clinical trial without any oversight of course not we do you know generally as a society we uh think very highly of you know human welfare and life and that it shouldn't be endangered recklessly but as this goes on as we're dealing with these more and more powerful Technologies we have to be very like what do you do when you have a technology where the blast radius is everybody including you like how do you develop a technology where getting it wrong ends everything ends you ends your experiment this is not something Humanity has experience dealing with but this is not something that we are generally set up to do the way we usually do technology is we build something and we fail a bunch of times you know we we mess it up you know a few lab assistants lose their hands you know like a bunch of stupid things happen you know government gets angry at you and 20 years later you know you maybe have something this is fine but predictably at some point this stops working me you know you maybe people could argue the time is not now and I would agree I don't expect GPT three or four to like you know kill all people I don't but you know gb5 6 7 8 you know combined with some you know modern RL agentic systems that's much less clear to me I'm happy to go into a bit more about why I think this technology has this kind of blacks radius But first you know just pause there for a second one thing I really want to emphasize in this argument is the neutrality of it it's not saying that AI is good or bad it's just saying it's just saying that AI is and this is a continuation of the Arc of technology technology is not good nor bad you're just really putting it into very neutral terms that technology has an arc of goodness but has uh turbulence with Associated blast radiuses along the way and that we've all accepted these because the Blast radiuses has been sufficiently small that it doesn't you know end all humans but what you're saying is that you know continue the arc get it to ai ai not it doesn't want to kill us it doesn't want to make our lives better humans will make choices with this technology but it just so happens that in the Advent of a bad outcome that bad out comes bad radio blast radius contains all of us so it's not even like a a political stance or a an opinionated stance about the goodness or Badness of AI it's merely just a statement about the magnitude of what could go wrong if something does go wrong yeah exactly I think a lot of discourse around this kind of stuff is very bad I think a lot of it is completely politicized and like psychologized is about is open source good or bad is this guy a good person or a bad person and I think this is just a terrible way to think about this we think you should think about this very neutrally the question is not is AI good or bad it's not is uh open source good or bad it's not is Sam Altman a good person or a bad person these aren't the intro the question is just what are the outcomes of the various choices we make how can we do our best to predict what the consequences of taking various actions are and how do we feel about those consequences how do we feel about taking these risks how risk tolerant are we who should be consulted you know I was never consulted to have you know uh gpt4 released onto the public internet which I use as well to for it to be released on my family and my friends I was never asked if this kind of experimental new technology should be Unleashed into the comments that I also inhabit and you know maybe that's fine you know we don't generally seek consent when someone you know wants to write a book you know someone wants to write a book you know I don't have to read it not my problem but if someone is you know releasing I don't know a new you know substance into the water supply well I sure hope that I would be consulted about whether I think that's a good idea or not because I'm drinking that water but like I I feel like I have a say in this and so the a lot of the problem with the current state of of discourse around the stuff is that there is some good technical discourse but then uh there's also a lot of discourse which focuses way too much on intentions it focused way too much on ideologies like just like you know that we can't derive from an ideology what is true you know if you believe you know open source is good you know maybe you believe that right but that's not a statement about reality it's it's a mood affiliation you know sometimes open source is great I worked a lot in open source I think in many cases open source is fantastic I'm so glad that Linux exists I'm so glad that we have so much great open source software I think this was really good but should the genetic sequence the smallpox be public like I don't know man like you know I love science I love Academia I think it's great that people are getting funded to do all kinds of you know cutting-edge research a group of researchers in I think Canada used you know government funds to reconstruct an extinct form of horse pox or smallpox virus and published how to do it I I think they should have done that like I I think this is bad I think people shouldn't do that I think that as much as I love Open Access science as much as I love science we have to be practical here I don't expect the upsides of this being public to be worth it so this is kind of like how I like to think about these kind of things I like to think about these things way more it's just like look what do we want like what are the scenarios what are the outcomes animals work from there let's let's work this completely neutrally let's be just like let's be realistic here I like to talk about strategy I don't like to talk about I really like to talk about like you know good or bad if that makes any sense right there's our there's a bunch of conversations to be had here about the players in the world of AI we have chat GPT and open Ai and Sam Altman we have stable diffusion with with emad there's a bunch of there's a bunch of people in this game there's The Regulators are also in this game there's I still want to continue a little bit down the defining the alignment problem uh conversation because there's there's uh something I want to parse apart you're talking about the sheer neutral Ai and its blast radius right what could go wrong if some if something went were to go wrong when we had our conversation with Eliezer Eleazar gave us a very strict prescription of how this will go wrong that your explanation of the AI alignment problem contains but his is more narrow and I want to parse apart that because he he calls this or we call this the AI alignment problem as in how do we align the goals of AI with the humans uh human goals and that's a more narrow conversation than I think what you you're presenting you're just saying hey AI is powerful it could go wrong and it could go wrong for any subset of ways some of which may be the AI alignment problem but there are also other ways that it could go wrong I'm wondering if how do you parse apart the ways that are there categories of AI like um Doom that are worth parsing apart one of them's AI alignment others there's like maybe humans go Rogue and then we've seen humans do this right like what what would happen if uh Ted Kaczynski got their hands on on a very powerful AI like that's another conversation how do you parse apart like the actual sources of Destruction here yeah so like the I like to like use like two maybe three or four categories is that the first one is misalignment it's just you don't have control just lack of control as saying does something random that's it and like there's no you know doesn't even need a human to be involved right and then that random things blast radius contains us is what you're saying yeah yeah so the claim is that a accidents can happen and that accidents can have blast radius of this side so there's like accidents the second one is misuse so misuse is you have a system which does what you say and someone tells it to do something bad or for there to be conflict of some kind you know maybe there's multiple actors who go to war using this kind of technology or like fight and then the in the as you can I can barely imagine something more horrific than multiple you know super intelligent systems fighting like could you imagine like it's as unimaginable what Horrors star systems could unleash in the terms of war and in that scenario that scenario is like kind of interesting because it implies that the AI alignment problems actually solved as in we've been actually able to align human humans and AIS together to achieve the same goals but human alignment is not solved and so using our AI superpowers we now commit war and our war blast radius once again contains everyone exactly so it's like it's it's even worse than that it's like there's so many ways in which this problem is very very hard we are this is not a super narrow specific little problem there are specific narrow aspects to problem-specific narrow technical problems which are very very important and we can talk about those but importantly this is a generalized problem of society this is a general's problem of The Human Condition is a general problem of how do we responsibly deal with powerful technology this is the meta problem that we have to actually have to solve and this is a problem that people have been talking about at least since like World War One you know there is a Polish Nova man named after krzy who after the horrors of World War One noticed in this like 1920s he was like wait if technology keeps increasing but our wisdom and our you know our control as a society increased much slower well then all Humanity will end he figured out X risk in the 20s and yeah and he wrote and so his solution was he had a math he had to figure out how to improve the art of human rationality sound familiar so there were there was an Eliezer in the 1920s actually named after zipski so this is not a new thought this is not something that you know I'm sure people before Alfred has also come up with variations on this thought and this is continuing this problem and we have not yet solved this problem we are already in the problem you know the last time we had a new level of powerful technology you know we did drop two nukes on purpose and after that there were several really really close calls where you know nukes almost did stop Flying you know at least two where it was just one single person each time who stopped the nukes from actually getting fired so our track record here is like decent but it's not good it is not good and it's going to be much worse when we're dealing with agis and aeon especially like imagine if these things are open source imagine if every person in the world had a nuke during the Cold War I expect we would have gotten nuked you know I expect it wouldn't have gotten well if we didn't have chains of command you know several people signing off on something if we didn't have sensible people that took their responsibility extremely seriously I think it just would have not gone well and the way things currently are there is currently more regulation on selling a sandwich to the public than there is to building an unprecedented AGI level technology and releasing it to the general public there is no oversight there's no you know process you know General processes here is there controls or like you know stakeholders or something is nothing it's it's just these private companies it's a small number of private companies to be clear about this it's basically currently mostly just Deep Mind anthropic and open Ai and a few others who are like trying to catch up that are really pushing forward to these high-end AGI level dangerous Technologies and erasing completely out of control so like how would you expect this to go well at this current Pace it's like it's like we're in the worst possible scenario we could be in kind of I just want to pin down the categories that we were talking about I I interrupted you and just want to make sure that we clearly identify them the categories of how AI progress would go wrong one of them is misalignment this is the Eleazar conversation the pay-per-clip maximizer conversation we create the super intelligent Ai and then we can't figure out how to harness its goals and align them with humans and so it accidentally turns us into paper clips that's one category another category that you defined was misuse um we do somehow do figure out the AI alignment problem but we just abuse AIS to kill us all so one super power fights with another superpower one of them has AI maybe both of them have ai and then we all die because we're using AI to have misaligned human uh goals so that's a misuse another one is accidents we have super powerful AI it accidentally does something that we don't like and we're inside of that blast radius that's a third category are those all of them or are there others that are worth unpacking yeah I mean I think you can move even fold accidents into misalignment in a sense is that in a line a truly aligned system if you tell it to shoot you in the foot it will say no that's not what you intended and you know bring you flowers or something if you have a truly it's like there's like maybe like three categories is or like maybe four is like you know I like a good hierarchy maybe four categories technology there's so dangerous that no one should do it you know just like there is nothing like you just you turn it on it blows up everything then there's like technology which is uh which is controllable if you're really careful if you're very sensible if you're very careful it's fine then there is technology that is uh safe for General use except if you misuse it except if you're specifically trying to do something bad and then there is technology that is good no matter who uses it so that you could give it to the most evil sociopath in the world and it's fine okay and AI I can see AI fitting into all those categories exactly I think we will start at one and then we can develop technology to move to two and then we can develop technology to move to three and then we can develop technology to move to four but by default we get the category one AI uh by default we Elevate AI as it progresses finds itself inside of the category of Technology you almost never ever want to even open up at all yes and this is what I expect the the like the shortest path to an AGI gets you this type of AGI a system which is misaligned you know paperclip maximizer has some random values it's very intelligent very capable it's very deceptive and if you just turn it on it doesn't matter who turns it on it doesn't matter if this is you know the USA or China it doesn't matter if it's open AI or anthropic it doesn't matter who does it it just blows up everything there is no it doesn't matter you said the shortest path to get to a super intelligent AI can you unpack why you emphasize the word short there what what does what is implied under a longer path to an AI what does that mean so importantly this is not a rule of nature there is no law of physics which states it is impossible to have a safe AI that does good things for you this is completely Allowed by the laws of physics and computer science we just don't know how to do it and this is a very narrow Target then as a each of these you know categories one to four are like a error and narrower and narrower Target you need to know more and more about control about intelligence about you know safe practices about human psychology about like you know about you know values and Game Theory and whatever to to narrow down on these more and more complex systems so by default if you just want to think which is just smart it's just powerful it just succeeds at goals well you know just you know big larger models you know just throw computed it man you know just you know continue doing what we're doing right now the basically all the research currently being done at AI companies is of the kind which gets you to type one there is a very little research that goes into getting you to type two three or four this is a great a great quote it's from Wilbur Wright so this is the uh one of the brothers who built the first airplane and what he said was is that when Once the machine is under proper control under all conditions the motor problem will be quickly solved a failure of a motor will then mean simply a slow descent in safe landing instead of a disastrous fault so this is the man himself built an airplane who realized the first step to building a good airplane was to solve the safety problem it was to build a safe glider that if something goes wrong it carefully slowly descends instead of killing the pilot because before this there was a lot of attempts at flying machines and they always killed the pilot so they couldn't develop them yeah he recognized that we had to First build a safe glider and then we can worry about the motor part you know and he even though it's like almost dismissive of the motor he was like ah you know you know we'll figure our motor it's not that big of a deal and so basically all of AI currently works on engines they work on Motors there's very few people working on gliders and there's a basically everyone is working feeling bigger and bigger turbo jet engines and you know just like as fast as possible quick as possible and if you get a bigger and bigger turbo jet engine without improving your glider design without improving your wing control design without in you know doing the necessary experience for this by default you get an A type 1 engine you get an engine that just explodes it just like you know zooms off into space and you know just like does something stupid and you know if it's like an aircraft engine the risk the blast radius is contained as before but AI is not just an airplane engine right yeah the AI engine is focused on elevating Humanity to as high of an elevation as possible and then all of the AI safety uh people are like hey we also need to make sure that we have a spaceship that is containing us that can take us back down in the inevitable case that eventually that that engine Runs Out Connor I'm hoping I can just get your lay of the land of the state of the AI alignment conversation because as as we've been saying as you've been talking about AI progress seems to be moving really fast uh is now June 2023 and it kind of seems that we have to time stamp these podcasts because if we were doing this podcast in just may or April of the same year it would be a slightly different conversation about what is the state of the AI alignment conversation uh last we checked there was this letter signed by many world leaders and AI experts asking for a pause on AI progress Beyond Chad 2bd4 I'm wondering like oh if you can just update us on the last few months in the mainstreaming of the alignment problem like is there more reasons to be optimistic are people burying their heads deeper in the sand like kind of where is the world with regards to AI alignment and AI safety yeah I mean what and has the world changed the world's changed insanely over the last you know six to 12 months I mean to say it lightly uh AI Lyman has gone to a large degree mainstream I just earlier today was talking to a member of parliament who didn't even have a smartphone didn't know what open AI was but he heard about his AI thing and he wanted me to tell him about it and uh he got quite Furious when I explained to him some of those risks and he's like no one taking this seriously what this is outrageous like of course we have to do something about this um so it's quite fun um for me it's also been a very enlightening experience so when I first got into this field you know I came in from like a pretty classic you know kind of like less wrong you know Eliezer adjacent kind of viewpoint very technical very nerdy very philosophical perspective on things um not much and there's a lot of weird social memes in like that sphere around politics that like politics is bad you should never do it don't you know don't talk to the public don't talk to the government they're all crazy can't talk to them and I feel it's completely gaslit because that's just not true uh don't get me wrong politics is hard you know politicians are have their incentives blah blah blah all these things are true but this is things you can do you can talk to them a thing that's just been incredibly positively shocking to me is that when I talk to normal people who don't work in Tech they really get it I'm so used to talking to people in Tech and they just totally dismiss these things you're like no yeah I can never do anything bad you know blah blah blah I can't hear you and I talk to normal people and explain to them hey you know these things have become more intelligent and they can do more and more things and we don't know how to control them and they're like holy well that's what that's terrible of course this is going to go wrong what do you mean like you know and then they you know I'll like repeat an argument that like some help me make something like that's not convincing at all like what do you mean he doesn't understand how a system works it's all black box says this is this is madness and this is the correct reaction the correct reaction is this is madness this is complete and utter ludacy look you like let me be blunt here even if you your listener don't totally buy the existential risk thing you know maybe you don't buy it's fine but it is the case that the leaders of all the top Labs anthropic Deep Mind Over have been on the record saying clearly that they do think that there is a realistic possibility that these technologies will kill literally everybody and they're doing it anyways even if you disagree about the risk being real I'm quite shocked that someone would like admit that they believe this you know state that they would that they do think this and then also that they are willing to do it without the necessary safeties and precautions and there are arguments and man are there arguments and we can get into those arguments the counter arguments about why actually this is fine I don't find them convincing obviously and we live in this weird Twilight World we're on the one sand um I don't think these people are malicious to be clear or like I think they're lying per se I think they're being inconsistent like you know Sam Altman will often go on the record and say oh you know he thinks AI is the biggest risk to humanity cool great thank you Sam that's really great that's like I'm not being facetious this is actually fantastic that he has the honor in the honor and The Bravery to say this publicly as someone you know who is ultimately a businessman but still willing to go onto the record with this that is respectable and deserves credit but then he keeps racing and then he keeps still Building Technology there's this incredibly funny interaction on Twitter where Jan like the head of safety and alignment and open air tweeted something like okay you know maybe we should be careful and like you know slow down a bit before we integrate all this AI technology into you know all facets of our life and then six days later Sam Altman tweets about chat GPT plugins you know plug-in chat GPT into whatever you want and I'm like man wow like if this wasn't a movie like I could match just like the cut and then like you know the laugh track playing I'm like this is this is um this is truly shocking and this is a consistent feature is that this is something I've been pushing on a lot in the current discords is that a lot of the discourse right now is people are starting to wake up to it and they'll be confused they're confused about like well is this risk real which is a good thing to be confused about this is a fair thing to be confused about and there's other things about like so it's a very funny thing that I see a lot is for example on Twitter people see like say sam Altman call for regulation and they'll be like wait this is sus like if he wants to be regulated so bad and he thinks this risk is so big why is he doing this just stop just don't race if you think this is an extra interest just stop and my honest opinion yeah that's pretty sus like some people criticize me by proxy they'll be like Connor you're one of these Doomer people and but you know you Doomer people you know if you take it so seriously why don't you start like well first of all I don't race and yeah that's a really good question this is a question that I have for the head of all these Labs why like stop I'm happy to go into the arguments that I expect their straw man version too but you know in case you have any comments yeah maybe we can actually just um define race um the the whole like race condition side of things uh a decent part of the bankless audience will be familiar with moloch and moloch traps um but it's been a while since we've had a mohawk episode so maybe you can kind of talk about just like what this term race means and why why we are in a race trap yeah so race conditions or you know it's kind of like in the sense of like a race to the bottom is that the idea is you and other people um don't want to go somewhere you don't want certain technology to exist but other people are heading towards it you think you're better than these people you're more responsible nicer whatever and so you think well you have to get there first before they get to it and so you start you know trying to get there as fast as possible they notice that now you're trying to get there as fast as possible and then they're like well I don't want that guy to get it because I'm the good guy and then they start going as fast as possible so you get this uh game theoretic problem where now it's a race to the bottom this is not unique to AI this examples for example with like safety regulation the reason we have regulations for safety standards on the government side instead of letting the market regulate itself is by default if you have a market and no regulation there is an incentive to cut as much safety measures as you could possibly get away with you want to do as little safety as possible if you don't get in trouble for your employees getting killed well then just let them die you know it's like not that big of a deal and this is exactly what happened like during the Industrial Revolution is that the pricing of risk and of dangers and these kind of things can be unlike an economic scale it's very easy to be mispriced from what we as a culture might want as a society might want and this is not supposed to be a statement that regulation is good or bad again it's not about good and bad this is how we started this podcast not saying regulation is good or regulation is bad it's never that simple it's the question of what do we want what do we as a society want and how can we get that and if we don't like where we are currently what can we do to move to something that we like so with the race what we're seeing here is as people is companies undercutting themselves on safety and speed and timelines sometimes I like to like sometimes um you call it um burning our Runway so Humanity has a Runway until AGI arrives it will arrive sooner or later it's before that time we don't have a safe glider we die and they're burning the runway they're making it shorter by pushing forward this technology by investing more into it by you know building bigger engines there's some engine size that when you get to that size it blows up everybody unless you have a safe spaceship built around it and currently we're not building a spaceship we're just building better engines because well I don't want the other guy to build engines he's really unsafe he he doesn't take it seriously or what if what if China gets it you know what if what if you know some other country gets it no America number one we have to get it first and I understand these arguments wrong and I'm happy to go into some of the details about why they're misleading but it doesn't matter there's nothing to win so the the real problem here is the like the the simple counter argument is there's nothing to win you just lose you don't get a type this would work if we were talking about type 2 AIS you know so AIS that are safe if you're careful if we get to this and I think this is one of the things that Sam Alpine would claim he would claim oh no no we're gonna build a type 2 AI don't worry we're not going to do those type 1 ones don't worry about that and I would strongly disagree I don't think that that is supported by our level of scientific progress on the alignment problem whatsoever and but if we were super on track to get like a type 2 3 or 4 AI then I'm like okay fine you know fair enough that's that's a reasonable thing to believe I would still be more careful than that because I would never be that certain I don't I wouldn't want to risk it even if I believe we're probably gonna get to type two or three I wouldn't want to risk all of Humanity on it I would like take some time but this is kind of the situation written now is that of course the people who are the most optimistic will be the ones who end up in the position to push this kind of race forward it's not a coincidence and they're going to be the ones who get billions of dollars of investment this is not a coincidence it's not a coincidence that you know Sam Altman is the head of open Ai and not Eliezer Eliezer is not the kind of guy to lead an opening eye of course not that's not what you would do and so that's exactly the kind of scenario where we're in right now who would you say are the main uh players in this race Sam Altman certainly of of open AI uh China as a techno country as a whole is maybe totally disagree totally disagree who's in the race so just to say a word on China um I don't want to go too much into this but like there is a meme that exists which I would like to dispel of like China is this massive rival here they're gonna you know overtake us they're gonna build AGI or whatever I think this is really ludicrous for anyone who has she knows about China what the Chinese Communist party wants is stability and to stay in power more of anything do you think they want a crazy uncontrollable technology that could topple governments no and China has been very clear multiple times that they're willing to take massive economic burdens to censor and stamp out their own tech industry they've done this multiple times of all the countries that I think is like most likely to regulate AGI away it's China this is like the this is completely in their interest that it is not in the Chinese Communist party's interest whatsoever for AGI to exist it is completely counter to their incentives and so and also they're very far behind technologically but this is like a comment I just I think a lot of people are perhaps perversely benefiting from holding up China as a boogeyman so that  you know but the truth is basically 100 of the risk comes exclusively from the United States of America there is no in my opinion appreciable risk from non-western countries well whatsoever maybe this will change you know 20 years or 50 years or something but at the current point in time all these models all these things are being built on U.S soil basically exclusively like you know even model like there's a recent model called Falcon which was created by the UAE um but it was trained in U.S data centers this was done on U.S Hardware on U.S data centers this is this by as far as I'm aware U.S programmers so they were working with the UAE on these kind of things so like this is very much an American and partially UK European uh issue but that being said so players the main players as a geopolitically it's the USA I mean to some degree the UK and the EU a little bit as well but not deeply so a little bit um and not not trivially so but not really it's the USA and within the USA it's generally a small number of private companies this is Google deepmind uh openai Microsoft and anthropic these are the main ones there are other people who are trying to catch up there's a bunch of other startups trying to you know race forward to that kind of Technology but they're all very very far behind and most of the other Technologies uh groups in these field are not nearly as ideologically devoted to Super intelligence as these three companies are the founders of all these companies have been very clear and publicly so that their interest is god-like super intelligence they want to build systems that can reshape you know all of humanity that can you know make you know you know upload us all into the cloud that can you know turn the whole world into Nanobots that can this is this is not me saying this like this is like what actually these people happen quite publicly about this is what they're trying to build they're not trying to build a better chap up they're trying to make the most shareholder return they're trying to do is to build Godlike super intelligence and then unleash it upon the world with you know they believe will bring Utopia and so this is you know people like Sam Altman people like Demis asabis uh daru emaday from anthropic these are like some of the CEOs of these various companies and again I would like to state I'm not saying these are bad people I really don't want to say this I've talked to all of these people and they're for the most part really pretty great like they're really pretty nice and very very smart and hardworking and mostly you know trying to do what they think is right some more than others but it should be clear what they are and what they are is transhumanists they have an ideological interest in doing this they they have strong incentives to be very optimistic and not think too hard about the dangers or to find excuses for why well it's a race you know my hands are tied nothing I can do about it and I understand I don't want to criticize and like say like these are like evil people there are evil people involved in this system maybe but for the most part these are people of certain beliefs who have certain incentives who are trying to make the world a better place but I from my perspective find what they're doing unimaginably Reckless and cannot continue yeah I'd like to unpack a little bit more around that just because I so I can understand where the um disposition of all of these Founders are like you said that the Tech Industries more or less in denial about this at least in comparison to the outside world and you've also said that people like Sam Altman and maybe the other Founders as well have said that yes AI is perhaps the most uh the largest existential threat that we have and yet they continue and so I'm wondering if you can just like diagnose that like is Sam Altman and all these other Founders they're just oh we're in a race my hands are tied I guess I'll just keep on racing until some sort of external Force stops me like to how would you actually Define why the disposition of these people are the way that they are the truth is of course I do not know and this is basically psychoanalysis I could do psychoanalysis I know some of these people you know at least a little bit I have studied pretty extensively what they do and why they do it I've read everything they've written and so on I have you know guesses about their internal lives the truth is I don't know their internal lives maybe you know maybe some of them are total evil you know scheming Bond villains I don't think so but like maybe I don't know you know maybe some of them have really good reasons to believe that AI is super duper safe I haven't gotten those reasons out of them even after talking to all of them many times and pressuring them on this so I don't know what their emotional motivation is but what I can describe is their stated opinions their stated beliefs and their stated actions my usual thing I would say and I recommend this to everyone I think this is a really really really important skill that a lot of people neglect is watch the hands not the mouth these people all say very very nice things that make you feel very very safe and then their hands do something very different this is a very consistent feature I found across dealing with I mean not just these like you know generally powerful people you know politicians CEOs um you know billionaires just wash their hands not the mouth the it's if you're very smart it's very easy to come up with explanations why it is actually good to do the thing that you wanted to do anyways and truth of the matter is you can go back to archives sometimes from like the 90s from some of these people talking openly on like you know email lists or on their blogs about how they want to build AGI how they want to bring in the transhumanist future they want to you know create this beautiful you know you know Immortal World of you know transhumanist cyborgs or uploads and whatever and they want to do it as fast as possible you know they want to save all this you know even Eliezer is was at least in this Camp um and was you know in the 90s and 2000s was accelerationist he thought that being building AGI was the best thing we could do and we should do it as fast as possible he changed his mind which is fantastic and speaks to his character and his like ability to think about things rationally and reasonably that aliaser did change his mind when he was quite young I think he was like 21 or something but for other people I feel like I think a lot of them have absorbed part of the arguments but like watch the hands not the mouth I think a truly damning example of this is the effect of altruist movement which is deeply ingrained with basically all three of these organizations to various degrees and look again effect vultures for the most part are really well-meaning good-hearted smart people trying to do the right thing are there bad apples of course there are you know no no question about it are there weird culty dynamics of course there are they exist in any large movement of this kind are there you know weird untolds absolutely but most individuals are good people my truth of the matter is is that you know effective altruists and people like Eliezer are speaking out against these corporations for racing Eliezer was one of the people who helped Arya found deepmind he was one of the people who introduced um uh Dario and Shane Legg to Peter Thiel for initial funding I don't know how much it helped them but it was something I was thinking about at the time how to you know push on open AI was founded by people many of which were effective altruists or effective altruist adjacent and open philanthropy uh the largest funder of uh effective altruist goals gave open AI a very large early Grant it is I think the second largest Grant they've ever made to an AI organization I think was 30 million dollars roughly I was given to open AI in the early days to make it a safe AGI lab in fact from what I hear it was Dario amodei later CEO of anthropic who suggested to Elon Musk to create openai as a uh you know counter lab to uh deep bind to do safety and as you probably know like early opening I was also very open source focused they changed their tune about that and then later you know after Sam got involves a lot of drama there about Elon and Sam and such that I'm not privy to I don't know exactly what happened but some drama occurred later on Dario led the project for gpt2 and then the project for gpt3 he was the one pushing scaling laws he was the one making these systems stronger and more powerful and pushing forward on this access and then after gpd3 um citing security concerns and disagreements with Sam and the direction of openai which seems you know completely plausible to me and have a reason to doubt this per se that he and a bunch of other people taught people all the gpd3 authors and people left to found anthropic and what did they do at anthropic they build larger models and other racing and the recent pitch deck I was leaked from anthropic they are talking about how they are going to build a 10x more powerful model in gbt4 there is a and to this day you'll have people in the AI safety community fear vigorously defend anthropic or even opening eye or Deep Mind as the safety oriented no this is good actually you know there are friends and but like you know there's all these weird incentives going on there like you know like the president of anthropic Daniela is uh Dario's sister is married to Holden karnowsky who was the CEO of open philanthropy like there's like and this is a great example of watch the hands what has happened is that these people stated very clearly that they think AI risk is real this is a huge problem et cetera et cetera but what has happened what has actually occurred is they've accelerated the race like no one else and this is I'm talking about this publicly for one of the first times I think uh because I think this is so important to make this clear what is going on here um I've gotten quite shunned by the EA Community for a lot of these uh you know angry um comments um but this is this is a common feature this is and the same thing with politicians I've talked to many politicians they come to me and I'd say them oh this is dangerous and I say well openai told me that they tested their model and it was safe and I'm like man uh okay where do we start with this so like none of this is surprising it's corporations doing Corporation things it's Microsoft lobbyists doing their Microsoft lobbying thing I'm not mad I feel like this is just this is what any I think reasonable political analyst would have predicted about how the discourse on AI safety would go before I say it if I talked to like an old school you know like I know like uh you know environmental activist who was there for like the oil lobbying stuff and I asked them how do you predict the current state of AI discourse is you would probably make the same predictions for where it currently is where like yeah people are not on board with like oil pollution they think it's bad but of course companies have lawyers they have lobbyists and they have a lot of great excuses and they have a lot of great comments about like oh we should do self-regulation unless the oil companies you know regulate themselves that's like the oil company set the safety standards for their oil you know extraction which is unironically what people are suggesting we have people at open AI pushing through evals you know for safety which is a great incentive but this obviously has to be nonpartisan this obviously has to done by people that are you know neutral parties it's currently not the case you know a lot of the people doing the evaluations are people who either you know work at one of these organizations or used to work at one of these organizations and this is just kind of kind of silly I'm kind of just getting the intuition that if we re-rolled the dice on this universe and maybe we plopped out Sam Altman an open AI something else would be there instead and so like we could go down the rabbit hole of just like hey let's talk about the personality disposition of each of these Founders but I think it's really just about the nature of the problem itself that it doesn't really matter like take out Google take out anthropic take out open Ai and you'll just find three other companies that will take that place and take their place in the race like the race conditions are race conditions that expand beyond the current set of players do you agree with this intuition I'm more skeptical about that than I think you are I think it is true in the limit like eventually someone will figure out AGI but I think you're underestimating how much ideology is actually at work here gpd4 was not a coldly calculated you know business decision it was an ideological decision gpd4 cost like 100 million dollars or a billion dollars to build it's not meant to make that much revenue it's like extremely expensive extremely risky it could have blown up at any time this is not what you know this is not what a Goldman Sachs does you know no Goldman Sachs is going to build a gbd3 or gpd4 you know no coldly rational organization like this is gonna it's gonna take risks like this and right you're saying the rational economic actor would not have chosen to produce chat GPT for yeah after they have chat GPT sure they might have liked it but no rational actor would have let at least not now you know maybe when the cost comes down you know 10 more years of progress maybe then so I do think eventually would have happened but we do have people who are accelerating it's not just that like oh this is the this is the one time where it happens no it could have happened in 10 years it probably could have been a few years earlier you know it could but like not many years earlier I think you know maybe other people would have arosen possible I'm not saying it's impossible like you know we can't re-roll history really but yeah I I don't like these arguments too much about like oh it's just incentive it's just a race also because this one of my maybe controversial beliefs is that I think that um people have way more control over reality than they think they do I think that actually the world is way more plastic and way more controllable than people think it is I think that individuals with high agency can get a lot more done than people think they they can I think that individual actors matter a lot individual great people you know or politicians or activists or whatever can make a huge difference actually there's a saying forgot who was from I was like never underestimate the ability of a small group of uh dedicated people to change history in fact it's the only thing that ever has so what you're saying is that uh what I what I hear you saying is that these the because of the commitment to building chat gbd4 even though it was economically just non-rational you're saying therefore it is ideological as in therefore these people are motivated by um something something else an ideology Maybe some sort of Glory I don't know maybe the idea of just like creating the AGI is so attractive that they want to they want to be the ones um and so and so that that is a little bit more of a Partnerships problem because that is like harder to Tinker around with with external incentives correct yeah and to be clear it's not just that it's also that they literally State this on their own personal blocks right like I am not doing just pure psycho now analysis here like this is literal actual statements you can actually read written by these people and like you know you know that will be confirmed by their friends and so on like you know not all of it but like these are not wild speculations that I'm pulling out of nowhere you know maybe they've changed their minds you know maybe there's some subtlety to it I'm not dismissing these possibilities by any degrees here but I'm saying ignoring that there is an ideological component I think is is not correct I think it is very reasonable there is an ideological Or Glory or a aesthetic preference sometimes there is a in retrospect really rather chilling interview with Jeffrey Hinton one of the you know Godfathers of AI from I think like 2000 like 15 or 20 I don't remember like I remember and he the article ends on I'm asking like well you know if you think these things could be dangerous like why would you do this and Hinton basically answers well you know sometimes the the uh lure of Discovery is just so sweet you can't resist and this is very grimly um prophetic in that in the sense that now Joffrey Hinton has disavowed his life's work and has now come out in favor of that actually this is an existential risk this could kill everybody I was in a lecture with him that he gave in Cambridge a few weeks ago and uh he's a wonderful lecturer and it was it was a very lovely lecture but it basically ended on this note of just like well it turns out yeah probably that this is yeah this is going to kill everybody and I really don't know what to do about it yeah damn it was nice like really this like weird note like even Joshua bengia one of the other like touring Award winners so one of the most senior most respected people in the entire field of AI has said that he feels lost after having come to realize just how it's regret his life's work imagine that imagine one of the most senior professors one of the most senior scientists to ever to existence who has built this field saying that he regrets his life work like this is unprecedented this is an Inc this is unprecedented basically in history I mean you know there's some cases but this is this is a truly extraordinary scenario for something to be this clear so I think a lot of it is just curiosity Beauty fun Glory Etc and you're correct this makes purely rational incentives harder to control I think this is this is a part of it if we try to model this as a purely capitalist problem this is a purely money problem I think I think we would be not get the correct solution I think this is again where we just have to be pragmatic I'm not being judgmental I'm just being like okay let's be pragmatic what can we do like what do things need to get them and like the truth is just this is where government has to step in like at this point like these people are not going to stop they've had many opportunities too and they will not do so uh government just has to make them metamask has something new introducing metamask portfolio metamask portfolio is the best way to view your crypto portfolio from a holistic level see everything across all the trains all at once in your portfolio metamask will report the aggregate value of all the assets in your metamask wallets and even the other wallets you import too but metabask portfolio isn't just a passive portfolio viewer it is a place to do all of the money verbs that make defy so powerful you can buy swap bridge and stake your crypto assets so not only is metamask the easiest place to see your wallets in aggregate but it's also a powerful battle station for all of your D5 moves so go check out your metamask portfolio because it's waiting for you to open it up check it out at portfolio metamask.io you know uniswap it's the world's largest decentralized exchange with over 1.4 trillion dollars in trading volume you know this because we talk about it endlessly on bakeless it's you to swap but uniswap is becoming so much more uniswap Labs just released the unit swap Mobile Wallet for iOS the newest easiest way to trade tokens on the go with a uniswap wallet you can easily create or import a new wallet buy crypto on any available exchange with your debit card with extremely low Fiat on-ramp fees and you can seamlessly swap on mainnet polygon arbitrarm and optimism on the uniswap mobile wallet you can store and display your beautiful nfts and you can also explore web3 with the in-app search features Market leaderboards and price charts or use wallet connect to connect to any web3 application so you can now go directly to D5 with the uniswap Mobile Wallet safe simple custody from the most trusted team in D5 download the uniswap wallet today on iOS there is a link in the show notes arbitrim1 is pioneering the world of secure ethereum scalability and is continuing to accelerate the web 3 landscape hundreds of projects have already deployed on arbitrum 1 producing flourishing D5 and nft ecosystems with a recent addition of arbitrum Nova gaming and social dapps like Reddit are also now calling Arboretum home both arbitrum 1 and Nova leverage the security and decentralization of ethereum and provide a builder experience that's intuitive familiar and fully evm compatible on arbitrum both Builders and users will experience faster transaction speeds with significantly lower gas fees with the arboretum's recent migration to orbital Nitro it's also now 10 times faster than before visit arboretum.io where you can join the community dive into the developer docs Bridge your assets and start building your first app with arbitrum experience web3 development the way it was meant to be secure fast cheap and friction free I was going to ask you like what are the next steps here because I resonated with the quote that you said where um the man feels lost because at the end of any sort of AI podcast and again our AI podcast us are primarily alignment and safety podcasts I I feel lost I feel a little bit hopeless uh and so yeah I think you what you just suggested is like the easiest thing the most lowest hanging fruit that you see possible which is that governments have to step in uh what does that look like to you how do we get that process started like what are your ideas here yeah and so again I want to call back to the beginning of this podcast and just be like this is not about government good or bad I don't want to have I don't want to talk about you know you're from a crypto background I assume many of your viewers are quite skeptical of the government I think there's very good reasons to be very skeptical about my general distaste yeah totally understood you know I I run a business nothing makes you a Libertarian faster than founding a business you know you know and noticing all the red tape you have to go through like I I understand I understand but this is not about good or bad it's not about an Ethics judgment this is not about oh you know they've messed up housing policy spoiler yes um but it's it's way more about okay let's be practical here what are the options what can we do so this is the kind of thing the government exists for oil companies are polluting they're just you know putting poison into the river or whatever and you ask them nicely they don't care and they keep doing it you know maybe they cite some reasons about you know acceleration or like races or whatever then you send police officers to make them stop you know like you know they give the polite knock and a polite letter and you tell them look you knock this off or you're in deep trouble so this is going to have to happen this is not a solution to be clear I want to be very clear none of this is a solution this buys you time this doesn't solve the whole problem of technology and you know the future of mankind this is no one would claim that no one would claim that you know us you know shutting down some of these dangerous experiments is you know going to solve us but it buys us time the we can talk about this in a second but the longer story of course is is that if we really want to solve this if you want a good future for Humanity I'm gonna have to do a lot more than this I'm gonna have to do a lot more than this I'm gonna have to solve a lot of technical problems and a lot of political problems and a lot of philosophical problems if we want to get there and it's unfortunately not optional but the first most clearest step from my perspective is that I I mean to be blunt um I don't currently see any good timeline in which there is not a pause if you don't buy more time if we just continue accelerating if we're just pushing as hard as possible as fast as possible with all the resources our economy can muster we're not going to make it we're gonna type one system is going to pop out and that's that's just gonna be it you know and you know we're not even going to make it to a type 2 system if we make it to a type 2 system metas can immediately make it open source and then we die that way but um you know that aside I don't even think we're gonna get to type 2 systems so um the government needs to step in and slow these things down luckily there are actually very good levers that the government does have access to in current legal Frameworks that can be used with quite small impacts on The Wider economy there is luckily we are in a scenario where really 99 of AI is completely fine you know we you know love you know medical research you know AI or you know using particle physics or you know doing like um you know picture Generation stuff even so that has some copyright issues and so okay let's not open that can of worms um but like there's a lot like 99 of AI is great it's like you know it's progress It's technology it would provide you know great benefits to society you know fantastic you know there will be ups and downs but the blast radius is contained 99 of AI things has a small blast radius not necessarily small not zero not saying zero but you know the kinds of blast radiuses that our society knows how to deal with like we can handle you know the internet had a huge blast radius but we're still here we can handle the internet and so we can handle 99 of AI just fine there's only really a teeny tiny percentage of these like hard you know huge you know gbt4 gp4 plus you know large language models general purpose reasoning agents RL you know like far end stuff that were basically all of the risk comes from that you know all of the existential risk comes from not that there's no risk from other things there are other risks but it's not existential the existential risk is really focused on this teeny teeny tiny subset of like you know three maybe you know companies in the United States that's kind of it and the government totally can just stop that this is we don't even need International treaties the United States government can just unilaterally tomorrow put out an executive order and just put it into this it's that easy and this exact suggestion I would I would I would give to the to the US government is to Simply ban or require strict oversight and uh regulation of any individual training run which takes more than 10 to the power of 24 floating Point operations this is a unit of measurement for the amount of computing power used and luckily this is very easy to measure we know gpus have this and this much no computing power run for this in this long you can calculate how much computing power is going to run this is very easy to measure this is it's very clear there's very few companies that have access to computers this big they are known to the US government they are easily tracked down and they are you know settled in the U.S these are you know large Cloud providers Microsoft Google Amazon Etc and they will comply of course they will like you know they're not going to risk their entire multi-billion Dollar business for these you know few you know Accenture clients so if the US government tells you know Amazon no more large training runs no more things above 10 to the power 24. I'd expect Amazon will just instantly comply and that seems actually to be kind of a long-term solution with my limited understanding of AI constraining how powerful our computation can be constrains how powerful the AIS can get universally across the board and so while that law would be in effect we have as and so long as no one's violating that law we have as much time as we need is that correct that's my intuition fortunately no um I wish though yeah uh oh yeah indeed the truth is is that there are we are not an exponential we're on a double exponential there's two exponents is it Hardware plus software yes that's correct okay we are go both on exponential and hardware and an expansion in software if we can cap the exponential and Hardware again you know if people want access to their small AIS at home they want to do stable diffusion they want completely fine you know go you know maybe the government gets involved for copyright reasons or something but like from my extension risk perspective I'm quite fine with that but not these Frontier runs so you know uh for for uh calibration 10 to the 23 is roughly how much the estimated gpt3 used gbd4 probably 20 10 to the 24 or 10 to the 25 and um we expect that you know gbd5 will probably take another you know factor of 10 on top of them and so the cap at 10 to the 24 I personally am confident that algorithms will improve that you could still build existential risk in that okay I actually expect this possible 10 to the power of 23. it's probably you powerful capable of even less than that it's probably possible I expect to do that you need algorithms that we don't currently have like I don't think currently you can make gpd3 existentially to interest I think it's currently possible but I'm not confident it's going to be true in 10 years um or in 20 or in 30. we will develop better algorithms for bootstrapping for you know more efficient you know more sample efficient learning for better RL planning things for bootstrapping and like there'll be like all these things that we will discover and that we will continue to improve there's lots of low hanging fruit here still um so but this does buy us time this does buy us a lot of time this buys us a lot of time where we can do it cuts down the acceleration by half exactly it makes a huge difference this is like It's Not Gonna Save Us but it's a huge difference it gives us the time to do Safety Research to deal with to build gliders to build Rockets it gives us time to you know actually study these systems like actually understand to integrate them Society to figure out what is the right regulation like how do we want to integrate these things into society like a thing that like you know opening AI likes to say is like oh we think the safest thing is to do iterative deployment and so they can integrate into society which is code word for LOL we release it immediately and just you know throw it on the market if openai had built a gpd3 and then completely stopped and you know and done you know only stay at GB3 until they fully understood it they had full safety control of it and you know all Society had like totally integrated it culturally and we had like regulation that like totally handles it and we're all comfortable with it and then they had produced gp4 yeah you know what fine that's fair honestly totally fair if this was the way our society handled AI fantastic I think this is great you know I want technological progress too of course I just want it to be go well and you know this is not the shape of a technological path that goes well yeah so what you're saying saying is that instead of testing chat gbt 3 or 4 inside of a contained environment in the lab so that we can fully unpack it and understand it open AI is just eating it out into the world and using the world as the test bed not the contained environment as the test bed it's even worse than that never mind a contained environment they're eating out you know the one thing and then eating out the second one before we've even you know recovered from Whiplash from the first one yeah right and so just to really unpack these two exponential curves we have the hardware curve and the software curve the the hardware curve is the thing that you're saying that we can easily and objectively place a cap on a government enforce cap on that curve with that specific uh certain to the power of a number that we can totally measure and so that that seems like a solved problem if we can actually get the governments to do that then there's the other equation which is like we are still in the early days of these AI model in the first place and so even if we do cap the hardware we can we still have at least a handful of orders of magnitudes efficiency in the algorithms that have access to that hardware and so we have like Nvidia the GPU supplier on one side maybe there's others in the game as well I don't know how relevant AMD is in the world of AI but Nvidia is the big player that I think most people know on one side supplying gpus into the market and then we have the consumers of those gpus which are the big the people racing right the open AIS uh and so we need to in order to have a complete uh contain containment of the problem there needs to be control on both sectors and you're saying the hardware side of things is relatively objective and easy and we we can prescribe action steps for governments to take those steps and I'm assuming on the software side of things it's a little bit more gray and we don't know how to proceed that well is my intuition correct here yeah that's definitely the case I mean you know easy is a strong word on the hardware side um I have talked to many people in policy positions and many of them are like this is literally impossible you are insane other people though have said oh no this is like totally something government can do especially using National Defense you know arguments like you know we already have a chip ban where we don't export h100s to China and stuff like this we already track this like the US government totally has the the affordances and the cape you know it has the capabilities and the incentives to do this this this is totally within their wheelhouse this is not some crazy new thing that has to be invented but getting the government to do anything is hell so this is a massive entertainment that being says so software is much more complicated I don't think we can or should try to ban mathematics that seems insane um I don't think this is a kind of level of coordination that humanity is capable of you know maybe we're some kind of like super smart enlightened aliens we could maybe like all handshake on it but like yeah I still don't think Humanity can really work that way but there are things that can be done here there are actual straightforward very easy and clear things that can be done here actually which is again does it say it doesn't fix everything but it does buy time so in particular the reason I think there are such easy wins here is because things are so bad because things are so bad no one's even trying to control these things there's a lot of low hanging fruit so one of the like very obvious things which is not again I'm not saying it's easy but I think this would make a difference is I think there should be strict liability for not just model deployers but also model Developers if you develop an AI system and someone uses it to commit a crime you should be charged as if you commit the crime yourself you should be liable and strict liability let's take a moment to unpack that strict liability is is when you the individual you cannot you cannot give your liability to an LLC strict liability is like yo you the individual that falls back on you yes just want to Define that term before we move on too far yes that's correct I think this would be the ideal case again I have had plenty you know a common section I'm sure is yelling at me right now I have heard from plenty of people of policy people telling me how this is impossible but I have also heard from very senior people that this is possible basically the number one step is removing 230 section 230 protections from AI systems this is already being discussed by senators in the Senate um so section 230 is the like you know internet neutrality thing where like if you host content which is illegal but you didn't know about it and you remove it you're not liable for it if your users were doing it and you're hands in it one of the obvious things just do not apply section 230 to AI uh simply if you develop a model or a system an AI system which causes actual criminal harm you are criminally liable for it personally this is currently not the case currently there's a again look I love open source I love my nerd friends but it's insane is that some nerd somewhere in a university can develop a voice cloning system you know take 15 second audio of anyone per filter on their voice and use it you know scan their parents to you know call and bomb threats you know whatever totally ruin their lives and the person who developed that model who posted it to GitHub has zero liability they don't it's not only that they don't have any legal liability they don't even feel responsible the people who do this feel no shame like it's not part of culture of Open Source culture to even like feel very responsible for this kind of things like oh you developed a cool thing you know throw it out there it's fine you know you know let the lawyers figure out how Society thinks it's not your problem like I had an actual conversation it's a real conversation ahead with um uh some tech people and I was talking about how I think there should be strict liability um those kind of things and then one of the first people sincerely made the point well that no that's impossible we can't do that like how would they possibly ever determine who is at fault this couldn't be done and then a friend of mine was like quartz that is what courts are for how do you not know what courts are so there's a huge disconnect between a lot of the tech sector and like how Civil Society actually functions like the the infrastructure of civil uh like actual functioning government and societies and courts and so on We have dealt with you know things that enable criminal activity in the past and you know like another great example is automobiles so automobiles there was a huge fight I think it was in the 70s if I remember correctly I might get the dates wrong I'm you know bad with remembering exact numbers there is a huge fight legal battle where automakers were arguing that they were not responsible for any deaths that occurred due to cars and including if their car is malfunctioned and a lot of people push back at that they said well no if your car malfunctions if the brakes you know don't work if it catches fire or whatever no the the maker of the car should be liable for that not you know the driver like how it's not his fault that the car caught fire and this was a massive legal battle this was not and now nowadays we do hold you know car companies liable if you build a car and it explodes and it kills somebody yeah you you your responsibility you built the car you put the car out there like of course you're responsible for this and of course there's a spectrum here no there's a large Spectrum I'm not saying I know where on the spectrum is the right thing you know should everyone who developed Linux be held personally responsible if someone uses Linux or something bad I think no but it's a spectrum it's not a yes or no it's not everyone is whole responsible or no one ever it's a spectrum and this is how laws work this is how courts work we as a society have to find where on the Spectrum are we comfortable and I'm simply stating I think we're way on the one side and we should move more towards the Middle where there is actual accountability that same conversation I had with this person um was very funny as a as a different person actually but same conversation when I talked about strict liability someone called me out and they're like okay Connor well you developed open source technology do you want to be held accountable if someone you uses your language models you developed a lutheri to cause harm and my answer is yes yes I would like to live in a society rise technologist if I cause harm to Civil Society if I cause harm to other people that I am held accountable to this I think this is good I think this is the kind of society I would like to live in I can see a large number of the crypto people which are 95 of this audience uh recoiling at this and and I'm sorry I want to give it well I want to give an anecdote from the from the crypto World which I think is universally accepted as like we are in a legal battle with um the United States Department of Treasury because they deemed I don't know how familiar you are with this but this uh smart contract on ethereum called Tornado cash they deemed using that software uploaded to the ethereum blockchain using that as an American citizen is illegal and so I I am personally actually suing Janet Yellen in the department of Department of Treasury based on that and saying somebody a open source developer they live in the Netherlands uploaded this piece of Hardware to ethereum and I was able to use that as a tool a neutral Tool uh they're happy I was achieving some sort of a financial privacy because ethereum is transparent and open and everyone can see my transactions and I would like to have created some sort of private version of my wallet that no one used that no one knew about so I used tornado cash in order to achieve that end simultaneously so did North Korea uh and so my money was right next to North Korea and so the Department of Treasury deemed this piece of software to be illegal to use by all Americans and so a few of us in the crypto World banded together to sue the department treasury saying hey you cannot make a neutral piece of software illegal just because some Bad actors are doing bad things with it including perhaps funding the development of nuclear weapons and so I can see where but like when you tell me this about AI is like how else do we solve this AI alignment problem other than giving strict liability to the people that create the models that ultimately destroy the world to me I'm like well I don't have a better solution and so I don't know what else to do and so I'm torn at this I see the crypto code libertarian inside of me on one side of things and then the hey let's not die from AI on the other side of things and I see the conflict here yeah and I'm sure you have run into these conversations a number of times absolutely and like I think so thank you for sharing that like that's that's a great illustration of again coming back to the beginning of the podcast it's not about good and bad it's about finding the right thing I think you suing the US government is fantastic like I think you're doing a civil Duty here I think you're you're improving Society for all of us by doing this whatever comes out of this case like someone has to do it this is how Society progresses this is the mechanisms by which our our society finds consensus on what is the correct dial setting it's not that the government magically knows the correct dial setting no this is why in the USA we do Sue and this is great this is good it's a huge pain I know this must be so much pain for you to do all this like such a such a and that's why I'm like thanking you get out to the coin Center lawyers yeah like you know that's why I'm thanking you and these guys you know I don't have a horse in this race you know I don't know whether this is good or bad I I don't have a horse in this race but I think it's good that this is getting going to court I think this is good that someone is holding the position this is good and someone is holding the position this is bad and this should be thought out this should be debated this shouldn't be taken for granted that things are obviously good or obviously bad again beginner's conversation things are good or bad it's about decisions it's about consequences I don't know maybe the software being banned is net good maybe I don't know maybe it's not like the way you describe it I don't know like I I really don't know and I would love for you know people who know understand this technology much better than me people like you and you know people you know at the government to hopefully have like a better understanding of national security or something to battle it out and like actually hopefully find truth and our courts are not perfect you know I'm sure there's going to be a bunch of  and like whatever the verdict is No One's Gonna be happy with it you know I understand but this is how our society works this is how we make progress this is how we as a democratic you know liberal you know judicial you know law rule of law country make progress and I'm saying that this is painful but it just has to be done it's a price of living in a complex society it's a price of playing in a complex society with complex trade-offs with complex Technologies which I think crypto is I think crypto is an amazing technology that has incredibly complex you know benefits and downsides you know they're always blast radius you know FTX was a huge blast radius you know it's a massive loss radius right and that doesn't mean crypto is bad no like of course it doesn't like just because they're scammers doesn't mean it's bad but it also doesn't mean it's good it's just a technology it's neutral the same way AI is it is not good or bad it's about how do we as a society digest this technology how do we get as much of the good stuff well preventing as much as the battle and this is simply not easy this is all I'm saying I'm not saying I have the perfect solution it solves all the problems forever don't worry just do this I'm saying this is a hard process this is like doing science like in a sense going to court is like doing science you're doing like like you know law making science or engineering you're doing the epistemological labor that needs to get done to actually get to the right kind of civilizational software that we want to be running because we don't know what the right software is we don't know what the right laws are we don't know what the right liability or the right Insurance schemes I don't know it is right and we have to find out someone has to come up with them and they have to debate against people who come up with alternative things and we have to try things out and some of them blow up and then people Sue and then we try another sage and so on so I think this is a natural part of of how a functioning Society should work and sometimes this is a huge massive pain I understand but this is like I've talked before about like how like okay Humanity can't coordinate that well you know we can coordinate but like this is what coordination actually looks like coordination doesn't look like oh we're all happy friends and we hold hands and we all sing songs that's not what I I mean when I use the word coordination I know coordination I mean we Sue each other I mean we have defense I mean we have we we fight we argue civilly you know no violence you know we're very civil about it we have rules we follow laws we also follow laws that we don't agree with like there's a bunch of laws I don't agree with but I am willing to make compromises on this and I'm willing to compromise with the government and with my fellow citizens that I will like about you know if I go to a country where maybe I don't agree with all their laws while I'm there I'm not going to break them and this is not just because of like fear of retribution it's also because of contracts and just like you know making deals and like being fair and changing processes I think violence is extremely bad for this reasons because violence violates our contracts our social contract is this is not how we solve disputes if you disagree with something you don't use violence you sue you you have debates you start a campaign you start of you know you you start you you talk about a podcast you know whatever like there we have mechanisms and we should use them and they're not great like in case you haven't seen Twitter lately our coordination mechanisms aren't great they're not very good but there's something and we can do better we can coordinate it is possible it's not easy again at the beginning of this conversation when we were trying to unpack the different uh categories for how uh AI might come to destroy the world you called this um a deeply meta problem you talked about how this how we need to solve philosophical problems about what it means to be human it's a problem that really goes to the heart of The Human Condition and I've I've been talking to a quite a large number of AI people in the last like couple months or so and it seems it's been very interesting to me every time the topic of AI alignment and ABI safety comes up which is like almost all the time uh how different approaches the conversation can take across the board and it seems to be that there are so many different problems that need to be solved in order to solve the AI safety problem in the first place I'm wondering if you could just unpack I'm pointing you in a direction here and seeing if I can just unleash you here but it seems to be like the AI alignment problem goes very deep as to what it means to be human and what it means to solve problems in the first place and especially when we have are approaching this technology that I think if we as humans just like developed as a society over and over and over and over again like we re-rolled the dice of the human experiment you arrive at AGI almost all the time like 99.99 of the time so it it seems to be like hey Humanity we have arrived at what seems to be the largest problem that Humanity has ever faced nuclear war nuclear bombs big problem not as big right like uh disease big problem not as big this is like the big problem and so it seems to be posing very big questions and so as a Conor as we come to the end of this conversation I'm wondering how to tie this off with just understanding the relationship between what it means to be human and how to solve AI alignment so what what does this Vector of conversation sound like to you sounds like the real question you know the ultimate question um AR Layman is an important field and sometimes its definitions get stretched um to the point where like it starts encompassing these larger and larger meta problems I think this is not a coincidence I think really in a potential rephrasing of the alignment problem is the general question of how do you control a powerful system using a weaker system this is something that is not unique to AI this is a problem that exists in organizations in multicellular organisms you know cancer is a form of misalignment some of your cells start having different values from the rest of your cells and they start you know uh taking actions of maximizing actions which are harmful to the rest of your organ organism the our organism has alignment mechanisms it has stuff like apoptosis so like you know cell suicide that when there's mechanisms that like if you if it detects that it's go that a cell is going down a path which might be dangerous the cell kills itself and sometimes these mechanisms don't work you get cancers so this might be a bit of a torturous metaphor but like I do think there's actually a deeper truth here in that uh alignment and power and coordination are in a sense all facets of the same coin they're all facets of the same problem of how do you organize chaos how do you create stable systems equilibria how do you resist entropy how do you resist just the Eternal cold noise of the universe because by default that's all there is by default there's only cold nothingness that's what the universe is that's its space there's nothing else space and just random particles but then things can cohere into complexity they can cohere into they can they can they can form you know whether by simple mechanisms such as gravity or chemical things to you know create planets and stars and then we have higher and higher level things we start you know seeing complex molecules then we start seeing you know the first self-replicators we see you know RNA molecules and you know protocells you see you know early bacteria and Archaea and then they start to coordinate among each other you know a lot of theories is that the uh the mitochondria in the cell is actually uh was once a separate organism that got absorbed into our cells into eukaryotic cells and so there's a coordination happening there in a very abstract sense and then eventually these cells learn to organize an even larger scale to organize into multicellular organisms and then first very primitive you know probably like like sponges or like you know like jellyfish or something probably like very simple organisms but then stuff started to specialize but to specialize you have to coordinate better you have to have better coordination mechanisms for a brain cell to live it needs a lot of digestion cells to help it out and a lot of blood cells to help it out and a lot of like like the amount of infrastructure that needs to be in place and coordinate it to support a human brain is ridiculous there's a massive huge system and everyone has to play their part you know some muscle cell can't just be like ah you know screw it I'm gonna be a neuron now you know if that happens we call this cancer you know we this is a disease this is not coordinated it's not controlled Mike Levin has a lot of great work on this on like bioelectricity and like you know cell identity and stuff like this and then it goes further and then we get to the to the metal level to the you know we go from the genes the memes you know as Stephen Dawkins might say um where uh we go from purely genetic Evolution or like multicellular Evolution to memetic Evolution to cultural Evolution there becomes now it's about cultures and tribes and civilizations and uh you know companies and gods and religions and these kinds of things and these can also coordinate and mutate and you know war and you know change so there's this this recurring motif of Life of these like higher order patterns of complexity and like and coordination and stuff emerging and the way for these things to emerge is through coordination it is through coherence of smaller patterns cohering into larger emergent patterns on higher levels of scale and this is also where we humans are we're nestled here somewhere in the hierarchy you know somewhere above multicellular somewhere below you know pure mimetic you know religions or you know stories somewhere in between we're kind of like you know with one one foot we're in the realm of animals of like the Earth and one foot we're in the the realm of the gods or in the realm of mimetics of information of stories of religions or kind of this interface between these two things and this so what is the next step from here like where do we go like what does it mean like what does it mean for us to want something like in a sense like you know if we build AI this is pushing us further into the realm of mimetics of knowledge of pure abstract software of something that exists not as a brain not as a specific piece of Hardware but as a purely like it's basically a spirit you know just like an immaterial you know code it's the next step you know it goes up it goes you know from the transition from biologic purely biological purely physical you know physical biological you know then half memetic half biological to purely mimetic they can run on any substrate where there's a CPU they can run its code AI can copy itself and it can exist and it's purely memetic Realm so how do what does this mean well this means many things it means could be mean fantastic things culture allows us to do fantastic thing our control over mimetics or culture of technology or science it allows us to build this wonderful place we're in like look at this you know we've got internet and we got air conditioning like I can eat all the food I want that's great my animal is feeling my inner animal is feeling great about this whole like Civilization stuff seems pretty cool nothing else go wrong blast radius as we Dabble in the powerful as we dabble in Magic you know in like the you know technology so we dabble in this we you know blast radius increases power increases and if we and we need to get control over that thing so this is all just a very nice poetic refrainment of where we started this podcast it's about how do we deal with the upper level you know there's also a question of down level like this can the question of cancer control or like you know medicine is the question of how do we align the levels below us or engineering or like physics are the questions of how do we line the levels below us with r values now we have the question how do we line the thing above us with our values because we want some things you know there's many things I like and as many things I don't like I don't like my friends to die I don't like being in pain I would like a beautiful Universal of Cosmopolitan art and adventure and all these nice things and this is possible but it's not something that an animal can do it's not something me or you as half animals could even do it is something that God could do it is something that a very very powerful intelligence a memetic you know super creatures could do it's the same way that the medic super creatures that we call civilization has built technology that you or me could never build by ourselves and this is great but what are our alignment mechanisms you know our cells have apoptosis you know they have a bunch of seven even that's not perfect but now I'm saying okay medicine is great you know physics is great you know we're developing a lower level alignment tax we also need to develop the upper level alignment text I think it is possible that we can understand mimetics and Ai and optimization and also unless this is Magic that's like that's the that's the cool thing in a sense is that like it is like it is both magic and it's not magic it's like it is like information processing it is things that can be understood it is coordination you know humans who dissolve many coordination problems you know mean you can coordinate you know me and all the other people down the street we can all live together peacefully this is coordination this is controlling something you know working as part of something bigger than myself and if we want a really good outcome if you really want the world to go well if you really want you know our far descendants you know uh you know whether they're human or not to you know live a great life but we have to not blow this all up we have to actually make sure that the next step we make is not just some random thing that you know someone cooked up on a server somewhere but it's actually something we like actually something we endorse after something we want we don't want something to inherit the universe and just you know blow it up or use it for nothingness we want the universe to be something that we like and this is really the full problem of alignment it's not just the question of okay how do we solve this narrow problem how do we regulate this thing it's how do we think about what does it mean for humans to want things what do we want you know a lot of our preferences are culturally constructed you know a lot of people want things because their friends want it or because they saw it on TV is that a real preference or is that a fake preference I mean I don't know like this is something we should be asking what does it mean to be good is that even a well-cohered concept if we can't agree in any of this is there some way we can bargain is there some kind of you know harsanis Veil type solution where you know we build some super aligned Type 4 system it goes to all the humans you know figures out what they all want and makes some kind of like Fair distribution I said I don't know this is to what we really want is these type 4 system Type 4 Alliance systems that are so aligned that they figure it out for us you know they're like I'm gonna figure out all humans I'm gonna you know I'm gonna understand their psychology their traumas their their person you know their life story I'm gonna deeply deeply understand them in ways that you know no human could understand them I will use all this you know incredible superhume intelligence in order not to do something random but to do what are these you know poor half-animals actually would like what would actually make them happy would actually be good for them and I have no idea how to build such a thing I mean I have a few ideas but I don't know this is this system is not forbidden by physics there's no reason that such a system cannot be constructed but we are so far away from that we're not even going to get to there if we can even get to the point where we can just have like systems that we can carefully use we coordinate on and we use them to you know make cool world you know where people are happy or people don't have to fear for the next day where people aren't hungry where people can just chill that'll be pretty nice I we can get there it's technically possible but it requires not just technology it's like technology is not free progress is not free if you just take the shortest path on the on the tech tree you don't get to the final Tech you blow yourself up on the way there is a final Tech somewhere deep deep down the tree and we're very careful about how we Traverse this if we you know improve our coordination Tech if we you know work carefully about this and we avoid the existential the traps you know the traps along the tree we can get there and that's you know I think like Eleazar as such would mean is truly solving alignment that final Tech that we're like we've understood what are values what does humanity mean how do we bargain between values how do we you know think about reality how do we think about you know meaning how do we think about all these things and how and not just that but you know never mind the philosophy here this is not just about talk this is how do we turn this into code how do we turn this how do we build this it's not a thing that I like to think about philosophy that I think is often lost in modern philosophy is that good philosophy should make you stronger philosophy should solve problems if your philosophy isn't ultimately going to solve problem why are you doing it the what it really this if you really do philosophy if you really go to the end of philosophy you should come out on the other side with something that allows you to do these things that allows you to build these systems to create this great future to make the universe good to make it great and I don't know what that means I'm not claiming that I do like oh God no you know I barely have a few technical ideas about how we can get to type 2 systems a little bit of an idea but even that it's going to be tough we just all of us you know Humanity are you know technology our politics everything uh it's going to be heroic it's going to be heroic task to get to that point but hopefully we just have to be heroic one more time Connor I really appreciated all of that and that was a a very cerebral and deep conversation if if bankless listeners are out there and they want to continue this conversation where should we send them I know that your website your company conjecture has a fantastic blog I could definitely put those links in the show notes is there any other place where uh people who want to continue this conversation might want to end up so our website is a bit uh not a super up to date but I think you're thank you for your kind words anyways I'm on Twitter at MP collapse I'm not super reactive but I exist there I sometimes hang around to lose three eyes Discord still not super common but you know if you pick me there's a decent chance I'll respond at some point um I go on a lot of podcasts um I do a lot of uh you know a lot of stuff like this um if you're interested in learning more about some of the regulation things I'm talking about there is a website we have made called stop.ai there's an Ask risk as for the AI is that we're not trying to stop all AI just a specific type of AI there's a lot of content on there about like fleshing out some of these these things um yeah and in general you know I'm around and uh this is not the last conversation I think that you know will be had on this topic oh I definitely think that's correct Bank location you know the deal crypto is risky ether's risky AI alignment is risky and apparently we need a heroic effort to solve that problem uh but nonetheless you can lose what you put in we are headed west this is Frontier it's not for everyone but we are glad you are with us on the bankless journey thanks a lot [Music] thank you [Music] 