the actual catastrophe scenario for AI looks like big advancement in a research lab the AI there goes over a critical threshold could be like can't write the next AI That's snowballs it gets an immense technological advantage if it's smart it doesn't announce itself it doesn't tell you that there's a fight going on it emails out some instructions to one of those labs that'll synthesize proteins from the DNA builds tiny diamondoid bacteria a diamondoid bacteria replicate you know a couple of days later everybody on earth falls over dead in the same second that's the disaster scenario if it's as smart as I am if it's smarter it might think of a better way to do things 