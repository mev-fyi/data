I think that we are hearing the last twins to start to blow the fabric of reality start to fray this thing alone cannot end the world but I think that probably some of the vast quantities of money being blindly and helplessly piled into here are going to end up actually accomplishing something where we explore the frontier of Internet money and internet Finance this is how to get started how to get better how to front run the opportunity this is Ryan Sean Adams I'm here with David Hoffman and we're here to help you become more bankless okay guys uh we wanted to do an episode on AI at bankless but I feel like what we asked for we accidentally wade into the deep end of the pool here yeah um and I think as we get before we get into this episode it probably warrants a few comments I'm gonna say a few things I'd like to hear from you too but one thing I want to tell the listener is um don't listen to this episode if you're not ready for an existential crisis okay like I'm kind of serious about this um I'm leaving this episode shaken uh and I don't say that lightly uh in fact David I think you and I will have some things to discuss in the debrief as far as how this impacted you um but this was an impactful one it sort of hit me during the recording and I didn't know fully how to react I honestly am coming out of this episode wanting to refute some of the claims made in this episode by Our Guest um Eliezer ikowski who makes the claim that humanity is on the cusp of developing an AI That's going to destroy us and that there's really not much we can do to stop it there's no way around it yeah I have a lot of respect for this guess let me say that so it's not as if I have some sort of big brain technical disagreement here in fact I don't even know enough to fully disagree with anything his he's saying but the conclusion is so dire and so existentially heavy that um I'm worried about it impacting you listener uh if we don't give you this warning going in um I also feel like David as interviewers maybe we could have done a better job uh I'll say this on behalf of myself sometimes I peppered him with a lot of questions uh in one Fell Swoop and he was probably only ready to synthesize one at a time I also feel like we got caught flat-footed at times I wasn't expecting his answers to be so Frank and so dire David like it was just um a rift of Hope and I appreciated very much the honesty as we always do on bankless but I appreciated it almost in the way that a patient might appreciate the honesty of their doctor telling them that their illness is terminal like it's still really heavy news isn't it um so that is the context going to this episode I will say one thing in good news for our failings as interviewers in this episode uh they might be remedied because at the end of this episode after we finished with uh hit the record button to stop recording Eliezer said he'd be willing to provide additional q a um episode with the bankless community so if you guys have questions and if there's sufficient interest for Ellie user to answer uh tweet us to express that interest hit us in Discord get those messages over to us and let us know if you have some follow-up questions he said if there's enough interest in the community in the crypto Community I'll say he'd be willing to come on and do another episode with with follow-up q a maybe even a vitalik and Eliezer episode is in store that's a possibility that we threw to him we've not talked to vitalik about that too but I just feel a little overwhelmed by the subject matter here um and that is the basis uh the Preamble through which we are introducing this episode um David there's a few benefits and takeaways I want to get into but before I do can you comment or reflect on um on that Preamble what are your thoughts going to this one yeah we approached the end of our agenda for every bankless podcast there's a equivalent agenda that runs alongside of it but it once we got to this Crux of this conversation it was not possible to proceed in that agenda because what was the point Nothing Else Matters and nothing else really matters which is also just kind of relates to the subject matter at hand uh and so uh as we proceed you'll see us kind of circle back to the same inevitable conclusion over and over and over again which ultimately is kind of the punch line of the content uh and so I'm of a specific disposition where stuff like this I kind of am like oh whatever okay just go about my life other people uh are of different dispositions and take these things more heavily so uh Ryan's warning at the beginning is if you are a type of person to take existential crises directly to the face perhaps consider doing something else instead of listening to this episode I uh I think that is good counsel so a few things if you're looking for an outline of the agenda we start by talking about chat GPT is this a new era of artificial intelligence got to begin the conversation there number two we talk about what an artificial super intelligence might look like how smart exactly is it what types of things could it do that humans cannot do number three we talk about why an AI super intelligence will almost certainly spell the end of humanity and why it'll be really hard if not impossible according to our guest to stop this from happening and number four we talk about if there is absolutely anything we can do about all of this we are heading careening maybe towards the abreast Abyss can we divert Direction and not go off the cliff that that is the question we ask Elisa with um David I think you and I have a lot to talk about during the debrief all right guys the debrief is an episode that we record right after the episode it's available for all bankless citizens we call this the bankless premium feed you can access that now to get our raw and unfiltered thoughts on the episode and I think it's going to be pretty raw this time around David I'm like I didn't I didn't expect this to hit you so hard oh I'm dealing with it right now really and this is probably you know it's not too long after the episode so uh yeah I don't know how this how I'm gonna feel tomorrow but um definitely want to talk to you about this and maybe yeah have you talk I'll put I'll put my psych hat on yeah please I'm gonna need some help uh guys we're gonna get right to the episode with Eliezer but before we do we want to thank the sponsors that made this episode possible including Kraken our favorite recommended exchange for 2023. Kraken has been a leader in the crypto industry for the last 12 years dedicated to accelerating the global adoption of crypto Kraken puts an emphasis on security transparency C and client support which is why over 9 million clients have come to love kraken's products whether you're a beginner or a pro the Kraken ux is simple intuitive and frictionless making the kraken app a great place for all to get involved and learn about crypto for those with experience the redesigned Kraken Pro app and web experience is completely customizable to your trading needs integrating key trading features into one seamless interface Kraken has a 24 7 365 client support team that is globally recognized Kraken support is available Wherever Whenever you need them by phone chat or email and for all of you nfters out there the brand new Kraken nft beta platform gives you the best nft trading experience possible Rarity rankings no gas fees and the ability to buy an nft straight with cash does your crypto exchange prioritize its customers the way that Kraken does and if not sign up with Kraken at kraken.com bankless the Phantom wallet is coming to ethereum the number one wallet on Solana is bringing its millions of users and beloved ux to ethereum and polygon you haven't used Phantom before you've been missing out Phantom was one of the first wallets to Pioneer Solana staking inside the wallet and will be offering similar staking features for ethereum and polygon but that's just staking Phantom is also the best home for your nfts Phantom has a complete set of features to optimize your nft experience pin your favorites hide the Uglies remove the spam and also manage your nft sale listings from inside the wallet Phantom is of course a multi-chain wallet but it makes chain management easy displaying your transactions in a human readable format with automatic warnings for malicious transactions or phishing websites Phantom has already saved over 20 000 users from getting scammed or hacked so get on the Phantom waitlist and be one of the first to access the multi-chain beta there's a link in the show notes or you can go to Phantom dot app waitlist to get access in late February hey bankless Nation if you're listening to this it's because you're on the free bankless RSS feed did you know that there is an ad-free version of Bank list that comes with the bankless premium subscription no ads just straight to the content but that's just one of many things that a premium subscription gets you there's also the token report a monthly bullish bearish neutral report on the hottest tokens of the month and the regular updates from the token report go into the token Bible your first stop shop for every token worth investigating in crypto bankless premium also gets you a 30 discount to the permissionless conference which means it basically just pays for itself there's also the airdrop guide to make sure you don't miss A Drop in 2023 but really the best part about bankless premium is hanging out with me Ryan and the rest of the bankless team in the Inner Circle Discord only for premium members want the alpha check out Ben the analyst Degen pit where you can ask him questions about the token report got a question I've got my own q a room for any questions that you might have at bank list we have huge things planned for 2023 including a new website with login with your ethereum address capabilities and we're super excited to ship what we are calling Bank list 2.0 soon TM so if you want extra help exploring the frontier subscribe to bankless premium it's under 50 cents a day and provides a wealth of knowledge and support on your journey West I'll see you in the Discord bankless Nation we are super excited to introduce you to our next guest Eliezer utakowski is a decision theorist he's an AI researcher he's the cedar of the less wrong Community blog fantastic blog by the way there's so many other things that he's also done I can't I can't fit this in the in the short bio that we have to introduce you to uh Eliza but most relevant probably to this conversation is he is working at the machine intelligence Research Institute to ensure that when we do make General artificial intelligence it doesn't come kill us all or at least it doesn't come banned cryptocurrency because that would be a poor outcome as well Eliezer it's great to have you on Bagless how you doing ah you know within one standard deviation of my own peculiar little mean fantastic um you know we want to start this conversation with something that is jumped onto the scene I think for a lot of mainstream folks quite recently and that is um chat gbt so apparently over 100 million or so have logged on to chat GPT quite recently I've been playing it with it myself I found it very friendly very useful it even wrote me a sweet poem that I thought was very heartfelt and almost human-like I know that you have major concerns around AI safety and we're going to get into those concerns but can you tell us in the context of like something like a chat GPT is this something we should be worried about that this is going to turn evil and enslave the human race like how should worried should we be about chat gbt and Bard and sort of the the new AI That's entered the scene recently PT itself zero it's not smart enough to do anything really wrong or really right either for that matter and what gives you the confidence to say that how do you know this excellent question so every now and then somebody figures out how to put a new prompt into chat EPT um you know one time somebody found that it would talk well not Chachi petite but one of the earlier generations of Technology they found that it would sound smarter if you first told it it was Elias radkowski you know there's there's other prompts too but that one's one of my favorites so there's there's untapped potential in there that people haven't figured out how to prompt yet but when people figure it out it moves ahead sufficiently short distances that I do feel fairly confident that there is not so much untapped potential in there that it is going to take over the world it's like making small movements and to take over the world that need a very large movement there's capability like there's there's there's places where it falls down on predicting the next line that a human would say in its shoes that seem indicative of probably that capability just is not in the giant inscrutable matrices or it would be using it to predict the next line which is very heavily what it was optimized for so there's there's going to be like some untapped potential in there but I do feel quite confident that the upper range of that untapped potential is insufficient to outsmart all the living humans and implement the scenario that I'm worried about so even so though is chat GPT a big leap forward in the journey towards AI in your mind or is this fairly incremental it's just for whatever reason it's caught mainstream attention gpt3 was a big Leap Forward um there's rumors about gpt4 which you know who knows um Chachi PT is a commercialization of the actual AI in the lab trying to Leap Forward um compare if you weren't if you had never heard of gpt3 or gpt2 or the whole range of text Transformers before chat GPT suddenly entered into your life then that whole thing is a giant leap forward but it's a giant leap forward based on a technology that was published in if I recall correctly 2018. I think the what's going around in everyone's Minds right now and and the bankless listenership and crypto people at large are largely futurists so everyone I think listening understands that in the future there will be sentient AIS perhaps around us at least by the time that we all move on from this world so like we all know that this future of AI is coming towards us and when we see something like chat gbt everyone's like oh is this the moment in which our world starts to become integrated with AI and so Elisa you've been uh you know tapped into the world world of AI is this are we on to something here or is this just another you know fad that we will internalize and then move on for and then the real moment of generalized AI is actually further much further out than we're initially giving credit for like where are we in this timeline you know predictions are hard especially about the future I sure hope that this is where it saturates this or like the Next Generation it goes only this far it goes no further it doesn't get used for to make more steel or build better power plants first because that's illegal and second because the technology is like the the large language model Technologies basic vulnerabilities that's not reliable like if you it's it's good for applications where it works eighty percent of the time but not where it needs to work 99.999 of the time this thing this this class of Technology can't drive a car because it will sometimes crash the car um so I hope it saturates there I hope they can't fix it I hope we get like a 10-year AI winter after this this is not what I actually predict I think that we are hearing the last twins to start to blow the fabric of reality start to fray this thing alone cannot end the world um but I think that probably some of the vast quantities of money being blindly piled it blindly and helplessly piled into here are going to end up actually accomplishing something you know not most of the money that that can Justice like never happens in any field of human endeavor but one percent of 10 billion dollars is still a lot of money to actually accomplish something so I think uh listeners I think you've heard eleazar's you know thesis on this which is a pretty uh dim with respect to um AI alignment and we'll get into what we mean by AI alignment and very worried about AI safety related issues but but I I think a lot of people for a lot of people to even sort of worry about AI safety and for us to even have that conversation I think they have to have some sort of grasp of what AGI looks like that that is um I understand that to me an artificial general intelligence and this idea of a of a super intelligence um can you tell us like if if there was a super Intelligence on the scene what would it look like I mean is this going to look like a a big chat box on the internet that we can all type things into it's like an oracle type thing or is it like some sort of a robot that it's going to be constructed in a secret government lab uh is this like something somebody could accidentally create in a dorm room like what are we even looking for when we uh talk about the term AGI and uh super intelligence but first of all I'd say those are pretty pretty uh distinct Concepts um chat GPT is shows a very wide range of generality compared to the previous generations of AI not like very wide generality compared to gpt3 not like literally the lab research that got commercialized that's the same generation but compared to you know stuff from 2018 or even 2020. chat GPT is gender is better at a much wider range of things without having been explicitly programmed by humans to be able to do those things it can it to imitate a human as best it can it has to capture all of the things that humans can think about than it can which is not all the things it's still not very good at long multiplication unless you give it the right instructions which case suddenly can do but you know so it's like significantly more General than the previous generation of artificial Minds um humans were significantly more General than the previous generation of chimpanzees or rather Australopithecus or latest or last common ancestor um humans are not fully General if humans were fully General we'd be at good at coding as we are at football throwing things or running you know we're not some of us are you know okay at programming but you know we're not specked for it we're not fully General Minds you can imagine something that's more General than a human and if it runs into something unfamiliar it's like okay let me just go reprogram myself a bit and then I'll be as adapted to this thing as I am to you know anything else so chat GPT is less General than a human and but it's like generally genuinely ambiguous I think whether it's more or less General than say our cousins the chimpanzees or if you don't believe it's as general as a chimpanzee a dolphin or a cat so this idea of of uh general intelligence is sort of a range of things that it it can actually do a range of ways that can apply itself how wide is it how much reprogramming does it need how much retraining does it need to get make it do a new thing bees build hives beavers build dams a human will look at a beehive and imagine a honeycomb shaped Dam and that's like humans alone in the animal kingdom but that doesn't mean that we are general intelligence as it means we're significantly more generally applicable intelligences than chimpanzees it's not like we're all that narrow we can walk on the moon we can walk on the moon because there's aspects of our intelligence that are like made in full generality for universes that contain simplicities regularities things that recur over and over again we understand that a steal is hard on Earth it may stay hard on the moon and because that of that we can build Rockets walk on the moon breathe amid the vacuum chimpanzees cannot do that but that doesn't mean that humans are the most General possible things the thing that is more General than us that figures that stuff out faster is the thing to be scared of if it's if the purpose is to which it turns our its intelligences are not ones that we would recognize as nice things even in the most Cosmopolitan and embracing senses of you know what's worth doing and and you said this idea of a general intelligence is different than the concept of super intelligence which which I also brought into that uh first part of the question how is super intelligence different than general intelligence well because chat GPT has a little bit of of General intelligences general intelligence humans have more general intelligence a super intelligence is something that can beat any human and the entire human civilization at all the cognitive tasks um I don't know if the efficient market hypothesis is something where I can rely on uh yes the entire we're all crypto investors here we understand the efficient market hypothesis for sure so the efficient market hypothesis is of course not generally true like it's not true that literally all the market prices are smart are smarter than you it's not true that all the prices on Earth are are smarter than you even the most arrogant person who is at all calibrated however still thinks that the efficient market hypothesis is true relative to them 99.99999 of the time they only think that they know better about one in a million prices there might be important prices now the price of Bitcoin is an important price it's not just a random price but if you were like but if the efficient market hypothesis was only true to you ninety percent of the time you could just like pick out the 10 of the remaining prices and and and compound like and and double your money every day on the stock market and nobody can do that literally nobody can do that so this this property of relative efficiency that the market has to you that the price that the price is estimated the future price it already has all the information you have not all the information that exists in principle maybe not all the information that the best Equity but relative to you it's efficient relative to you for you if you pick out a random price like the price of Microsoft stock something where you've got no special advantage that that estimate of its price a week later is efficient relative to you you can't do better than that price we have much less experience with the notion of instrumental efficiency efficiency in choosing actions because actions are harder to aggregate estimates about than prices so you have to look at say um Alpha zero playing chess or just you know like stockfish whatever the latest stockfish number is and advanced chess engine when it makes a chess move you can't do better than that chess move it may not be the optimal chest move but if you pick a different chest move you'll do worse that that you'd call like a kind of in efficiency of action given its goal of winning the game there is once you know its move unless you consult some more powerful AI than stockfish you can't figure out a better move than that super intelligence is like that with respect to everything with respect to all of humanity it is relatively efficient to humanity it has the best estimates it not not perfect estimates but the best estimates and its estimates contain all the information that you've got about it its actions are the most efficient actions for accomplishing its goals if you think you see a better way to accomplish its goals you're mistaken so you're saying this is a super intelligence we'd have to imagine something that knows all of the chess moves in advance but here we're not talking about Chess we're talking about everything it knows all of the moves that we would make and the most Optimum pattern including moves that we would not even know how to make and it knows these things in advance I mean how how would like human beings sort of experience such a super intelligence I think we still have a very hard time imagining something smarter than us just because we've never experienced uh anything like it before of course you know we all know somebody who's a genius level IQ maybe quite a bit smarter than us but we've never encountered something like uh that you're describing some sort of mind that is super intelligent what sort of things would it be doing like that humans couldn't how would we experience this in the world I mean we do have some tiny bit of experience with it we have experience with with chess engines where we just can't figure out better moves than they make we have experience with market prices where even though your uncle has this you know like really long elaborate story about Microsoft stock you just know he's wrong why is he wrong because if he was correct it would already be incorporated into the stock price and this notion and especially because the Market's efficiency are not perfect like that whole downward swing and then upward move in covid um I have friends who made more money off that than I did but I like still managed to buy buy back into the broader stock market on the exact day the low you know basically coincidence but so the markets aren't perfectly efficient but they're almost but they're efficient almost everywhere and that and that sense of like deference that sense that your weird Uncle can't possibly be right because the hedge funds would know it you know unless he's talking about Kobe in which case maybe is right if if you have the right choice of weird Uncle you know like I I have weird friends who are like maybe better calling these things than your word uncle but yeah so so among humans it's subtle and then with super intelligence it's not subtle just massive Advantage but not perfect it's not that it knows every possible move you'll make before you make it it's that it's got a good probability distribution about that and it you know has figured out you know it has figured out all the good moves you could make I figured I'd reply to those and and what's that I mean like in practice what's that like well unless it's limited narrow super intelligence I think you'd mostly don't get to observe it because you are dead unfortunately what so you can so you know like stockfish make strictly better chest moves than you but it's playing on a very narrow board and the fact that it's better at you than chest doesn't mean it's better at you than everything and I think that the actual catastrophe scenario for AI looks like big advancement in a research lab may be driven by them getting a giant venture capital investment and being able to spend 10 times as much on gpus as they did before may be driven by a do algorithmic Advanced like Transformers may be driven by hammering out some tweaks in last year's algorithmic Advanced that that's the thing to finally work efficiently and the AI there goes over a critical threshold which you know like most obviously could be like can write the next AI that was you know you know that's that's so obvious that like science fiction writers figured it out almost before there were computers possibly even before there were computers I'm not sure what the exact dates here are but if it's better at you than everything it's better at you than building AIS that's snowballs it gets an immense technological advantage if it's smart it doesn't announce itself it doesn't tell you that there's a fight going on it emails out some instructions to one of those labs that'll that'll synthesize DNA and synthesize proteins from the DNA and get some proteins mailed to a you know hapless human somewhere who gets paid a bunch of money to mix together some stuff they got in the mail in a file you know like smart people will not do this for any sum of money many people are not smart builds the you know the ribosome but the ribosome that builds things out of covalently bonded diamondoid instead of proteins folding up and held together by Van Der waals forces builds tiny diamondoid bacteria the diamondoid bacteria replicate using atmospheric carbon hydrogen oxygen nitrogen and sunlight and you know a couple of days later everybody on earth falls over dead in the same second foreign that's the disaster scenario if it's as smart as I am if it's smarter it might think of a better way to do things but it can at least think of that if it's relatively efficient compared to humanity because I'm inhumanity and I thought of it this is I've got a million questions but I'm gonna let David go first yeah so we've introduced uh sped around in the introduction of a number of different concepts which I want to go back and take our time to really dive into there's the AI alignment problem there's AI escape velocity uh there is uh the question of what happens when AIS are so incredibly intelligent that humans are to AI is what ants are to us so I I want to kind of go back and Tackle these Eliezer one one by one uh we started this conversation talking about chat gbt and everyone's up in arms about Chad gbts like oh it and you're saying like yes it's a great step forward in the generalizability of some of the technologies that we have in the AI world all of a sudden chat gbt becomes immensely more useful and it's really stoking the imaginations of people today but what you're saying is it's not the thing that's actually going to be the thing to reach escape velocity and create uh create super intelligent AIS that perhaps might be able to enslave us but my question to you is how do we know that they don't enslave you but sorry go on yeah sorry murder David kill all of user was very clear on that so if it's not chat GPT how when where it like how close are we and like how because there's this like unknown Event Horizon where you you kind of alluded to it we're like we make this AI that we train it to create a smarter Ai and that's smarter yeah it's so incredibly smart that it hits escape velocity and all of a sudden these dominoes fall how close are we to that point and do we are we even capable of answering that question well and also when I know well when you were talking Alias it was like like if we had already crossed across that event horizon like a smart AI wouldn't necessarily broadcast that to the world and it's possible we've already crossed that event horizon is it not I mean it's theoretically possible but seems very unlikely somebody would need inside their lab an AI that was like much more advanced than the public AI technology and as far as I currently know the best labs and the best people are throwing their ideas to the world like they don't care now that could and there's probably some secret government labs with like secret government AI researchers my pretty strong guess is that they don't have the best people and that those labs are like nothing are like could not create chat GPT on their own because chat GPT took a whole bunch of fine twiddling and tuning and visible access to Giant GPU farms and that they don't have the people who know how to do the twiddling and tuning this is just a guess could you walk us through uh one of the big um things that you spend a lot of time on is this thing called the AI alignment problem uh some people are convinced that or not some people are not convinced that when we create AI that AI won't really just be fundamentally aligned with humans I don't believe that you fall into that camp I think you fall into the camp of when we do create this super intelligent generalized AI we are going to have a hard time uh aligning with it in terms of our morality and our ethics can you can you walk us through a little bit of that thought yeah the dumb way to ask that question too it's like uh Elias why do you think that the AIS automatically hates us like why is it why does it want to kill us all the AI doesn't hate you neither does it love you and you're made of atoms that it can use for something else it's it's indifferent to you it's got something that actually does care about which makes no mention of you and and you have are made of atoms they can use for something else that that's all there is to it in the end the reason it the reason you're not in its utility function is that the programmers did not know how to do that the people who built the AI or the people who built the AI that built the AI that built the AI did not have the technical knowledge that nobody on Earth has at the moment as far as I know whereby you can do that thing and you can control in detail what that thing ends up caring about so this feels like where humanity is hurtling itself towards this uh what we're calling it again an event horizon where there's like this AI escape velocity and there's nothing on the other side as in we do not know what happens past that point as it relates to having some sort of super intelligent Ai and how it might be able to manipulate the world would you agree with that no um again the the stockfish chest playing analogy you cannot predict exactly what move it would make because in order to predict exactly what move that it would make you would have to be at least that good at chess and it's better than you this is true even if it's just a little better than you suck this is actually enormously better than you to the point that when once it tells you the move you can't figure out a better move without consulting a different AI um but even if it was just a bit better than you even um then you're in the same position but you know this kind of disparity also exists between humans you know if you ask me like where will Gary casparov move on this chessboard and like I don't know like maybe here and then Gary Kasparov moved somewhere else doesn't means that he's wrong it means that I'm wrong I can't PR if I could predict exactly where Gary kasprov would move on a chessboard I'd be Gary Casper I'd be at least that good at chess possibly better I could also be like able to predict him but also like see an even better move than that um so that's the that's these that's the that's an irreducible source of uncertainty with respect to Super intelligence or anything that's smarter than you um if you could predict exactly what it would do it'd be that smart yourself it doesn't mean you can predict no facts about it so with stockfish in particular I can predict it's going to win the game I know what it's optimizing for I know where it's trying to steer the board I can predict that I can't predict exactly what the board will end up looking like after stockfish has finished winning its game against me I can predict it will be in the class of states that are winning positions for black or white or whichever color stockfish picked because you know wins either way and that's similarly where I'm getting the kind of prediction about everybody being dead cause if everybody were alive then there'd be some state that the super intelligence preferred to that state which is all of the atoms making up these people and their Farms are being used for something else that it values more so if you postulate that everybody's still alive I'm like okay well like why is it you're like postulating that stockfish made a stupid chess move and ended up with a non-winning board position that's where that prediction class of predictions come from can you reinforce and reinforce this argument though a little bit so like why is it that an AI can't be nice sort of like a a gentle parent to us rather than sort of a murderer looking to deconstruct our atoms and apply you know apply for you somewhere else like what what are its goals and why can't they be aligned to at least some of our goals or maybe why can't it get into a status which is you know somewhat like like us in the ants which is largely we just ignore them unless they interfere in our business and come in our house and you know rate our zero boxes there's a bunch of different questions there so first of all the space of Minds is very wide all the humans are Imagine like this giant sphere and all the humans are in this like one tiny corner of the sphere and you know we're all like basically the same make and model of car same running the same brand of engine we're just all painted slightly different colors somewhere in that mind space there's things that are as nice as humans there's things that are nicer than humans they're things that are trustworthy and nice and kind in ways that no human can ever be and there's even things that are so nice that they can understand the concept of leaving you alone and doing your own stuff sometimes instead of hanging around trying to be like obsessively nice to you every minute and all the other famous disaster scenarios from ancient Science Fiction with folded Hands by Jack Williamson is the one I'm quoting there um we don't know how to reach into mind design space and pluck out an AI like that it's not that they don't exist in principle it's that we don't know how to do it and I know like hand back the conversational ball now and figure out like which which next question do you want to go down there well I mean uh why like why is it so difficult to sort of align an AI with even our basic um Notions of morality I mean I wouldn't say that it's difficult to line an AI with it with our basic Notions of morality I'd say that it's difficult to align Ai and a task like build two identical strawberries or no let me let me take this strawberry and make me another strawberry that's identical to this strawberry down to the cellular level but not necessarily the atomic level so it looks under the same under like a Standard Optical microscope but maybe not a scanning electron microscope you know do that don't destroy the world as a side effect now this does intrinsically take a powerful AI there's no way you can make it easy to align by making it stupid to build something that's cellular identical to a strawberry I mean mostly I think the way that you do this is with like very primitive nanotechnology we could also do it using very Advanced biotechnology and these these are not technologies that we already have so it's got to be something smart enough to develop new technology never mind all the subtleties of morality I think we don't have the technology to align an AI to the point where we can say build me a copy of the strawberry and don't destroy the world why do I think that well case in point look at Natural Selection building units natural selection mutates the humans a bit runs another generation the fittest ones reproduce more their genes become more prevalent to the Next Generation natural selection hasn't really had very much time to do this to modern humans at all but you know the hominid line the mammalian line go back a few million Generations and this is an example of an optimization process building an intelligence and natural selection asked us for only one thing make more copies of your DNA make your alleles more relatively prevalent in the gene pool maximize your inclusive reproductive Fitness not just like your own reproductive Fitness but your you know two brothers or eight cousins as the joke goes because they've got on average one copy of your jeans two brothers eight cousins this is all we were optimized for for millions of generations creating humans from scratch from the first accidentally self-replicating molecule internally psychologically inside our minds we do not know what genes are we do not know what DNA is we do not know what alleles are we have no concept of inclusive genetic fitness until it you know our genetic our scientists figure out what that even is we don't know what we were being optimized for for a long time many minions thought they'd been created by God and this is when you when you use the hill climbing Paradigm and optimize for one single extremely pure thing this is how much of it gets inside in the ancestral environment in the exact distribution that we were originally optimized for humans did tend to end up using their intelligence to try to reproduce more put them into a different environment and all the little bits and pieces and fragments of optimizing for Fitness that were in us now do totally different stuff we have sex but we wear condoms if natural selection had been a foresightful intelligent kind of engineer that was able to Engineers think successfully it would have built us to be revolted by the thought of condoms to met men who would be lined up and fighting for the for the rights to donate to sperm banks and in our in our natural environment the little drives that got unto us happened to lead to more reproduction but distributional shift run the humans out of their out of their distribution over which they were optimized you get totally different results and gradient descent is by default just like do not quite the same thing it's going to do a weirder thing because natural selection has a much narrower information bottleneck in one sense you could say that natural selection was at an advantage because it finds simpler Solutions you could imagine some hopeful engineer who just built intelligences using gradient descent and found out that they end up wanting these like thousands and millions of little tiny things none of which were exactly what what the engineer wanted and being like well let's try natural selection instead it's got a much sharper information bottleneck it'll find the simple specification of what I want but we actually got there as humans then gradient descent probably may be even worse but but more importantly I'm just pointing out that there is no physical law computational law mathematical logical law saying when you optimize using hill climbing on a very simple very sharp Criterion you get a general intelligence that wants that thing so just like natural selection our tools are too blunt in order to get to that level of granularity to like program in some sort of morality into these super intelligent systems or build me a copy of a strawberry without destroying the world yeah the tools are too blunt so I just want to make sure I'm I'm following with it with what you were saying uh I think the conclusion that that you left uh left me with is that my brain which I consider to be at least decently smart is actually a byproduct an accidental byproduct of this desire to reproduce and it's actually just like a tool that I have uh and just like conscious thought is a tool uh which is a useful tool in means of that end and so if we're applying this to to Ai and ai's desire to achieve some certain goal what's what's the parallel there I mean every organ is your body is a reproductive organ if it didn't help you reproduce you would not have an organ like that your brain is no exception this is merely conventional science and like merely the conventional understanding of the world I am not saying anything here that ought to be at all controversial you know I'm sure it's controversial somewhere but you know within it within a pre-filtered audience it should not be at all controversial um and this is like the obvious thing to expect to happen with AI because why wouldn't it what new law of existence has been invoked whereby this time we optimize for a thing and we get a thing that wants exactly what we optimized for on the outside so what are the types of goals an AI might want to pursue what types of utility functions is it going to want to pursue off the bat is it just those that's been programmed with like make it an identical strawberry well the whole thing I'm saying is that we do not know how to get goals into a system we can cause them to do a thing inside a distribution they were optimized over using gradient descent but if you shift them outside of that distribution I expect other weird things start happening when they reflect on themselves other weird things start happening what kind of utility functions are in there I mean darn defino I think you'd have a pretty hard time calling the shape of humans from Advance by looking at Natural Selection the thing that natural selection was optimizing for if you'd never seen a human or anything like a human if we optimize them from the outside to predict the next line of human text like gpt3 I don't actually think this line of Technology leads to the end of the world but if it but maybe it does and you know like gpt7 you know there's probably a bunch of stuff in there too that desires to accurately model things like humans under a wide range of circumstances but it's not exactly units because ice cream ice cream didn't exist in the natural environment the ancestral environment the environment of evolutionary adaptedness there was nothing with that much sugar salt fat combined together as ice cream there's we are not built to want ice cream we were built to want strawberries honey um a gazelle that you killed and cooked and had some fat in it and was therefore nourishing and gave you the all-important calories you need to survive salt so you didn't sweat too much and run out of salt we evolved to want those things but then ice cream comes along and it's fits the those taste buds better than anything that existed in the environment that we were optimized over so a very primitive very basic very unreliable wild guess but but at least an informed kind of wild guess maybe if you train a thing really hard to predict humans then among the things that it likes are tiny little pseudo things that meet the definition of human but weren't in its training data and that are much easier to predict or where the problem of predicting them can be solved in a more satisfying way where satisfying is not like human satisfaction but some other Criterion of thoughts like this are tasty because they help you predict the humans from the training data when we we talk about like um uh all of these like ideas about just like the the ways that AI thought will be fundamentally just incompatible or not be able to be understood by the ways that humans think and then also all of a sudden we see this like uh Rotation by Venture capitalists by just pouring money into AI do alarm Bells go off in your heads like hey guys you haven't thought deeply about these subject matters yet just like the uh immense amount of capital going into AI investment scare you I mean alarm Bells went went off for me in 2015 which is when it became obvious that this is how it was going to go down um I sure am now seeing the realization of that stuff I felt alarmed about back then pleaser is this uh this view that a AI is incredibly dangerous and that um AGI is going to eventually end humanity and that we're just creating toward a precipice uh would you say this is like the consensus view now or are you still somewhat of an outlier and like why aren't other smart people in this field as alarmed as you can you like Steel Man their arguments foreign again like several questions sequentially there is it the consensus view no do I think that the people who that do I think that the like people in The Wider scientific field who dispute this point of view do I think they understand it do I think they've done anything like an impressive job of arguing against it at all no they like if you look at the like famous prestigious scientists who sometimes make a little fun of this view in passing um by either making up arguments rather than deeply considering things that are held to any standard of rigor and people outside their own fields are able to validly shoot them down I have no idea how to pronounce this last name Francis c-h-o-l-l-e-t if you know like said something about like ah this you know I forgot his exact words but it's something like I never hear any good Arguments for stuff and I was like okay here are some good Arguments for stuff and you can read like the reply from utkowski to c-h-o-l-l-e-t and Google that and that'll give you some idea of what the like eminent voices versus like the reply to the eminent voices sound like and you know like Scott Aronson who's off who is who at the time was often complexity Theory um you know like it was like that's not how no free launch theorems work correctly and and that's so yeah I think the State of Affairs is we have eminent scientific voices making fun of this possibility but not engaging with the arguments for it now if you step away from the eminent scientific voices you can find people who are more familiar with all the arguments and disagree with me and I think they lack security mindset I think that they're engaging in the sort of blind optimism that many many scientific Fields throughout history have engaged in where when you're approaching something for the first time you don't know why it will be hard and you imagine easy ways to do things and the way that this is supposed to naturally play out over the history of a scientific field is that you run out and you try to do the things and they don't work and you go back and you try to do other clever things and they don't work either and you learn some pessimism and you start to understand the reasons why the problem is hard this is the field of artificial intelligence itself recapitulated this very common uh ontogeny of a scientific field where you know initially we had people getting together the Dartmouth conference I forgot what their exact famous phrasing was but it's something like we think we can make uh you know like we are wanting to address the problem of getting AIS to you know like understand language improve themselves and I forget even what else was there a list of what now sound like Grand challenges and we think we can make substantial progress on this using 10 researchers for two months and I think that that at the core is what's yeah I think that the core is what's going on they have not run into the actual problems of alignment they aren't trying to get ahead of the game they're not trying to panic early they're waiting for reality to hit them onto the head and turn them into grizzled Old cynics of their scientific field who understand the reasons why things are hard they're content with the predictable lifestyle life cycle of starting out as bright-eyed younger bright-eyed youngsters waiting for reality to hit them over the head with the news and if it wasn't going to kill everybody the first time that they're really wrong you'd be fine you know this is how science works if if we got unlimited free retries in 50 years to solve everything it'd be okay we could figure out how to align AI 50 years given on given unlimited retries you know the first team in with the with the bright-eyed optimists would destroy the world and people would go oh well you know it's not that easy they would try something else clever that would destroy the world people would go like oh well you know maybe this feels actually hard maybe this is actually one of the thorny things like computer security or something and yeah otherwise so what exactly went wrong last time why didn't these hopeful ideas played out oh like you uh you optimized for one thing on the outside and you get a different thing on the inside wow that's that's really basic all right uh can we even do this to using gradient descent can you even build this thing out of giant and screwable matrices of floating Point numbers that nobody understands at all you know maybe we need different methodology and 50 years later you'd have an aligned AGI now if we got a limited free retries without destroying the world you know that it did be did it play out this the same way that you know chat GPT played out it's you know not from 1956 or 55 or whatever it was to 2023 so you know about 70 years give or take a few and you know 70 years later you know just like we can do the stuff that seven years later we can do the stuff they wanted to do in the summer in 1955. you know 70 years later you'd have your aligned ATI problem is that the world got destroyed in the meanwhile and that's why we you know that's the problem there so this feels like a gigantic uh don't look up scenario if you're familiar with that movie there's it's a movie about like this asteroid hurtling to Earth but it becomes popular and in Vogue to not look up and not notice it uh and uh Eleazar you're the guy who's saying like hey there's an asteroid we have to do something about it and if we don't it's going to come destroy us if you had god mode over the progress of AI research and just Innovation and development what choices would you make that humans are not currently making today I mean I could say something like shut down all the large GPU clusters how how long do I have got mode do I do I get to like stick around for the for the 2020 decade for 2020 decade all right that that does make it pretty hard to do things um I think I shut down all the GPU clusters and get all of the famous scientists and Brilliant talented youngsters the vast vast majority of whom are not going to be productive and where government bureaucrats are not going to be able to tell who's actually being helpful or not but you know put them all on an island large island and try to figure out some system for filtering the stuff to through to me to give thumbs up or thumbs down on that is going to work better than scientific bureaucrats producing entire nonsense because you know the the trouble is the reason the reason why scientific Fields have to go through this long process to produce the cynical old stirs who know that everything is difficult it's not that the youngsters are stupid you know sometimes youngsters are fairly smart you know Marvin Minsky John McCarthy back in 1955. they weren't idiots uh you know privileged to have met both of them didn't strike me as idiots they were they're very old they still weren't idiots but uh but you know it's it's it's hard to see what's coming in advance of experimental evidence hitting you over the head with it and if you if I only have the decade of the 2020s to run all the researchers on this giant Island somewhere it's really not a lot of a lot of time mostly what you've got to do is invent some entirely new AI Paradigm that isn't the giant inscrutable matrices of floating Point numbers and gradient descent because I I'm not really seeing what you can do that's clever with that that doesn't kill you and that you know doesn't kill you and doesn't kill you the very first time you try to do something clever like that you know there's I'm sure there's a way to do it and if you got it to try over and over again you could find it Ellie is there do you think every intelligent civilization has to deal with this exact problem that Humanities is dealing with now is um how do we solve this problem of aligning with an advanced general intelligence I expect that's much easier for some alien species than others like there are there are there are alien species whose solution who might or who might arrive at this problem an entirely different way you know like maybe instead of having two entirely different information processing systems the DNA and the neurons they've only got one system they can trade memories around heritably by swapping blood sexually um maybe the way in which they confront this problem is that very early in their evolutionary history they have the equivalent of the like DNA that stores memories and like processes computes memories and they swap around a bunch of it and it adds up to something that reflects on itself and makes itself coherent and then you've got a super intelligence before they have invented computers and maybe that thing wasn't aligned but you know how do you even align it when you're in that kind of situation it'd be a very different angle on the problem but every year do you think every Advanced civilization does create a uh is on the true dietary to creating a super intelligence at some point in its history maybe there's ones in universes with alternate physics where you just can't do that their Universe their universes computational physics just doesn't support that much computation maybe they never get there maybe their lifespans are long enough and their star life pans short enough that they never got to the point of a technological civilization before their star does the equivalent of expanding or exploding or going out and their Planet ends every alien species covers a lot of territory especially if you talk about alien species and universes with physics different from this one well I talking about kind of our present Universe I'm curious if you've sort of been confronted with the question of like well then why haven't we seen some sort of super intelligence in our universe when we sort of look at look out at the stars sort of the the Fermi Paradox type of question do you have any explanation for that oh well supposing that they got killed by their own AIS doesn't help at all with that because then we'd see the AIS and do you think that's what happens and yeah it doesn't help with that wouldn't we would see evidence of AIS wouldn't we yes so so why don't we I mean the same reason we don't see evidence of the alien civilizations not with AIS and that reason is although it doesn't really have much to do with the whole AI thesis one way or another because they're too far away or so says Robin Hansen using a very clever argument about the apparent difficulty of hard steps in Humanity's evolutionary history to further induce the rough gap between the hard steps and you know I can't really do justice to this if you look up grabby aliens grabby aliens I remember that grabby aliens g-r-a-b-y you can you can find Robin Hansen's very clever argument for listeners there's an entire website called grabby aliens.com you can go look at yeah and that contains which is by far the best answer I've seen to where are they answer too far away for us to see even with even if they're traveling here at nearly light speed how far away are they and how do we know that but yeah this is amazing it I I there's not a very good way to simplify the argument uh you know any more than there is to explain you know simplify the notion of zero knowledge proofs it's not that difficult but it's just like very not easy to simplify but if you have a bunch of locks that are all of different difficulties such that at a limited time in which to solve all the locks such that anybody gets all through all the locks must have gotten through them by luck all the locks will take around the same amount of time to solve even if they're all a very different difficulties and that's the core of Robin Hansen's argument for how far away the aliens are and how do we know that pleaser I know you're very um skeptical that there will be a good outcome when we produce uh an artificial general intelligence and I said when not if because I believe that's your your thesis as well of course um but is there the possibility of a good outcome like I know you are working on AI alignment problems so which means me leads me to believe that you have like greater than zero amount of Hope for this project um is there the possibility of a good outcome what would that look like and how do we go about achieving it it looks like me being wrong I I basically don't see on model hopeful outcomes at this point we we have not done those things that it would take to earn a good outcome and this is not a case where you get a good outcome by accident it's it's you know like if you have a bunch of people putting together a a new operating system and they've heard about secure computer security but they're skeptical that it's really that hard the chance of them producing a secure operating system is effectively zero that's basically the situation I see ourselves in with respect to AI alignment I have to be wrong about something which I certainly am I have to be wrong about something in a way that makes the problem easier rather than harder for those people who don't think that alignment's going to be all that hard it you know there's you know if you're if you're building a rocket for the first time ever and you're wrong about something it's not surprising if you're wrong about something it's surprising if the thing that you're wrong about causes the rock to go twice as high on half the fuel you thought was required and be much easier to steer than you were afraid of so are you implying the alternative was the rocket if you're wrong about something the rocket blows out yeah and then and then the rocket ignites the atmosphere is the problem there or rather you know like a bunch of rockets blow a bunch of rockets go place if you you know the analogy I usually use for this is um very early on in the in the Manhattan Project they were worried about what if the nuclear weapons can ignite Fusion in the nitrogen in the atmosphere and they could they ran some calculations and came and decided that it was like incredibly unlikely for multiple angles so they went ahead and uh and were correct you know we're still here and I'm not going to say that it was luck because you know the calculations were actually pretty solid and AI is like that but instead of needing to refine plutonium you can make nuclear weapons out of a billion tons of laundry detergent you know the the the the stuff to make them is like fairly widespread it's not it's not a tightly controlled substance and and they spit out gold up until they get large enough and then they ignite the atmosphere and you can't calculate how large is large enough and a bunch of the people the CEOs running these projects or making fun of the idea that it'll ignite the atmosphere it's not a very clear situation so the economic incentive to produce this AI like one of the things why uh chat gbt has sparked the imaginations of so many many people is that everyone can imagine products like products are being imagined left and right about what you can do with something like chat gbt like the the there's like this meme at this point of people leaving and to go start their their chat GPT startup and so like the metaphors that like what you're saying is that there's this generally available resource spread all around the world which is Chad gbt uh and everyone's uh hammering it in order to make it to spit out gold but you're saying if we do that too much all of a sudden the the system will ignite the the whole entire uh Scotland well no you can run chatbt any number of times without igniting the atmosphere that's that's that's about what research labs at Google and Microsoft counting deepmind as part of Google and Counting open AI is part of Microsoft um that that's about what the the research labs are doing bringing more metaphorical plutonium together than ever before not about how many times you run the things that have been built and not destroyed the world yet you can do any amount of stuff with chat GPT and not destroy the world it's not that smart it doesn't get smarter every time you run it all right can I ask some uh you know questions that the 10 year old me wants to really ask about this um and uh I'm asking these questions because I think a lot of listeners might be thinking them too so you knock off some of these easy answers for me if we create some sort of unaligned let's call it bad AI why can't we just create a whole bunch of good AIS to go fight the bad ai's and like solve the problem that way can there not be some sort of counterbalance in terms of aligned human AIS and and evil AIS and there'd be sort of some Battle of the um Battle of the artificial Minds here nobody knows how to create any good AIS at all the problem isn't that we have like 20 good AIS and then somebody finally Builds an EVO AI the problem is that the first very powerful AI is evil nobody knows how to make it good and then it kills everybody before anybody can make it good so there is no known way to make a friendly human aligned AI whatsoever and you don't know of a good way to go about thinking through that problem and designing one neither does anyone else is what you're telling us I I have some idea of what I of what I would do if there were more time you know back in the day we had more time humanity squandered it I'm not sure there's enough time left now um I I have some idea of of what I would do if I were in a 25 year old body and had 10 billion dollars that would be the island scenario of like your God for 10 years and you get all the researchers on an island and and go really hammer for 10 years if I if if I have buy-in from a major government that can run actual security precautions and more like and and more than just 10 billion dollars then you know you could run a whole Manhattan project about it sure this is another question that the 10 year old in me wants to know is um so why is it that at least are people listening to this uh episode or people listening to the concerns or reading the concerns that you've you've written down and published um why can't everyone get on board it was building an AI and just all agree to be very very careful is that not a sustainable game theoretic position to have is this sort of like a coordination problem more of a social problem than anything else or like why can't that happen I mean we have so far not destroyed the world with nuclear weapons uh and we've had them you know since the 1940s um twice it's harder than nuclear weapons why is this a lot harder why is this harder and why can't we just coordinate to just all agree internationally that we're going to be very careful put restrictions on this put regulations on it do something like that heads of current heads of major Labs seem to me to be openly contemptuous of these issues that that's where we're starting from the politicians do not understand it there are distortions of these ideas that are going to sound more appealing to them than everybody suddenly Falls over dead just a thing that I think actually happens everybody falls over dead just as like doesn't Inspire the monkey political parts of our brain somehow because it's you know like it's not like it's not like oh no what if what if terrorists got the AI first it's like it doesn't matter who gets it first everybody falls over dead and yeah so you're you're describing a World coordinating on something that is relatively hard to coordinate it's it's not it's it's maybe it's so you know like could could we if we if we if we tried starting today you know like prevent anyone from getting a billion pounds of laundry detergent in one place worldwide control the manufacturing of laundry detergent only have it manufactured in particular places not concentrate lots of it together enforce it on every country you know if if it was legible if it was clear that a billion pounds of laundry detergent in one place would end the world if you could calculate that if all the scientists calculated it arrived at the same answer and told the politicians that maybe maybe Humanity would survive even though smaller amounts of laundry detergent spit out gold the settled can't be calculated I don't know how how you'd convince the politicians we definitely don't seem to have had much luck convincing those CEOs whose job depends on them not caring to care ing is easy to fake it's easy to to you know like hire a bunch of people to be your AI safety team and and and and and have the and redefine AI safety as having having the AI not say naughty words or you know I'm I'm speaking somewhat metaphorically here for reasons but you know it's it's it's like the the basic problem that we have is like trying to build secure OS before we run up against a really smart attacker and there's all kinds of like fake security it's called a password file this system is secure it only lets you in if you type a password and if you never go up against a really smart attacker if you never go far to distribution against a powerful optimization process looking for holes yeah maybe then how do you how does a bureaucracy know come come to know that what they're doing is is not the level of computer security that they need the way you're supposed to find this out the way that's the scientific Fields historically find this out the way that fields of computer science historically find the size the way that crypto found this out back in the early days is by having the disaster happen um and we're not even that good at learning from from relatively minor disasters you know like covid swept the world did the FDA or or the CDC learn anything about don't tell hospitals that they're not allowed to use their own tests to to detect the coming plague do they do are we installing UV C lights in public in in public spaces or in ventilation systems to prevent the next respiratory born pandemic respiratory pandemic it is you know we we lost a million people and we sure did not learn very much as far as I can tell for next time we could have an AI disaster that kills a hundred thousand people how do you even do that robotic cars crashing into each other have a bunch of robotic cars crashing into each other it's not going to look like that was the fault of artificial general intelligence because you're not going to put atis in charge of cars they're going to pass a bunch of regulations that's going to affect the entire AGI disaster or not at all what what is what is what is what is the what does the winning world even look like here how in real life did we get from where we are now to this worldwide ban including against North Korea and you know like that some one Rogue nation whose dictator doesn't believe in all this nonsense and just wants the gold that these AIS spit out how did we get there from here how do we get to the point where the United States and China signed a treaty whereby they would both use nuclear weapons against Russia if Russia built a GPU cluster that was too large how did how did we get there from here uniswap is the largest on-chain Marketplace for self-custody digital assets uniswap is of course a decentralized exchange but you know this because you've been listening to bankless but did you know that the uniswap web app has a shiny new Fiat on-ramp now you could go directly from Fiat in your bank to tokens in defy inside of uniswa not only that but polygon arbitrum and optimism layer twos are supported right out of the game but that's just D5 uniswap is also an nft aggregator letting you find more listings for the best prices across the nft world with uniswap you can sweep floors on multiple nfts and uniswap's Universal router will optimize your gas fees for you uniswap is making it as easy as possible to go from bank account to bankless assets across ethereum and we couldn't be more thankful for having them as a sponsor so go to app.uniswap.org today to buy sell or swap tokens and nfts arbitrim1 is pioneering the world of secure ethereum scalability and is continuing to accelerate the web 3 landscape hundreds of projects have already deployed on arbitrum 1 producing flourishing defy and nft ecosystems with a recent addition of arbitrum Nova gaming and social dapps like Reddit are also now calling Arboretum home both arbitrim1 and Nova leverage the security and decentralization of ethereum and provide a builder experience that's intuitive familiar and fully evm compatible on arbitrum both Builders and users will experience faster transaction speeds with significantly lower gas fees with the arboretum's recent migration to arborstone Nitro it's also now 10 times faster than before visit arbitrim.io where you can join the community dive into the developer docs Bridge your assets and start building your first app with arbitrum experience web 3 development the way it was meant to be secure fast cheap and friction free how many total airdrops have you gotten this last bull market had a ton of them did you get them all maybe you missed one so here's what you should do go to earnify and plug in your ethereum wallet and earnify will tell you if you have any unclaimed air drops that you can get and it also does poapps and mintable nfts any kind of money that your wallet can claim earnify will tell you about it and you should probably do it now because some airdrops expire and if you sign up for earnify they'll email you anytime one of your wallets has a new airdrop for it to make sure that you never lose an airdrop ever again you can also upgrade to earnify premium to unlock access to airdrops that are beyond the basics and are able to set reminders for more wallets and for just under 21 a month it probably pays for itself with just one airdrop so plug in your wallets at earnify and see what you get that's e-a-r-n-i dot f i and make sure you never lose another airdrop correct me if I'm wrong but this seems to be kind of just like a topic of despair like I haven't I uh talking to you now and then hearing your your thought process about like there is no known solution in there the trajectory Is Not Great like do you think all hope is lost here I'll keep on fighting until the end which I wouldn't do if I had literally zero hope I could still be wrong about something in a way that makes this problem somehow much easier than it currently looks I think that's how you go down fighting with dignity go down fighting it with dignity that's the that's the the stage you think we're at um I I I want to just double click on what you were just saying so part of the case that you're making is um Humanity won't even see this coming so it's not like a coordination problem like global warming where you know every couple of decades we see the world go up by a couple of degrees things get hotter and we start to see these effects over time the characteristics or the Advent of an AGI in your mind is going to happen incredibly quickly and in such a way that we won't even see the disaster until it's imminent until it's upon us I mean if you want some kind of like formal phrasing then I think that AI that super intelligence will kill everyone before non-super intelligent AIS have killed one million people I don't know if that's the phrasing you're looking for there I think that's a fairly precise definition and why what is the what goes into that line of thought I I think that I think that the current systems are actually very weak if you so if you I mean I don't know maybe I could use the analogy of go where you had systems that were finally competitive with the pros where Pro is like the the set of ranks and go and then a year later they were challenging the the world champion and winning and and and then and then another year they threw out all the complexities and the training from Human databases of go games and built a new system alphago zero that trained itself from scratch no looking at the human playbooks no special purpose code just a general purpose game player being specialized to go more or less and uh three days there's a there's a quote from guern about this which I forgot exactly but it was something like we know how long alphago zero or Alpha zero to different systems what was equivalent to to a human go player and it was like 30 minutes on the following floor of of uh this such and such deep mind building and maybe the first system isn't doesn't improve that quickly and they build another system that does and all of that with alphago over the course of years going from like it takes a long time to train to it trains very quickly and without looking at the human Playbook like that's not with an artificial intelligence system that improves itself or or even that sort of like get smarter as you run it the way that human beings not just as you evolve them but as you run them over the course of their own lifetimes improve so if the first system doesn't improve fast enough to kill everyone very quickly they will build one that's meant to spit out more gold than that and there could be weird things that happened before the end I did not see chat GPT coming I did not see stable diffusion coming I did not expect that we would have ai's smoking humans in rap battles before the end of the world it's kind of clearly much dumber than us kind of a nice send-off I guess in some ways I so you said that you have uh your hope is not zero and you are planning to fight to the end what does that look like for you I I know um you're working at m-i-r-i which is uh the Mean Machine intelligence Research Institute uh this is a non-profit that I believe that you've sort of set up to work on this AI alignment and safety sort of issues what are you doing there what are you spending your time on what do you think like how do we actually fight until the end if you do think that an end is coming how do we try to resist I'm actually on something of a sabbatical right now because we have oh I'm not saying it with sabbatical right now which is why I have time for podcasts um that's a sabbatical from you know like been doing this 20 years it became clear we were all going to die I felt kind of burned out taking some time to rest at the moment when I dive back into the pool um I don't know maybe I will go off to conjecture or anthropic or one of the smaller concerns like Redwood research Redwood research being the only ones I really trust at this point but they're they're tiny um and try to figure out if I can see anything clever to do with the Giant and screwable matrices of floating Point numbers um maybe I just write continue to try to explain in advance to people why this problem is hard instead of as easy and cheerful as the current people who think they're pessimists think it will be I might not be working all that hard compared to how I used to work I'm I'm older than I was my body is not the greatest of Health these days um going down fighting doesn't necessarily imply that I have the stamina to fight all that hard well it's uh I I wish I had prettier things to say to you here but but I do not no this is this is uh you know we intended to save probably the the last part of this this episode to talk about some crypto the metaverse and Ai and how this all intersects but um I gotta say at this point in the episode it all kind of feels pointless to go down that uh track record I we were going to ask questions like well um in crypto should we be worried about building sort of a property rights system an economic system a programmable money system for the AIS to sort of use against us later on but it sounds like the easy answer from you to those questions would be yeah absolutely and by the way none of that matters regardless uh you could do whatever you'd like with crypto this is going to be the inevitable outcome no matter what let me ask you what would you say to somebody listening who maybe has been sobered up by this conversation is a uh version of you in your 20s does have the stamina to continue this battle and to actually fight on behalf of humanity against this existential threat um where would you advise them to spend their time is this a technical issue is this a social issue is it a combination of both should they educate should they spend time in the lab like what should a person listening to this episode do with these types of Dire Straits I don't have really good answers it depends on what your talents are if you've if you've got the very deep version of the security mindset the part the part where you don't just put a you know like put a password on your system so that nobody can walk in and directly misuse it but the kind where you were the kind we don't just encrypt the password file even though nobody's supposed to have access to the password file in the first place and thus already an authorized user but the part where you hash the the passwords and salt the hashes you know if if you can think if if you're the kind of person who can think of that from scratch maybe take your hand in alignment if you can think of an alternative to the giant inscrutable matrices then you know don't don't tell the world about that um I'm not quite sure where you go from there but you know maybe you work with redwood research or something um a whole lot of this problem is that even if you do build an AI That's Limited in some way you know somebody else steals it copies it runs it themselves and takes the bounce off the for loops and the world ends so you know there's yeah but so there's there's that there's you can do you think you can do something clever with the Giant and scootable matrices you're probably wrong if you have the talent to try to figure out why you're wrong in advance of being hit over the head with it and on the way where you just like make random far-fetch stuff up is the reason why it won't work but where you can actually like keep looking for the reason why it won't work um we have people in crypto who are good at breaking things and they're the reason why anything is not on fire and some of them might go into breaking AI systems instead because that's where you learn anything you know it you know that any fool can build a crypto system that they think will work breaking existing crypto systems crypto cryptographical systems is how we learn who the real experts are so maybe the people finding weird stuff to do with AIS maybe those people will will come up with some truth about these systems that that makes them easier to align than I suspect there's out there the the outfits that are how do I put it the the the saner outfits do have uses for money they don't really have scalable uses for money but they do burn any money literally at all like if you gave Miri a billion dollars I would not know how to I I well I I might at a billion dollars I might like try to bribe people to move out of AI development that gets broadcast to the whole world and moved to the equivalent of an island somewhere not even to make any kind of critical Discovery but you know just to remove them from the system if I had a billion dollars um if I just have another 50 million dollars I'm not quite sure what to do with that but you know if you donate that's to to Miri then you at least have the assurance that we will not randomly spray money on looking like we're doing stuff and will reserve it as we are doing with the last giant crypto donation somebody gave us until we can figure out something to do with it that is actually helpful and Miri has that property probably I would say probably Redwood research has that property um yeah there's I I realize I'm sounding sort of disorganized here and that's because I don't really have a good organized answer to you know how in general somebody goes down fighting with dignity I know um a lot of people in crypto um they are not as in touch with artificial intelligence obviously as you are and the AI safety issues and the existential threat that you've presented in this episode they do care a lot and see um coordination problems throughout society as an issue many have also generated wealth from crypto and care very much about Humanity not ending um what sort of things has Miri that it that is the organization I was talking about Miri um earlier what sort of things have you done with funds that you've received from crypto donors and and elsewhere and what sort of things might an organization like that pursue to try to Stave this off I mean I think mostly we've pursued a lot of lines of research that haven't really panned out which is a respectable thing to do we did not know in advance that those lines of research would fail to pet out if you're doing research that you know will work you're probably not really doing any research you're just like doing a pretensive research that you can show off to a funding agency we try to be real we did things where we didn't know the answer in advance they didn't work but that was where the Hope lay I think um but you know that's but you know having a having a research organization that keeps it real that way that's not an easy thing to do and if you don't have this very deep form of the security mindset you'll end up producing fake research and doing more harm than good so I would not tell all the successful crypto people to uh cryptocurrency people to um run off and start their own research outfits Redwood research I'm not sure if they can scale using more money but you know you can give people more money and wait for them to figure out how to scale it later if the kind if they're the kind who won't just run off and spend it which is what Mary aspires to be um and you don't think the education path is um a useful path just educating the world um getting I mean I I would I would give myself a merry credit for for why the world isn't just walking blindly into the whirling razor Blaze here but it's not clear to me how far education scales apart from that you can you can get more people aware that we're walking directly into the whirling razor blades because even if only 10 of the people can get it that can still be a bunch of people but then what do they do I don't know maybe they'll be able to do something later can you get all the people can you get all the politicians can you get the people whose job incentives are against them admitting this to be a problem I have various friends who report like oh yes if you talk to researchers at open AI in private they're very worried and say that they like cannot be that worried in public um this is uh all a giant moloch trap is is sort of what you're telling us I um I feel like this is the part of the conversation where we've gotten to the end and the doctor has just um said that we have some sort of terminal illness and you know at the end of the conversation I think you know the patient Dave and I have to ask the question okay doc how long do we have um like seriously what what are we talking about here if you turn out to be correct are we talking about years are we talking about decades like what uh what's your idea here what are you preparing for yeah how the hell would I know Enrico Fermi was saying that nuclear that like fish and chain reactions were 50 years off if they could ever be done at all two years before he built the first nuclear pile the Wright brothers were saying heavier than air flight was 50 years off shortly before they built the first right flyer How would how on Earth would I know it could be three years it could be 15 years we could get that that AI winter I was hoping for and it could be 16 years I I'm not really seeing 50 without some kind of giant civilizational catastrophe and to be clear whatever civilization arises after that that you know would probably I'm guessing end up in stuck in just the same trap we are I uh think the other thing that the patient might do at the end of a conversation like this is um also consult with other doctors um I'm kind of curious if you know who we should talk to on this on this Quest um who are some people that if people in crypto want to hear more about this or learn more about this or even we ourselves as podcasters and Educators want to pursue this topic who are the other individuals in the AI alignment and safety space you might recommend for us to have a conversation with well the person who actually holds a coherent technical view who disagrees with me is named Paul Christiano he is he does not write Harry Potter fan fiction and I expect to have a harder time explaining himself in concrete terms but that is like the the main technical voice of opposition if you talk to other people in the effect of altruism or a alignment communities who um disagree with this view they are probably to some extent repeating back their misunderstandings of Paul christiano's views um you could try who's worked pretty directly with Paul Christiano and I think sometimes expires aspires to explain these things um that that Paul is not the best at explaining um I'll throw out Kelsey Piper as somebody who um would be good at explaining like would not claim to be a a like a technical person on these issues but would but is like good at explaining the part that she does know um man who else uh that disagrees with me you know I'm sure Robin Hansen would be happy to come up well I'm not I'm sure not sure he'd be happy to come on this podcast but you know Robin Hansen disagrees with me and I kind of feel like the the famous argument we had back in the to like early 2010s late 2000s about how this would all play out I basically feel like this was the yadkowski position this is the Hanson position then reality was over here like to the Wells of the atkowski side of the of the atkowski position and the Kowski Hanson debate but Robin Hansen does not feel that way and I would probably be happy to expound on that length um I don't know it's yeah it's not hard to find opposing viewpoints um the the ones that'll that'll stand up to uh a few solid minutes of cross-examination from somebody who knows which parts to cross-examine that's the hard part you know I've read a lot of your uh ratings and um listen to you on previous podcasts one was in 2018 um of the Sam Harris podcast this conversation feels to me like the most dire uh you've ever seemed on this topic and maybe that's not true maybe you've you've sort of always been this way but um it seems like the direction of your hope that we solve this issue has declined um yeah I'm wondering if you feel like that's the case and if you could sort of summarize your your take on all of this as we close out this episode and offer I guess any thoughts uh concluding thoughts here I mean uh so I don't know if you've got like a time limit on this episode question mark or is it just as long as it runs as long as it needs to be and I feel like this is a pretty important topic so you answer this and all right however you want well there was a conference one time on what are we going to do about looming risk of AI disaster and Elon Musk attended that conference and I was like maybe this is it maybe you know that maybe this is when the the powerful people notice and it's you know like one of the relatively more technical powerful people who could be noticing this and maybe this is where Humanity finally turns and starts you know not quite fighting back because there isn't an external enemy here but conducting itself with uh I don't know acting like it cares Maybe and what came out of that conference well it's open AI which was basically the fair nearly the worst possible way of doing anything a whole like this is not a problem of oh no what if secret Elites get AI it's that nobody knows how to build a thing if if we do have an alignment technique that's going to involve running the AI with a bunch of like careful bounds on it where you don't just like throw all the cognitive power you have at something you have limits on the for loops and whatever whatever it is that that could possibly save the world like turn all go out and turn all the gpus and the surfer clusters into Rubik's Cube or something else that prevents the world from ending when somebody else builds another AI a few weeks later you know anything that could do that as an artifact where somebody else could take it and take the bounce off the for loops and use it to destroy the world yeah so like let's open up everything let's accelerate everything it's it was it was like gpt3's version though gpd3 didn't exist back then it was like chat gpt's blind version of like throwing the ideals at a place where they were exactly the wrong ideals to solve the problem and the problem is that demon summoning is easy and Angel summoning is much harder open sourcing all the demon summoning circles is not the correct solution I'm not even using and I'm using Elon musk's own terminology here and they talk about AI is summoning the demon which you know not accurate but and then the solution was to put a demon and summoning Circle in every household and why because his friends were calling him luddites once he'd expressed any concern about AI at all so he picked a road that sounded like openness and said and like and like accelerating technology so his friends would stop calling him ludice it was very much the worst you know like maybe not the literal actual worst possible strategy but so very far pessimal and that was it that was like that that was me in 2015 going like oh so so this is what Humanity will elect to do we we will not rise above we will not have more grace not even here at the very end so that is you know that that is uh that is when I did my crying late at night and then pick myself up and fought and fought and fought until I had run out all the Avenues that that that I seem to have the capabilities to to do there's like more things but they require scaling my efforts in a way that I've never been able to make them scale and and there and all that's pretty far-fetched at this point anyways so you know that that so what's you know what's changed over the years well first of all I ran out some remaining Avenues of Hope and second things got to be such a disaster such a visible disaster the ai's got powerful enough and it became clear enough that you know we do not know how to align these things that I could actually say what I've been thinking for a while and not just have people go completely like what are you saying about all this you know now now the stuff that that was obvious back in 2015 is you know starting to become visible and distance to others and not just like completely invisible that's what changed over time what kind of um what do you hope people hear out of this episode and out of out of your comments the liaiser in in 2023 who is sort of running on the last fumes of of Hope um yeah what do you what do you want people to get out of this episode like what are you planning to do I I don't have concrete hopes here you know when everything is in Ruins you might as well speak the truth right maybe somebody hears it somebody figures out something I didn't think of I mostly expect that this does more harm than good in the modal Universe because a bunch of people are like oh I Have This brilliant clever idea which is you know like something that somebody that you know I was arguing against in 2003 or whatever but you know maybe maybe there maybe somebody out there with the proper level of pessimism hears and thinks of something I didn't think of I I suspect that if there's hope at all it comes from a technical solution because the difference between technical solution technical problems and political problems is at least the technical problems have Solutions in principle at least the technical problems are solvable we're not of course to solve this one but I don't really see the I think anybody who's hoping for a political solution has frankly not understood the technical problem they do not understand what it looks like to to try to solve the political problem to such agree that the world is not controlled by AI because they don't understand how easy it is to destroy the world with AI given that the clock keeps sticking forward they're thinking that they just have to stop some bad actor and that's why they think there's a political problem their political political solution but yeah I don't have concrete hopes I didn't come out in this episode out of any Concrete Hope I I have no takeaways except like don't make this thing worth don't don't like go off and accelerate AI more don't um if you have a brilliant solution to alignment don't be like oh yes I have solved the whole problem we just used the following clever trick don't you know don't make things worse than very much of a message especially when you're pointing people at the field at all but there aren't I have no winning strategy might as well go on this podcast and say what I think yeah as an experiment and say what I think and see what happens and probably no good government comes of it but you know there you might as well go down fighting right yeah if they if there's a world that survives maybe it's a world that survives because of a bright idea somebody had after listening listening to this podcast that was brighter to be clear than the usual run of bright ideas that don't work the laser um I want to thank you for for coming on um and uh and talking to us today I I do I don't know if by the way you've seen that movie that David was referencing earlier the movie don't look up but I sort of feel like that uh that news anchor who's talking to like the scientists is a Leonardo DiCaprio David and uh the scientist is talking about kind of dire straight to the world and um I the new snacker just really just doesn't know what to do I I'm almost at a loss for words uh at this point I um but what one thing I've had nothing for a while one thing I can say is um I appreciate your honesty um I appreciate that you've uh given this a lot of time and given this a lot of thought everyone anyone who has heard you speak or uh read anything you've written knows that um you care deeply about this issue and uh have given it a tremendous amount of your life force uh in trying to you know educate people about it and um thanks for taking the time to do that again today I'll uh I guess I'll just let the the audience digest this episode in the best way they know how um but um I want to reflect everybody in crypto and everybody listening to bankless uh their thanks for you coming on and explaining thanks for having me um we'll see what comes of it action items for you bankless Nation we always end with some action items not really sure where to refer folks to today but one thing I know we can refer folks to is Miri which is the machine research intelligence institution that Eliezer has uh been talking about through this episode that is at intelligence.org I believe uh and um I you know some people in crypto have donated uh funds to this in the past vitalik buterin is is one of them you can take a look at what they're doing as well that might be an action item for the end of this episode um gotta end with risks and disclaimers man this seems very trite but um our our legal experts have asked us to say these at the end of every episode crypto is risky you could lose everything but we're headed west this is the frontier it's not for everyone but we're glad you're with us on the bankless journey thanks a lot and and we are grateful for the crypto Community Support like it was possible to end with even less Grace than this wow you made a difference we appreciate you you really made a difference thank you thank you 