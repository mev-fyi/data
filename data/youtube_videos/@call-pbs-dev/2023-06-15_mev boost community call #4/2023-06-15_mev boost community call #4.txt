okay I should be ready to go with the recording hey everyone this is MAV boost Community call number four and yeah the agenda is quite packed uh I posted in the chat so I think we'll just go ahead and dive in um there's a lot today so briefly I wanted to touch on 4844 uh the main thing is that in terms of my Boost and the Builder spec there's an update here uh PR 61. you can see it linked from the agenda there and essentially it's just changing the types of the apis for the my Boost flow such that they can support blobs that's sort of the flagship feature 4844 is adding these data Bobs and there's a few different pieces we need to coordinate so that the current flow still works it also supports the construction of these blocks of blobs and that's what this PR knows uh otherwise it's pretty straightforward one thing just to call out is that you know now with these blocks with blobs uh relays may see this regime where you're passing back much more data for API call than you were and you know I don't think anyone's expecting any issues but definitely just an eye something to keep an eye on is uh you know especially as we move forwards test Nets and things if you see that like latencies for some of your API calls are spiking it might be because there's just a lot more data to send and we might need to think about optimizations there uh not expecting anything on priori but just worth calling out as we start to move that direction uh otherwise let's see Chris do you want to walk through some updates uh on the flashbots code I think there are some for my Boost yeah sure there is code on my Boost and this branch is linked there is some exploratory code on the relay and indigo boost utils and in the block Builder I don't I can't find the links right now but um yeah there's like work in progress I think within maybe two weeks we can be at a point where we can participate in a test net within two weeks is reasonable okay cool and I don't know if anyone else on the call with uh different real implementations or Builder implementations if you've been able to look at this stuff but please do so so moving on uh let's see here so yeah I think metacris you had some updates for us on relay performance and we've been doing a lot of work there and have some pretty exciting improvements uh did you want to walk through those yeah um just real quick in the past month or so we had a ton of performance improvements I think we plugged most of the low hanging fruits here they were mostly related to not simulating as many submissions as possible like one chunk of the improvements which is mostly outlined in the first link and then red is improvements on the other hand which is in the critical path in a bunch of points during block submissions and also during storage which put quite a strain on the Reds clusters too and the recent changes brought resource usage down to like 10 of previous and it's pretty good pretty fast I think we are validating non-optimistic block submissions uh the whole submission latency is about 200 milliseconds right now with kind of like stock ec2 instances and the stock code yeah I don't expect big jumps very soon anymore but I think this should reduce the operating costs and effort from other release by quite a bit too I think that's all I want to share maybe if there's any questions maybe a few more details about canceling uh and non-canceling bits that is um alluded to in the first link block validations are skipped for there's two types of block submissions one is non-cancelable the default submission and one is a cancer level submission where you add a specific URL query argument that this is a canceled a little bit cancel cancel level bits can lower the top bit so even yeah because non-cancelable bits you can all discard if they're below the current best bit of a block but the highest the highest non-cancelable bit provides a bit floor and even cancelable bits that are below the speed floor cannot lower the top speed value of the slot so they can also be discarded and not validated and I posted some um a table here I think we are skipping about two-thirds of all submissions now and and sometimes even more yeah that's it for that okay great yeah that's really exciting to see are there any uh you know so if I'm a relay and I'm running you know this this uh the relay code here that you're referring to are there any things I need to be aware of around deployments or upgrading or anything like that only the sinking thing that is outlined in the release details that if you upgrade from a previous version that you update both the block Builder API and the proposal API simultaneously if you run them as like separated API instances because they depend on a certain radius key that is used for delivering payloads from pillow submissions to the proposal and if in in this release it changed the key so they have both to be updated um otherwise it would still fall back to the secondary payload storage which can be memcache and which can be the database so it should not be like catastrophic um depending on your configuration otherwise I think it's pretty straightforward robot no no database migrations in the latest releases um yeah oh one more interesting thing about this is we've been doing a really big database migrations from like a 10 terabyte postgres instance down to like one terabyte and this was pretty much a hassle and we put together a quick migration guide if any of other relays need to downscale their database because in in the cloud managed databases like Amazon or Google you cannot downscale an up sized database anymore you need to create a new database and migrate all the data there and here in this guide there should be some some helpers if you want to embark on this feel free to also reach out if you're happy to support great thanks yeah I Justin has something to say I think we we also went through this very painful database migration um and so if you need any help Nicholas might have some tips as well okay great so yeah just be aware that there's maybe a caveat here um and then also yeah if you need to do any migrations sounds like there's some helpful tips uh in this PR so great uh I think we'll move along then uh the next thing I believe there's some updates from optimistic relay so uh Justin or Mike I think maybe one or both of you uh had some things to share yeah I just wanted to share a quick update on on the progress of optimistic relaying some of the good news so we have very good adoption of from the builders now we've had 21 builders make a deposit of one if and that's almost all the the winning bid flow um only one of the 21 Builders asked for for refund and we gave them the refund of one if because they were basically shutting down as a as a builder um so far we haven't had any um optimistic blocks um that were invalid uh and signed by the proposal so the proposers haven't needed any kind of of refund we haven't had issues there um and uh you know we're also making significant progress with optimistic reading V2 and Mike can talk about that uh I guess right after me um I guess what one of the things that I wanted to to give a heads up about is the possibility of you know increasing our collateral limit from one Eve to 10 if and there's a couple reasons here like one is that um some Builders have asked us for for higher um limits and the reason is that you know a lot of the Mev opportunities for demo when when there's these big Mev blocks and they don't benefit from optimistic relaying um and we you know we provide a a more consistent service I guess than the other relays when we're under one Eve and they want the same kind of service up to up to 10 if and I guess the other reason is that you know in in the spirit of moving towards and trying PBS um we we need to find some sort of amount which will disincentivize uh basically Builders putting in bad um bad blocks to to send to to basically to to cause one miss slot um so like the the current design of and try and PBS that we have there's a maximum amount of collateral for example 32 if and you know you're allowed to submit bids above the 32 if um but if you make an invalid block uh then that that leads to um to to miss slot and so basically we want the the cost you know to the blockchain for censoring one block to be to be under this this magic number and 32e but one if just seems way too small um and so tanif is kind of a way to move towards this this may be more appropriate number um and I guess one I guess a couple of things on the on the bad news side uh for optimistic reading one is that right now where um the only relay that's running optimistic reading uh despite the fact that the the code base has now been merged upstream and I guess the second piece of bad news is that it has been quite a bit more effort than we anticipated I think a lot of this effort you know onboarding and educating the builders um you know getting them to fix all their simulation errors um you know just going going through all the process of setting up the deposit address Etc it's kind of this this this one-time thing and we've done most of the heavy lifting but even after that there is some amount of Maintenance uh because you know for example every time a builder has a new key they will uh you know they might want that key to to be uh promoted to optimistic relaying and so like every every other day or so there's just stuff to be done to maintain optimistic relaying so that's it from the the sync I'm happy to answer any questions otherwise I guess Mike could talk about optimistic relaying V2 foreign I guess I'd be curious to hear if you thought about uh so one thing I was kind of thinking about is uh moving validator registrations on chain in some way and if there's timing into that but I wonder if there's a way with optimistic relaying where you could do something similar where it's you know more things are automated so Builders can kind of like you know deposit withdrawal and do all this kind of on their own and so then in terms of operators you just have to like you know sync the chain so to speak um maybe it would reduce some of the Ops overhead there yeah that's interesting like maybe we could have um some sort of sharp database where the builders can say I want this to be optimistic I mean another use case for example is uh the build is going through some sort of overhaul of the infrastructure and they want to temporarily disable optimistic relaying um and so they could just do that with this kind of this shared shared Channel I guess um and then we could automatically um demote them but I guess there's also other things that we've noticed like one is that um you know every time there's a timeout so if there's a huge Spike of activity and there's a there's timeouts then we take the very conservative approach which was we did we demote blocks that timed out and that we were not able to to simulate um and also there's this Edge case when there's reorgs and basically there's this um this does this an ability to basically simulate the block because the the the the parent um block kind of changed under you um and you know the the consensus client moved on and so in both of these edge cases we take the conservative approach and you know we had to build kind of custom infrastructure to uh to to make the maintenance button uh lower but yeah this if someone wants to do optimistic reading we can kind of give you a heads up as to all the the maintenance departments that there are I think we've reduced it to a good amount but I just want to give you a heads up that it's still definitely there if the optimistic relay demotes due to a Timeout on the relay side you know like the Sim is overloaded whatever it is and um how is the Builder uh sort of promoted again that's sort of automated or does that require some kind of intervention is there any kind of uh signal to the that the Builder can get that that occurred right um so the way we used to do it is manually so we we have this telegram bot which tells us that there's been a timeout and then some minutes or hours later you know we we see the notification and we manually review it and the review process basically involves um looking at whether or not the there was a Miss slot in in that case and if there was no missed lot then we kind of assumed that everything everything is fine and we repromote uh what we ended up doing um is basically automating this manual process so um like a few minutes after there's a demotion specifically due to a timeout um we look back you know uh whether or not there was a Miss lot and if there was no mislots we automatically repromote um so we wrote this custom piece of logic which I guess sits outside of uh of the go code base um and that's something that we'd be more than happy to I guess uh open source if it's probably already open sourced uh if you're interested yeah that would be interesting um if you're only looking at Miss slots what about the case where uh uh because they're a a risk that a builder this represents the bid so the slot would not be missed but the validator would have been um you know received a different value than the bid which which the the Sim would normally have caught right that is an excellent point um what we should be doing is basically um looking at the Block that one and if it was one of our blocks then verifying the payment and that that is that is a great Point thank you so much I will say okay go ahead Mike I was just gonna say a lot of the timeouts come from like some of the builders that have less winning blocks but very often like send a ton of blocks so usually the if there's a demotion that Builder didn't end up winning the slot so one of the you know the if if the winning block did come through our relay we probably did get simulated that's the point I'm trying to make but yeah go ahead Stokes I was going to say just like a remark I'm like very surprised I don't think anyone's really misrepresented the value of this um we like spent a lot of time thinking about payment proofs and we could still introduce them especially if it becomes a problem but yeah it's uh that doesn't really seem to be a place where people are uh trying to agree with the whole system so but that being said it's definitely worth keeping an eye on as an operator I mean on the topic of like these Builders which internally we call spammy Builders um our definition of spammy Builder is you know if you have less than one inclusion for every 100 000 submissions then you know we qualify you as spammy and I guess there's there's like three or four of them um and like one of the things that we've noticed is that you know they put a quite a bit of burden on our infrastructure and that has like real you know devops cost in terms of cloud bills and they they provide some of the you know the least value to the ecosystem I guess and not only that yeah one in a hundred thousand Chris um not only that but um they actually you know consume simulation resources so they they basically if we were to ban all those spammers uh then we'd we'd have a greater win rate um so I I'm not sure what kind of discussion we need to have here but uh like we you know we want to be credibly neutral we don't want to be like censoring anybody but like there are some Builders out there that are providing extremely little value and consuming a ton of resources is this not handled via rate limits I know it's some relays like ours does have rate limits to mitigate this so we we've taken the decision to not have rate limits we could have rate limits uh that that is that is correct um I mean this above and beyond providing the relay service one of the things that we kind of are are doing right now kind of as a public good is basically keeping a full Archive of every single submission that we receive um and you know the idea here is to provide uh um a huge database for researchers down the line maybe a few years down the line to to to do whatever research they they need um and so we we are actually Keen to keep this information and not rate limit too much and another thing with rate limiting so we consider we initially we had the cloudflare um but one of the problems of cloudflare is that it adds 10 milliseconds of latency and so um kind of Shaving that off uh kind of makes you more competitive and I guess as as Max said like the rate limiting is kind of easy to get around if you really want to get around it uh and anyway um I mean one of the things we are considering is doing um rate limiting based on on deposits so if a builder has collateral then that's a form of anti-civil infrastructure that we can reuse for for rate limiting yeah this is kind of thinking out loud here but we could say you get like x amount of optimistic blocks per one eth collateral or something per slot we haven't thought too much about it but seems potentially useful I have one the same related question how often do you get um block submissions that result in an emotion or have a problem so there's kind of two types of blocks that lead to the emotion like there's the false positives where there's been either you know a spike of activity which leads to timeouts or some sort of reorg that happens I know maybe twice a day and they'll or maybe even more now um a handful of times a day but that that's okay because of the automatic repromotion and then there's like the most serious uh problems where the block was actually invalid um that used to happen uh quite a bit when we got started and the reason is that Builders has all had all sorts of bugs which they were not even aware of because they didn't get this feedback loop but you know optimistic reading it was a forcing function for us to report every single Builder bug and I'd say nowadays we get and then once one a week one one every couple weeks it's it's relatively rare now thanks yeah a lot of Education effort okay uh I think Mike did you have some updates on optimistic relay and V2 or yeah I feel like we've covered them okay no I I'd like to give a couple minutes of of chat um can you guys see my screen yep cool so yeah just to kind of remind everyone what optimistic relaying V2 actually looks like I thought it'd be useful to kind of revisit this optimistic relay roadmap um so this is kind of the schematic we use to describe optimistic V1 which is Builder submits the bid we have this like block thing here to represent that the whole payload is like decoded by the relay before the bid is marked as eligible so when we're looking at kind of the the performance of submissions we see that for example um the decode duration for for different Builders can be like 10 milliseconds for some Builders it can be like 100 milliseconds and that's kind of this first part of the of the submission that that is kind of the latency bottleneck once we remove the simulation from the submission pipeline so optimistic relaying V2 is is something we call header only parsing and the idea here is that the Builder sends the the bid with a new message type and the the message includes the full execution payload header along with the list of transactions in which are all separately so this is slightly different than the current submission process but what it allows us to do is we can mark the bid as eligible and we can say the get header response is like constructed before we actually receive the full contents of the of the block so you know the block might be a few hundred kilobytes up to like a few megabytes and so that the download speed of that is no longer part of the hot path for a for a builder submission so that's kind of the goal of optimistic V2 at a high level we have some um let's see yeah I put a link to the um to the pr in um let's see I can't find the chat oh here it is okay yeah here's a link to the pr I opened up this against the flashbots repo just kind of for visibility um it's it's sort of work in progress still but we've done a little testing that I thought it would be interesting to to share so I guess the the tldr is we're working with rsync to kind of demo this um the decode durations for for V2 with SSD and header only parsing are like really fast so this is the decode duration in microseconds and we're getting around like 40 microsecond median there's a little bit of a longer tail because just I think go garbage collection go runtime stuff but yeah I guess the the tldr is this is about two orders of magnitude faster than decoding the full payload so the average for of a 100 or sorry 10 000 microseconds is for the full V1 uh decode versus the V2 decode is closer to like 50. uh microseconds so that's pretty encouraging the other interesting number I would say is what I'm calling the time to save payload so the important thing about V2 is there's now kind of a new state for the bid and that is the bid could be eligible to win the auction but it could not be available so you know a proposer can call get header they'll get the header because that's that's available but when they call get payload the relay might not have have downloaded the whole block yet from the Builder so this this box and whisker plot shows the difference between when the payload is received and when the bid is marked as eligible so like generally speaking it's it's quite low but there's a long tail up to around like 800 um milliseconds this is 800 000 microseconds so that's like almost a full second at the long tail so this means that the bids could be in a state where they could win the auction but the block isn't available yet for a full second and you know that's potentially problematic especially if someone calls get heather near the end of a slot because like let's say they get they call get header at like T equals 2.5 or something um then they might call get payload but we have to wait a full second for the payload to be downloaded suddenly we're like really butting up against the end of the app station deadline so these are kind of like some of the concerns around V2 um the this right hand one is the received at so this um receive that is is not when the bit is marked eligible but it's when the first packet is received from the Builder so this is like a little bit longer up to like 1.5 seconds um but really the the left hand plot is is the critical data point and again this is this is from a a builder that's like pretty well co-located with ultrasound relay it's it's in Europe it's you know I don't think it's in the same data center but still the fact that it the the long tail is that long is pretty concerning so just a few quick takeaways the the single packet decode durations are really fast um the way that SSD encoding works we can be guaranteed essentially that the full header fits in one packet so it's like 900 bytes the MTU of an Ethernet pack is 1500 so they send us one packet and we can mark the bid as eligible um but as I mentioned before there's like this new status where a bid is eligible to win the auction but isn't available that's kind of the this gray area that we need to be careful with I would say um the current retry Logic for get payload is we retry once after 100 milliseconds so when the validator calls when the proposal calls get payload we try to read it from redis we've tried to read it from the database if if both of those fail then we sleep for 100 milliseconds and then we retry but as we've seen like they could call get payload before the block is downloaded so we might need to retry like you know up to one second or retry like five times or something that's part of the logic that we need to to demo and and kind of harden a little bit um the fourth Point here is that the time to save payloads is the real critical metric um because it's it's eligible not available kind of reiterating the same point um our test Builder that we've been running this with we've seen that they can get demoted just from the time out so what this means is um there's kind of a new demotable event where if the full payload isn't received in time or like whatever there's some networking issue blah blah blah that is actually demotable because now we can't publish the get payload response um at all because we don't have it um and I think this kind of leads nicely into my 0.6 which is that the V2 failure mode is actually like way worse than the V1 failure mode so this was coming up and we were talking about different demotions that are happening in V1 and like most of the time a demotion in V1 isn't actually that big of a deal because very likely if if the demotion was just from like a timeout or something the even if the block ended up winning the auction it was probably like valid um however the V2 failure mode is like we didn't actually receive the block so it's like guaranteed we're gonna miss the slot if if someone signs that header so I think we're approaching this with a lot more caution because the situation of of like even if even if the Builder wasn't intentionally misbehaving if there's a network issue that'll like guarantee a missed slot which is not ideal um a few ideas that this kind of brought to mind for me was like maybe we should just go straight to Builder sign publishing this is like maybe a little more controversial because it takes away some of the power from the relay um and it it makes it so that the relay wouldn't have access to the to the contents of the block ever but the idea here would be that the builders just send the the headers to the relay the relay still executes the auction but it doesn't download the payload and then the winning bid um you know the relay sends a request to the Builder to tell the Builder to publish the block and the Builder's responsible for getting it published in time the relays would still hold collateral and say like hey the we told the Builder to publish the block on time they didn't get it published on time so they either have to refund the proposal or we slash their collateral but that's kind of the idea um I think in some ways this is a little less brittle because we don't have this like intermediate state of bids but in other ways it's it's kind of changing the data model more because the relay won't have the full payload and um the relay won't be able to return the payload to the to the validator when they call get payload so another idea to kind of Harden this would be we could have an early cutoff for v2 submissions so we could say like V2 submissions have to arrive before T equals two into the slot because we're worried about this um about this Delta between when the bid is available and when it's eligible um I'm not sure this is the right approach either because then it just kind of moves the latency games slightly earlier but it could potentially cover cover for the networking latency issues and then the third thing I'm just kind of throwing out there is you know we could have some stricter qualification for v2 optimistic relay you know for V1 we basically said like anyone who sends us one eth will will let them do it and you know that's been great like we have a ton of Builders running optimistic but we could have like some stricter performance requirements like we monitor the average time to to download the block from Builders and we say that you know your your P99 or your p999 has to be within one second for 24 hours in order for us to like activate you for v2 so these are just kind of some ideas to make V2 safer we haven't activated it in prod yet we're just these are all just tests where we're kind of like simulating it um but all the code is written it's in that PR and yeah I think that's kind of what I wanted to say happy to continue discussion here and and offline too yeah Chris oh thank you there's also really interesting all of you and very nuanced summary of the different perspectives I my understanding was that the performance improvements by header on the parsing is in itself only like on the order of microseconds so like 0.15 milliseconds how does that impact the the Mist slot chance that much twice the overall latency uh lower so the decode time without header only parsing ranges from like 10 milliseconds up to like 100 to 200 milliseconds um and if we do header only parsing then we cut that down to like around 50 microseconds so that's like the the two order two orders of magnitude in the in the lower bound oh okay yeah I see so yeah if you look at the actual like total durations like you were talking earlier about how the total duration might be like 200 milliseconds now for it for a good block um if 100 milliseconds of that are the the simulation then probably the other 100 milliseconds are are from the decode like if mostly red is otherwise really what do you what do you see what the decode is is it usually like 10 milliseconds or what is it on your end I can look at the data I don't have a link right now maybe I can get something up because we do lock the decode time so it should be pretty easy to find it out in a few minutes yeah I mean as the I guess as the redis stuff gets optimized more though like basically the bottleneck becomes the decode at least that's what that's what we saw in in our data so the it does shave off like another important set of milliseconds but okay I asked them but like in a more optimistic case let's say decoding takes only like 10 milliseconds on average so you're kind of save 10 milliseconds of the Overflow is it worth the added complexity on on both the code and the wider implications with higher uh with slots chances and like are these 10 milliseconds worth this type of change I mean Mike would it be fancy said it's not about the the decoding it's about the the downloading of the block right the downloading itself can take hundreds of milliseconds yeah well when we talk about decode time in the context of V1 we mean download and then like um parse basically so I think we're we're talking about the same thing yeah I see it's just the long tail is long basically that's the problem like on average we might only save 10 milliseconds but for some blocks we might save like 100 to 200 milliseconds and those are the ones that like would really benefit from V2 parsing but those are the ones that are also I guess higher risk for for Miss slot candidates if that makes sense but the reason why the average was so low is because async is located with us right but many Builders are not located and so they'll have a much larger decode time though yeah yeah yeah I think 100 milliseconds is is what I haven't looked at the aggregate data in a while because we've mostly been focused on rsync and they're pretty well collocated but yeah I think 100 milliseconds is what we saw generally for this also creates another incentive for collocation right which is like one layer of centralization well it actually it kind of decreases the need for co-location ready because if you can if you just have to get one packet there then in some ways like the download speed is no longer the bottleneck like the bottleneck the risk for Builders decreases when they are collocated much more in V2 submissions than in V1 because in V1 either they make it or they don't but yeah the two there's this large cap in between so yeah the risk is the risk is publication is yeah if Force towards collocation here I do think there is I guess the question is is there already a strong collocation force I mean the other thing is that this is where we're heading towards with and shine PBS right there will be this bit pool where only the headers and then if there's some sort of Internet issue between when the proposal signs a header and when the Builder is meant to release the payload then it's tough luck on the Builder yeah in terms of code complexity I think that's one of my points that I was making is like maybe we should just go straight to build their Publications I don't know did you have a thought on that Chris or Justin I just took a quick look at the code it's it's only complex it's not like totally overwhelming but it adds like two big chunks of duplicated functionality that can be like refactored over time but it's non-trivial I would say and it needs to be highly maintained by optimistic um three layers because flashbots cannot does not run it's called Branch then um so this leads to further operational complexity and and over ahead Maybe by the way Chris had his uh Chris metal has his hand up uh very long so maybe yeah how's the mic hey Chris um yeah I was wondering Mike if you had in your analysis if you had correlated those uh for a long tail latency to the block size because obviously the larger the block then the longer it's going to take to to get the rest of it and I'm wondering if if if this sort of plan creates a an interesting new uh dependency with with the block size yeah so it definitely is correlated to the block size for example the first iteration of V2 we we weren't using the SSD encoded bids and then the headers might have been split like they could be split over two packets if it's if it's unlucky and that like took away basically all the benefit from from the header only parsing so it definitely is correlated to size um and this kind of does also speak to Jacob's chat which is that the the decode will be more meaningful with 4844 and like the the relationship there feels very important but if if that means that the decode time is longer than yeah I guess potentially the the time between when a bid is eligible and when it's available might even be longer um I haven't done like I don't have the data plotted of just like decode time against uh payload size I can try to figure out if I have that in the log somewhere but I think it's worth studying for sure yeah I was thinking about that in terms of you know uh risk to the Builder uh because you know the a small a small block is lower risk to the Builder and a large block as higher risk and you know that puts the Builder in a in a situation where uh they might have a strong internet connection to the relay they may be co-located but if the block gets big enough especially with some of the future changes to blocks then um you know is is that going to create more complexity for the Builder to sort of make a decision on on the Block that they deliver because of the increased risk of of a Timeout on the uh on the full Block versus the header yeah sticking ahead on that I I think the like HTTP request has a Content length thing so maybe we could have like a a check for v2 that says if the if the block is really big then don't do it but yeah these edge cases are hard to hard to necessarily like code around I would say yeah I just had this like I imagine this future world where uh where builders start to optimize on block size due to this latency more than they do on you know like packing the block you know appropriately based on gas uh because you know winning a smaller block might be better because you actually win it versus delivering a bigger block that has more value but you can't win it because of the latency that it induces so it kind of distorts the market a bit maybe yeah responding to your second comment there Jacob we initially were talking about censorship resistance as as some kind of nice benefit of this but it kind of can immediately be defeated by if a relay just um still wants to censor with optimistic they can just say like a builder another demotable Builder event is publishing a block that touches like an ofac band transa transaction and then like slash the Builder for that too so even though the relay would end up publishing one of fact block they would never publish another one because they they would like not let that Builder submit anymore so the the censorship resistance capabilities of optimistic relaying aren't like the guarantees don't feel that strong to me gotcha that makes sense reading Lucas's message here um yeah I think co-locations generally regarded as not great but seems kind of inevitable especially with relays yeah but like should we actually do that on the cost of entire network Integrity like I would prefer to have Network integrity and like you know uh ethereum working regardless of uh like natural disasters Wars and etc etc uh and not have any at the same time like you know this this kind of Trump's the idea that you know this should be like you know that this this impact of that should be like a bit more like anything is not something that I would trade for Network Integrity personally well I guess I don't see how optimistic V2 necessarily changes this significantly because like the most that could happen is 20 missed slots one from each Builder you know it it just doesn't seem like V2 moves the needle on that compared to just having relays in general I would say um so yeah I mean that was my general comments to everything that we're talking about here because I've heard that Mev and yes it drives collocations etc etc and it's it's actually true but at the same time I think that you know the network Integrity is something that you should also think about especially if uh suddenly everyone decides to run their notes in ews and ews will actually own ethereum yeah I guess one nice thing is there's like this relay circuit breaker thing so the clients implement you know that they stop using the relays if there's a lot of Miss slots I don't remember the exact Criterion but yeah I think there is some protection against it in general but overall latency games definitely are centralizing okay so sounds like there's still some open questions do you feel like you have enough Direction mic to move ahead with uh what you want to do here yeah I mean the the point of this presentation was kind of just get everyone up to speed um and kind of solicit opinions generally um the code is is all open source and like I'll probably talk with Chris a little bit about it over the coming days and um yeah if anyone has anything to add definitely jump in otherwise we'll probably keep running tests and and eventually turn it on okay what a record so I am I just think that I'm still I see the appeal or to shaver of 10 to 15 more milliseconds I am not convinced that at the current time it's it's worth the trade-off of the additional complexity everywhere um I just want to record that I'm open to for discussions but um just saying yeah I think I think that's fair I mean you did you did mention that there is this opportunity to to refactor some of the uh the code because it's mostly just duplicated code um with a few of the checks the pre-checks that I removed as I understand and so maybe there is a a PR that could be done which significantly reduces the the maintenance burden that could be yeah sure one part of the plan um I think for me it's most clarity about the um incentives this creates whether there is actually um this is acceptable for additional incentives to collocate um maybe it is I just want to think a little bit more before making up my mind on this yeah that's fair I think we'll do we'll move slowly we'll be conservative we'll do lots of tests but I think my intuition is it actually removes the incentives for for collocation because uh you know as Mike said you don't need a fat pipe between the Builder and the relay and the reason is that the only thing you need to communicate is a thousand bytes whereas if you need to be communicating very large blocks especially after 4844 then you know you want to want a 10 gigabit connection co-located anything other than that is going to be non-competitive but I think it's on the contrary because um in V2 if you take too long to deliver the payload you have a massively increased risk of missed slots versus in V1 you have a general incentive to get blocks there fast but there is not this additional risk which is like very real like a straight demotion and losing your collateral if you go too slow right like this is um a much more stronger Force towards collocating I think at least intuitively right you're right there's a trade-off there's these two conflicting forces and we need to study it forever okay so yeah um looking forward to further research here and please keep everyone updated as that evolves uh there's a sort of medley of more open-ended or more miscellaneous questions were there any you know closing thoughts on obvious degree lane or anything we've talked about so far otherwise we'll kind of move into uh the next portion of the call I have one closing call um I totally forgot to shout out blocks route and also Ben from blocks Road particularly for um working more in the open and giving a couple of ideas for yeah just cross-pollinating Improvement ideas for the Relay it's really great also seeing you guys work more in the open now great yeah definitely it's good all good to see uh collaboration with the relays okay so we'll move on to the next section so there is a few here that Chris brought up um and yeah maybe we'll just kind of go through them they're all kind of unrelated so we'll just kind of go and turn the first one was considering this change in that boost to essentially have the get payload call go to all relays rather than just the relays that serve the winning bid and the reason we wanted this is I think we saw some issues uh what's in the slots were basically you know for whatever reason if the relay can't uh those sort of winning relay can't return the payload in time then the members client gets kind of upset and you know ultimately leads to in a slot so the suggestion is instead to say okay uh maybe other relays that maybe didn't win the auction but could still have that block why not give them a chance for data availability as well and so that's what the suggestion is um has anyone had time to consider this do they feel strongly we should do it or should not do it sorry how do we identify a win attribution to the relay if uh if it might be delivered by a different relay than one I think it would be just the same as it is now that multiple relays can be the winner because like from their point of view they are already because they delivered a block by The Proposal call off right like for instance really scan IO there are the the number of um included blocks is much higher than the total number of blocks in a day because they just it just overcomes because you cannot attribute the sinking slot distinctly relay it can always have multiple winners so I don't think this would actually change that would it when would you record the get payload then into the data API I mean currently all of them do every every related um delivers a payload which currently already is usually true a free release deck has high overlap between flashbars agnostic blocks route Ultra sounds like usually it's free relays very often that have this in the data API and that um belief or a good reason that they deliver the payload which I think is fair I think that doesn't need to be a single attribute that will relay yeah but like if you get the header from let's say relay a and then suddenly you request only the relay B that also had that dog uh how come that how would you know that the relay a actually said that as I understand the win rates are actually by the get payloads and not necessarily by the uh you know the block submissions uh I don't quite understand what do you mean I pay for explanation math boost currently concurrently tries at all the relays that serves the speed and the first really that response um will be and the others will be canceled and the block sent to the proposal the sealed client but still all really receive the rig Fest and prepare the response and probably send the response already on the way so they like why would you need to attribute a single reading as a winner hey yeah I'm I'm just asking because maybe uh maybe there is a lack of my understanding here so relay scan operates on which part of the uh the data API the submissions and block submitted or the the payload returned if they return the get payload then they had to have the block submission that's the point like if they responded with the full payload that means the Builder sent it to them too yes but like right now we're talking about the slightly different scenario anyway okay headed from relay a and ask only really big let's say right maybe I'm not not no let's say you have really ANP and really serves speed with one Eve and really B's are speed with two if it will currently my Boost all these hands get payloads to really be and the proposal is to send it also to really a and really be because in the meantime really it could also have received the payload and could have it now even if it didn't served a bit so the proposal is just sent to all of them yeah but Chris this type of significant impact on the statistics um because imagine the Builder is connected to every relay then more likely than not every relay is going to think like that they they want whereas currently it's about winning the the get header race whereas with with this change it's about just winning the submission race like who's yes I think there is like the question is what is the notion of winning because right now only the really end the proposal knows that they like when get header was actually on the proposal of this um like tools like really scan or the data API they do not expose uh who was actually the winning get header response right rightly get header calls are not signed so they really doesn't even know which get header call was from The Proposal so the game is won with get header but uh release only know if get payload was called for them really that that is what they can expose in the data API foreign setup where you know we assume the the relays are being honest and Reporting you know the the data then the the get the signed get payload requests are proxy for when you want the get header race yeah possibly but what's the point why why do you want to attribute a winner like what's the use of that I don't know I mean I guess it's a vanity metric to an extent um but right so I think the suggestion was to like help availability and you know by extension security so this seems more like can we make boots more secure and then if there's something with like the apis we can adjust we can do so I don't really see that being a fundamental blocker here I mean today as a relay you could just uh spoof that you delivered a payload if you knew that you had the block hash you saw the bill it was delivered you could you could track that that was a delivery payload already even if you didn't have the block cash I mean you could just see that another relay has a delivery payload and just say you did it too oops would this be an incentive for validators to connect to more relays because it would increase redundancy and reduce Miss slots in the scenario I think there's two types of connections that the proposals can have like they can I think it's not beneficial for them to connect to every single relay for get payload but forget header that's a different story they should only connect to those that they Trust so there could be an advanced version of math boost where you have two two lists of relays one for get header that are trusted and one forget payload which are not trusted it seems a bit of have your cake you needed to just food for who's from whose perspective look we're not we don't like block native for whatever reason like screw you guys oh but we're in a bind hey block native come save us um yeah I don't really follow as in I don't think that's the right way to look at it because like in that case it's like relays are just helping this security of the network it's not like I don't know I mean like really if you're worried about that as a Relay operator you could also just ignore these requests right yeah again we we want to make sure that the ecosystem as resilient as possible but it creates creates a very weird incentives and very weird Optics about who's who's perceived to be important and who's actually doing the work so yeah I would also prefer to keep it simple and propose it just choose whatever real estate they want and not have like a two-tier system that will more easily also lead to like some some lists and some operators like I don't know I'd rather keep it simple I think okay so then the proposal would be there's the one list of configuration of relays that you start my Boost with you would call it header or all of them and then regardless of who can serve you the winning bid you call get payload for all of them that's what I'm hearing one thing from the relay side this would introduce a lot of like error logs because basically every you know the relay will very often just not have the payload and then it'll like those those already exist to some extent but it'll increase the number of those I don't think it's a deal breaker but worth mentioning yeah that's a good point and it's also something we should look at the code paste that if the check is that we have we do not have a bit for it then we should just not lock this as an error and if you did have a bit then it should have been fine anyway I think that should be like relatively easy to mitigate the excessive login here yeah we've actually seen that someone started doing that yesterday already doing what sorry exactly what we're just talking about like we had those we started receiving those erosion suggestions yesterday about like beats that we don't have so I think someone they just let the agenda of that meeting can trade that ourselves yeah we've also seen uh some validators requesting payloads that we don't have but we do see that they get delivered from another relay anyways um so anyone any of these validators could be running a modified mvp-based client but I think this is just more bringing it into the vanilla code base okay so yeah I mean it sounds like we can think about this change there might be implications for the login and just how we think about errors on the relays um and then also we should investigate the data apis in case there's something there and there's some Downstream effect we don't want um I can move all this forward seems pretty straightforward so I'll do that and perhaps I'll have another call in a couple weeks and circle back to this I would imagine before anything there'll be like a PR to my Boost that people can agree to right before this is uh just sort of upstreamed on everyone yeah I think if relays are concerned about this too you can also track which headers you've returned and if you didn't return a header for a requested payload you don't have to uh you want to publish the payload foreign interest of time I think we'll move on uh there's still quite a number of things here maybe we can move them kind of quickly uh Chris another one that you brought up was this proposer payment methods allowing coinbase payments so if I leave go ahead I will try to keep it real quick it's about different relays to implement different payment methods Sean really allow payments to the proposal by setting the proposal fee recipient to block coinbase and some have a payment protection at the end of the block the flashbots really requires a payment transaction at the end of the block um but ultrasound enabled credit payments and some other release Market way too um I think we are starting to consider opening up the coinbase payments as well in particular for most blogs this uh the gas fee for the last transaction can be as much as five percent of the block value that could be captured by the proposer through a coinbase payment and payment transactions are only really useful if you want to take a profit otherwise there is no reason to have a payment transaction um yeah this is um don't know if any of the other release Here have any opinions about their payment methods but I think the flashbots really will start just allowing both both ways of payments I think that'd be good because we'd get you know standardization I guess another advantage of allowing payments to the coinbase is that in in the case of a reorg it protects the the the the the payment so that there are instances where the block was made public the last transaction was made public and then the transaction still goes through because it's unprotected because it's a simple payment yes absolutely we've seen the tool like this just happens this is not good to so on the uh on the cognitive side we we actually started that way just a little history when we you know the first few months that we operated we were only doing uh public blocks using our our mental infrastructure and so we did everything with coinbase and uh and then when we started running private blocks you know with no with Mev um there was we had we had a pretty big support burden with um with validator pools trying to understand whether they were getting paid because they weren't checking coinbase uh changes they were only looking at the payment transaction so I think if we were the more that this becomes sort of standard um that there may be some education or tooling required to make it clear to validators that they are actually getting the value um it's just not as obvious to uh to sort of piece that together in either scan or whatever have you guys seen that like issues with sticking pools with Corpus payments did get any reports about problems we haven't had reports or problems but uh we are aware of the possibility of a problem uh so we do coinbase payments as well at blocks route um we do get people who are confused where they say they see the bid is one eighth right but they uh only saw that they produced a block for say 0.98 but there was an internal transaction so the kind of maybe tip uh that transferred that extra 0.1 to them uh so the the full payout for those blocks ends up being Spread spread across either you produce blocks and internal transactions um so it can be it can be kind of confusing to validators yeah same here we get we get confusion because they don't see it as an internal payment or as a transfer so they're just wondering and what we do is basically we show them a screenshot of the historical balance so if you go to IFA scan then you can um and you go to if balance you can graph it over time and then you can look at the Delta of the each balance there's also exactly what we've been doing there's also the case where they have an incoming or outgoing transaction within the same block um so showing the full balance difference doesn't reflect what the actual bid value was um we're talking we're working with ultrasound right now about implementing that on their end we we check uh all transactions inside of a block during validation for those uh to like valid into the payout that the payout is accurate with that um because there's like the gas that they're using for their own transactions along with the balance leaving their wallet or going to their wallet um so those need to be considered you can't just do a full balanced difference but it's it's more of an edge case and it doesn't work well yeah I mean transfers in and out yeah this is an important case to consider and right now on the ultrasound code base I think it's just a flat balance difference I think what should be easy to Discount would be the withdrawals from the address but you'll need a tracer to trace like through all like smart quantity directions and instead of just figuring out the transactions but I how about you like do you think you can accurately collect like all incoming transactions that shouldn't be counted because like for us it we have been thinking about that now and it seems that you just have to count all the incoming transfers no matter what because any of these could be from from searches or bundles yeah I have to double check about the traces uh we do modify our our validator get node to address on all the receipts that from into addresses for them while we're applying the state transition transitions uh so we'll know we'll know how they're involved with each transaction uh but yeah I guess I'll have to double check about the traces as well for internal transactions all right but will be good to collaborate on this together and then we basically have a reference implementation that does exactly what it but it shouldn't that everybody can can Implement that would be great I think yep share my uh the gists I have for enough right now that I sent ultrasound to you Chris uh later today amazing that's it I think for this topic for don't want to park up too much space I think just one one thing we are taking a look at like early investigation is could work even without showing the bid value maybe even to the proposal that could be a way with zero knowledge proofs that the proposer can compare and find the highest bid without knowing the values and this could be an interesting Avenue to explore further this was the other topic I wanted to add I don't know if anybody has thoughts on this but this is an idea yes this is something I mentioned in the Mev Berlin post you can use fhe as you said to encrypt the bids and then compare them so you have two input bids both encrypted and then you have one output bid which is also encrypted which is the max of the two and you don't know which one it is um and so what the what we could do is we could have the relays return encrypted bids and then locally the uh the proposer just runs this Max function on encrypted payloads but then we need kind of this this extra step to force the decryption and you know this is very natural to do when trying PBS because you have these two steps going on but it's it's less clear how to do it with Mev boost so wait why do we want this in the first place uh is it will remove strategic bidding which is a big part of the building strategy that Builders withhold each other's like data apis and get header calls and then they just up their own bits a little bit to out compete they are the best speed like just in time like we did a lot of we're doing a lot of data analysis right now and uh probably showing something a bit more publicly but there's a lot of like strategic building going on another thing that you can do which is kind of cool is um second price auctions so if you do them in in public they're kind of completely broken but if with fhe you can you can salvage them and they're kind of from an auction Theory standpoint like maybe better than first price options right I mean this all sounds very cool I mean definitely I think around the Strategic bidding issue that'd be nice to have something sooner rather than later um it'd be cool to see I mean especially if anyone's listening and wants to work on this stuff it'd be cool to see sort of prototypes maybe that don't involve fhe my understanding is that that's not quite ready for prime time but I'm not quite sure from there you know if there's like maybe some BC construction or something like this where we could have these properties where like uh let's see you at least with the bid values encrypted but maybe just for Simplicity the entire bit encrypted and then yeah have some ways to run the auction in zero knowledge that would be super cool so it's the so the objective then is just to prevent bid competition outside of sort of inherent block value by by the sort of game that Builders do of incrementing their bigger in order to just win out right it would it would yeah like is that is that uh is that a goal that we want to achieve and I kind of asked that in in the sense of builders are definitely operating in a way where they want to try to generate some revenue and if they have the opportunity to build a you know an excellent block that is outperforms everyone else and that's their opportunity also to um to generate a little revenue for themselves you know using the payment transaction like we talked before with Builder margin um but this would make that it would be very difficult for a builder to be able to compute what kind of margin they can have because they don't know what the competitive landscape looks so it's a it's like a sealed auction at that point is that like an objective that we want to have I'm asking openly not no no uh No Agenda right so I think the game theory is that you know every Builder just ends up bidding you know the fair value from that perspective and then they make kind of the optimal reward from that perspective which is the Delta between them and this the second price so you can you don't need to be guessing anymore you just get the the fair uh profits when you when when you do when I guess one one effort well two other advantages one is that um it will just dramatically reduce the amount of bidding because right now we're getting on the order of a thousand bits per slot but really we should be having you know just way fewer and part of the reason why we have so many bids is because as you said like they do these micro incremental um it increases to these uh to these bit values which creates a lot of spam and then the other Advantage is that we we wouldn't have to put so much effort necessarily in in having low latency uh you know get header stream because right now the get had a uh API endpoint is like really heavily uh spammed almost um by the Builder so it would make being a relay so much easier and it would also make building a a builder so much easier because right now every builder needs to be aware of all the other builders whereas with a a sealed second price auction you can just bid independently in isolation and you know that you'll get the fair outcome so do you mean like a sealed second price do you mean that the to winning Builder would get the Delta between that between their bid and the uh I'm not quite sure how that yeah for the Builder the winning Builder is the top bit but they only pay the second bid value okay so that right so then there'll be some validator education that the winning bid that you get is not the is not what you get you get the second price and then there's some yes and I don't quite see how that that would I'm just trying to map out the changes required to do that because the then the uh what would some kind of block have to be reconstructed the second bid block would have to be delivered but it's the contents of the first block I'm not quite sure how that works right so that it would have to be a hard fork in the in the context of uh yeah yeah cool thanks um since we're in the last 10 minutes now uh maybe really funding would be one topic tool um still spend a little bit of time on or to get payload not yeah I'm happy I'm happy to get into it I was even thinking we just defer to the next call because I don't know if 10 minutes is really enough to get into it um but yeah let's take the 10 minutes now and yeah let's start it like these calls are too far apart this is too too important of an issue to delay okay yeah I mean I was thinking we even have like a funding breakout call we could do that in next week or two but uh yeah so let's see with our 10 minutes here yeah I mean there's I think there's there's a lot to say here I mean it's definitely a very important topic um but I think there have been a lot of proposals kind of discussed here and there asynchronously um yeah I mean I think the first starting point is just recognizing like yes you know relays are providing this very important service and uh it's important for to be acknowledged meaning that you know it probably makes sense to like seek public goods funding for these relays um and yeah the question that I think is how to structure that and and how to go about it but okay so you I've been quite vocal on this point I don't think it's anyone's surprise I I just want to make sure that we're all the same pages one we are a line that we are going to move forward with some form of funding for the relays there's not a question of yes or no or now or later we we're all agreeing that we're going to pursue this now is that a fair assumption I see Max as a thumbs up yeah I mean I don't see why you wouldn't like if there's no guarantee that like like again it's like what does this mean it's like do we go to different Dows and like ask for Grants and things and like that's a viable strategy and then so if you do that then there's no guarantee that that you know capital is out there but certainly there's no reason for that Avenue not to be pursued you know I've I've heard murmurings uh there's conversations with the es there's conversations protocol Guild there's other entities that are interested but I haven't seen I haven't heard anything concrete that says this group is willing to do this thing does does anybody have anything like that are there even conversations I have personally not pursued this at all we've been pursuing other stuff yeah I mean I don't think this is the EF would directly be involved and also the protocol goes not really set up to like hand out grants like that either um the protocol Guild could be an inspiration for what this looks like but yeah I mean part of it is like this is just kind of a big there's like a huge design space here and so like a lot of different things could happen um personally I think keeping something very focused is very important in terms of it just like being successful so then I'd rather like think about more of this grant route and just be really targeted with like grant funding and trying to connect you know relays with the right people who can supply funding um and lieu of some much bigger like protocol Guild style structure uh Chris you had your hand out briefly do you have somebody say or was that unrelated yeah I think that's where all good points um at flashpoints we have been uh a little bit quiet on this topic because we are also working on on thinking through really funding VR Pro release funding finding ways to finance themselves we are working on a broader funding basically as uh explained before in certain conversations too I think we will have some proposal ready around FCC and will host the round table there where there will be a more concrete proposal how a broader funding vehicle could look like how it could get funded and how it might work on on other funding really operations in addition to research and development on a bunch of different fronts um yeah I'm not sure these answers like any questions right now but just the perspective of what flashbot is up to here but we have been relatively quiet on this topic okay um one I'm happy to hear that too I'll reach out to you one on one to see if there's a little more we could learn there to me they're sort of two very obvious pathways one is we pass the hat and try to get grants but I think we've heard well the two obvious sources aren't that interested even though we quite clearly are critical to the orderly operation of the chain being out of protocol again it's sort of it feels like we're in this bizarre Never Never Land uh and so there's path a which feels better but is highly indeterminate or path B which is hey we're handling all this value the validators receive all the benefit and we not only incur the cost and the the um the sort of mental overhead but we're expected to refund on this slots so it's like 100 risk zero percent reward versus 100 reward zero percent risk that's not an aligned economic incentive and I've heard the arguments which are well and a long enough timeline the game theoretic outcome is the relays if they get some piece of the validator action they just give it back but I don't really subscribe to that line of reasoning because this is not an ideal scenario there's just a few relay operators we're all here on this call and there's a limited timeline for all of this because epbs is coming and so the long-term game theoretic outcome I would say is highly improbable in this scenario and so the second way we could pursue this would be hey look there's some slice of value that the relay the relays themselves capture as a result of the benefit that they're delivering to their primary customer set of validators and we all know by the way in this ecosystem the the the entire block building ecosystem is sort of economically broken the only piece of elements that make money in this really are the trader Searchers and the validators everybody in the middle basically has really crappy economics and those builders that are making money are basically Trader Searcher vertically integrated so the actual state of the market is to incentivize vertical integration which is kind of the whole thing we're trying to avoid and so this is an attempt or this proposal the suggestion is an attempt to counteract and an admittedly sort of a coarse fashion but it does need to be perfect given the current state of the market so again I'm I'm uh we've talked about this but I feel like there are two avenues that we should be pursuing one is Grants the other is uh Performance Based what example even Performance Based like having realistic a cut of the block value yeah the simplest thing would be one percent of block value goes to the relay Guild and then the relay Guild basically redistributes that on an equitable Pro ratabasis to the relay operators okay and by the way the justification for this is just look at rated networking say relay handle bids are three to four hundred percent more valuable than vanilla builds bids so hey the validators still have plenty of value and and the idea by the way the one percent it's an arbitrary figure is intended to be a small enough slice that the validators are not incentivized to them uh subvert the relay Network right like hey we don't really like having to pay this fee but it's like way cheaper than if we try to do this ourselves sure are you worried about relays defecting because basically we're saying like we're going to unionize and then someone could just sidestep us well absolutely I mean they have free will right but again the economics are such that their disincentivized to do so so the Converses hey look you guys do all this work for us for free or we're going to go elsewhere you're like fine go elsewhere we don't get any value for this and again the the whole the thesis of all of this is to as to disincentivize vertical integration and that vertical integration is already becoming enshrined in this model right so like these theoretical downsides like again I don't really understand like oh gosh so staking pool X decides to do why like okay fine I didn't mean to cut you off Matt I was just I thought you were done yeah um I mean I agree like this there's these two experiments that we can do they could both fail what could succeed or both could succeed and it's worth trying both in parallel on the first uh experiment where we basically just asked for donations there has been some amount of success so uh Mount thou uh just made this proposal to give the ultrasound relay a 100 if the proposal hasn't passed yet but it's expected to pass in the in the following hours or days um huge thank you to the to the announce down for for doing this um but you know I think if we were to set up uh some sort of relay Guild that we may be willing to make a larger donation I also know that the the optimism retroactive public goods funding was was very successful we're talking about tens of millions of dollars um and there were some large uh recipients for that and the relay God could be successful in terms of like the the one percent kind of flat three that we impose um I agree it's unionizing you know it's like at the relay as a cabal kind of all need to do this and if one of us the effects then then it doesn't really work um and I think you know at partially speaking on behalf of the of the ultrasound with a team you know we'd be happy to at least try out the experiment and not defect at least not uh not at the beginning until you had your hand up hey um it's Tina from flashbots here so um yeah I definitely want to say that you know take off my flashcards hats I think that public goods funding and actually like I think all of the efforts that has been put into public goods funding all of the great experiments will fail if we couldn't even figure out a way to how to find really great work into relay um into um the actual unnecessary operational component um like that to maintain until we get to epbs and that said I want to be uh kind of constructive here because I see there are essentially two um approaches that can be run in parallel but there may be certain dependencies so the grants approach is no strings attached sorry is like there is no downside we can already do it today it's about us bending together and um it's not in the form of a curtel that in the form of a shared vision and a shared interest for what we believe in and I think everyone here is here for a reason and doing what we're doing for a reason and second which is experiments really fun economic experiments I think the entire Med space is a Sandbox for a mechanism design experiments so I think I'm all for it that's it putting back on my flashlight as a research organization one of the things that I think we would like to um bring forth is more Community involvement in actually providing the measurable uh outcome accountability to these experiments and reason through through sound research there are quite a few of researchers who's been you know doing great work on PBS simulation and there should be more and much much more so this way on these calls or in the various forums we will be showing our steps we'll be showing um you know let the data speak and um we will be reasoning not only based on intuition and this way it can provide an actually more positive feedback loop for us to either the former path which is the grants approach we could actually establish benchmark and let the various Community who are experimenting on retroactive public goods mechanism and they would love to have us to be able to be beneficiary from this but we can also put you know something more meaningful in front of the community in terms of okay so how do we evaluate what's the good design versus what is sub-optimal that's the whole point of experiment and that's the whole point of research so um to I guess sum up my rent here is that I think um the urgency is here I think that from The Flash fast point of view we would like to bring together more um um of a research focus and data-driven approach to um this effort such that whether it's a Grant's effort or the experiment economic experiment effort can be sustainable and we could have something more I would say uh meaningful to chew on and be I think um a support for all the great work that the EF research team the new new formed art group and you know the super always you know super busy you know Barnaby's rig right like we want to be able to be complementary to all of those efforts so um yeah maybe let's get together at um um East CC um happy to host all of you at a uh our mad Hacker House um msgm on this idea uh in person for coming and if there is a in between there is a hall specific on really funding uh happy to share more thoughts on how to make this um uh results oriented and can actually provide sustainability and a positive feedback loop between Community oriented research and development of this middleware yeah thanks again that was really nice um yeah definitely I think some of this would be ACC so we can do the conversation there but that's not the only place these conversations will be happening um I think certainly in the meantime we'll have plenty more conversations around the path forward here uh Max you had your hands so uh yeah if you'd like to say something oh hi everyone Max from Asus I appreciate you running really short on time so I'll try and be brief I agreed with almost everything that Matt said um and to keep it short I think we've reached consensus I'm hoping we have to form a guild to relay funding and what I'd suggest is a kind of positive action out of this is that each relay effectively elects somebody to form a kind of working group that we can continue this discussion in in a focused way in telegram or Discord and feedback to to their respective teams because you know there's going to be quite a bit of organization to get it going um I'm happy to be involved foreign great so yeah I do think there's a lot more to say I think we're just getting started um thank you everyone for your thoughts so far um it sounds like we have two like very concrete promising directions to at least you know further sort of spec out and and consider um one being just more of like this Grant's arm and then the other idea of being um some sort of like um like you know I'll call it like a protocol Guild style uh uh situation where the expectation is that yeah somehow there's contribution from the validators uh back into this thing uh to recognize the value relays provide so yeah I mean there's a lot of really compelling ideas and I think yeah to the extent that we can we should explore them all in parallel we are over time um so I'll say that um perhaps the best way forward now so we can definitely have like a another call to talk about funding if we'd like um and it can happen soon so that's not really a blocker um so someone will like that please speak up or let me know some other way I'm happy to to schedule another call and yeah otherwise anything else anyone sees I don't know exactly where to go from here especially given uh the fact that we're over time and I don't want to take up any more of anyone's time than we need to right now thank you Alex for setting up the community call and keep this going and for the moderation I appreciate it a lot yeah thanks Adam yeah for sure this is the easy part much appreciated and plus one on having a chord allocated to funding okay yeah I mean what I would probably like to do then is it's actually formally write down the ideas we've just been talking about and then you know that kind of serves as a coordination point and then we can have a call so just keep pushing things forward I just want to kind of make populate the fact that there is also a third option to what they relay monetization which is basically the most competitive the one with the latency Edge over all the other relays can you know basically just extract value from The Edge that they have uh but you know the big downside here is that it's well one is a raised to zero and two only one relay you know wins or wins most of it so it doesn't really solve the public school funding so again my proposal here is one percent again whatever the percentage goes to the We Lay Dao which I think we just agreed on and then it gets redistributed on a pro router basis that may or may not have anything to do with win rate so it doesn't doesn't necessarily Advantage the number one or number two relay okay should we close the call I think that's yes yes please yeah okay thank you everyone for your contributions um we're just starting this conversation there's plenty more to say and yeah otherwise uh it'll be exciting to see developments on uh you know more immediate term things with 444 and uh all the the changes there so I'll see everyone soon thanks Alex everyone bye everyone thank you 