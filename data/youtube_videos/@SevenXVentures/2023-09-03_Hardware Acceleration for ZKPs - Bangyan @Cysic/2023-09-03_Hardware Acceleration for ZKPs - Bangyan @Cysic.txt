gkp accelerations and today I will show like some recent progress of our development our recent findings and the OLX OLX per step in the future especially I want to share with you about like something that you can expect in the next year like what kind of Hardware you can see on the market so so some background about the DK snack uh honestly I I think like we can keep this part because today everyone is so professional so in short it's a short proof about the sunscreen it's true usually it's about the sound commutation is correct and the proof itself should be short and it should be fast to be verified and the DK snack is like something that you can hide the information behind the computation and it helps severalized properties to make it useful for example it can have some concept of verified computation you can ensure the desired computation is face fully evaluated you can hide the knowledge about a special specific piece of information sometimes it is useful for example Z cash and the under the size and proof time should be short and the one example of the application is like a DK low up and the basic idea is like you have a bunch of huge amount of computation which is already happening on the layer one is about to verify some computation is correctly evaluated and by moving it out of the first layer you can reduce the computation on layer one the problem is like how can you trust the result is correct you have multiple methods but the Lord basically says like using some cryptography method you can spend a little computation to ensure that the whole bunch of computation is correctly evaluated and that's why there's so many projects using gkp there are some of them are able to uh project some of them are L1 and and in general like the system behind the GK Slack are consists of Swiss stage the first stage is you write your applications using some dlcl and then it was transformed the Tucson some kind of snack friendly format and finally the the encoding such as r1cs format is sent to a snack backend approver and in the last step you have a huge amount of computation that computation is so expensive of course it's already much better than 20 years ago maybe 10 years ago but like it's still so expensive that restrict its application at least restricts the size of the circuit you can write and exact example is that you can see the idea how at least like 300 000 RTX 3080 equivalent GPU wrongly on their test later three how about the future you have more GK related projects and each project are using another and another circuit you need a huge amount of computation power to support the approvers and then we ask how about the specialized Hardware it have it can immediately bring your server Advantage for example you can have a short proof time or equivalently with the same time budget you can work with much larger circuit and you cannot spend less money compared to GPU like a performance per dollar and you may have less electricity bills so if we look at the hardware acceleration what kind of problems we have to look at so today I will discuss two three points the first is like which part of the system which part of the computation should we accelerate and then the second is a recent funny about the dimension return after we accelerated the MSN identity part and the last part is like how can we handle the diverse kernels in the long tail distribution so the first part let's look at the general gkp proof process also like this summarization is pretty bad but I'm sorry for that so you in general you first write all your witness and then you commit the business when you're coming to visit this you see it as a polynomial and then you you perform some kind of polynomial commitment and finally you prove that the witness is correct and and and if you look at the uh you send like strip the mathematical way and if you look at the computation only you will find like they involve like large field arithmetic for example 256-bit additions multiplications I mean modular addition modular uh multiplications and then when you have when you come B to your witness you have some kind of multi together multiplications and the numerical theoretical transform so we start with MSM and NTT the reason is like first those two are the most time consuming Parts before acceleration it takes like 80 of your execution time in some cases and the second thing is like they are Loosely come coupled with the CPU thread so that means like if you offload them into a specialized Hardware it will be pretty easy because the overload overhead is small uh just imagine if you want to use the accelerator for every single mode module multiplication then the system overhead will be unacceptable so that's why we start with the first two and in the past year we developed a several system for this two actually three kernels for the MSM the implementation is some kind of similar to the software implementation which is the bucket method you divide the scalar part into multiple shorter bit number and then you use a bucket to accumulate them uh and the hardware design is consists of multiple independent process elements each of them have a standard along aou which can perform a full easy point and and double in a pipeline fashion so each cycle it can finish one that operations and you have other control Logic the tricky part in this design is because this is a pipeline design you have a huge number of like uh easy group AD and operation on the Fly and they might be added to the same position of the table and you need to get carefully handle those kind of potential like right conflict problems and on top of the vpe you have multiple per year located in one fpg board and one each fpga and multiple fpg by located in one one PCB board and that forms our final system and we reduce the MSM on BN 254 curve uh the execution time of like one billion point evaluation like it was reduced to to below 200 microseconds if you familiar with you if you have a sense of like what the expected time if you run this on the CPU you will know like how much difference it is and we also have some kind of entity implementation the the general implementation follows like a kind of four parts algorithm uh uh and and like I would assume like this is like a common way to do entity you organize it in your Matrix and then you do entities and then column back column and this is also how we do it in a hardware and some new problem emerged when we do it on Hardware the first is like the memory access pattern is pretty bad especially if you have a lot of data on CPU side uh sometimes we see some other design Hardware Designs that like requires the CPU to do some kind of tricky data reorganization to promote the data in the entity array and because that part was happening on CPU that makes the CPU overhead pretty big and we have customized the intermediate data layout to make every memory access no matter on CPU on Jeep or on our accelerator card to be much more Hardware friendly the second part is like you really have to store a huge 3D Factor array which is near the size of the entity Vector itself on CPU it's good because like CPU have a huge memory but like on your fpga the memory is pretty restricted so so so you may come up with some method that you compute on the fly but the computer on the Fly you have the double the computation that's also bad so like we use some kind of a mixed story and compute strategy strategy to to not only reduce the memory footprender but also minimize Rhythm which means like the compute on the Fly overhead is near zero so so that is some kind of good thing and with those designs we have a massively connected fpd system well the hardware design is kind of boring here but in general we have a performance like this okay so if you have like a one billion Point nttu can finish it in like uh 200 and about 200 microseconds so the speed up comparing to CPU over here is like smaller than the MSM because entity itself the computation is less than the CPU so the transmission time becomes a more important problem and I finally we have like kind of potato Tree in which case uh uh the keyboard it's a keyboard problem yeah so like a one billion Imports like I remember I think the design is about 64-bit Target Knox if my my memory is correct so it can finish in 43 seconds so this one is basically a pcie bounded and uh so all the performance that we show here is not a single fpga die it can actually come from our FB server so so it can have multiple accelerator card each card several like uh process elements on it and this is how it looks like a huge machine with like customized happened with interconnect customized PCB design and customized power delivery and what's calling and the problem is this is too big too heavy so that's why we are developing a Asic design on top of our implementation of like the fpp design because fpga itself on Hardware is not so efficient because like to make the hardware reprogrammable you lost so much seeing for example your frequency is restricted to 300 megabytes that kind of stuff so what we are going to do in the next step is to squeeze all the performance all the computation power in that machine into a single accelerator and that's our project and and but like this is not the end of the story because we found something bad after we accelerated the MSM and entity part for example if you accelerate your circuit itself is kind of a possible half circuit and if you look at the the performance before and after acceleration you will see that's like P4 acceleration it takes 800 seconds and then after accelerations you have five times speed up but after acceleration MSM entity itself added together contribute only four percent of execution time you can see this figure so the orange part is the MSM entity entity so before you accelerate it it's the dominating part but after that it's only a fraction which means you if you continue to build like the braking elements and entity implementation that's just again it's nothing useful for end-to-end performance not only a special case or a corner case like you can say also says on other things for example evm circuit which is widely used in Daily 2 project and we you also see this so MSM entity is three percent uh gpt2 like ck252 funny uh and the result that it looks better because like MSM entity is still contributed to 20 but like you can imagine in general this one is already the one this like this has an activity is a kind of magic selector modification so the stuff is like uh the proof generation the Vietnamese generation part is kind of easier comparing to you to come to the commitment so that's why it is very high for example 20 but you can imagine in normal case in normal application scenario of gkp you would have that person's smaller than 20 so what you have to do is like you need to reduce the long MSM and entity part you need to do that part but the other part is difficult because they are coupled they are not they are not Loosely covered with the men's thread they are tightly covered with men's red so you need to come up with bad Hardware architecture to do that and then in order to do that along MSM entity party you have to handle the diverse kernels in the long tail distribution uh and you if you look at the uh the evolution of static agreements you can find that different in different aspects but if you look at the lowest level of operations you can say commitment they really use this for and they are used together with some kind of technical composed Vector module multiplication stuff and so so so for these four it's fixed so you can write hard code Hardware to accelerate for those outside of this four you need to kind of software support and the current our plan is to use uh Isa for this part so if you skip the all the yellow part it is pretty common compared to a common like CPU or GPU answer well the only difference is you have native support to modular operations for example modular multiplication and modular additions but on top of that you have a kind of hard coded unit for example easy group add an addition you have programmable register password and these two together make make it like assistance strong enough but also flexible enough so comparing to the traditional traditional CPU or CPUs what is the main thing that like it can do but traditional civil cannot do that is like you can you can easily offload very fine grained operation to your Hardware especially a hardware accelerator hard-coded Hardware unit on CPU if you want to offload some operations from your software you have to go through a very complicated system stack you have to have a system card you go to the driver the driver sends the command through the pcis and the pcie goes find a little bit send the data to your accelerate Card wait for the result to be ready and then give us like interrupt and your system takes the data back and that Force you are offloading to be restricted to those like huge operations that each of them takes like for example one gigabyte or like an easy the several attempts of megabytes operation but this one can be much more final granted and using this way you can handle both those like hot like fixed kernels but also flexible kernels and and then we hope that like this system can support all of this kind of mainstream like zero logic proof systems we are going to build up like this like each of those Asics available we are going to build build a class on top of it and we plan to leave the possibility to it let it connect with everybody card so if you're lucky enough we can do everything with our executive design then we just connect our rescue diet together to form of like unified solution otherwise if you are so anakulated that like for example a new protocol comes out with some kind of a trigger operations that we have never seen before and the level nobody expected that how can we do that and like if we still have the possibility to connect it to the ffvj card so even if you have some like Corner case operations that we cannot cover we we can do it on ffj and uh we expect the card to be read in next year and finally uh the QR code so on the left one I want to remind that tomorrow we have canceled Z price and actually the track one is like we architected a competition is about end-to-end performance acceleration so welcome to drawing that part and thank you I guess we're open for questions of it it's a bit hard I probably have a long question so to mentioned that we can achieve some kind of linear scaling by putting several machines yeah on the side and then getting them together into a cluster um I was wondering if there are any limits Beyond which there will be diminishing returns yeah composing machines yeah composing machine like every every time you increase your scale of your system like there are two parts that might limit your speed up the first is like the interconnect might be a problem for example you have if you are too common on connected on by on the die the band is huge on the same board it's still good if it's connected via the pcie on the same like motherboard the connection is really restricted to the PSI benefit which is pretty low I'm sure actually pcie four or five you have like 30 to 60 gigabytes per second and compared to compared to your computation power position number is pretty small and if you have two machine or like yeah two machine and you have to connect it using like like internet then then yeah God like that transmission time might be even longer than the computation time on a single machine so that will be the huge problem the second problem is if you scale you still have to handle those sequential part and this speed up of like a measurement TDP because like screenshot part is like just becoming big like huge and and sometimes like we cannot expect every part of the computation can to be like paralyzed so so they were always there so you have to think of some other way to speed up it other than just like duplicate your machine 