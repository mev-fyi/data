so this semester where we're trying and we're actually it's our second semester collaborating in Cornell and city ventures and this collaboration has kind of been a lot of fun we've really tried to find the the most cutting-edge stuff because in the the most cutting-edge speakers and I'm very proud to say that this semester we're going to have one talk on AI one blockchain and and one on robot training so it's sort of we hear about but we wanted more with us here as professors know canary and has a background in the bug for machine learnings early on and it's now director of the artificial finance hands-on approach to AI and machine learning and all these topics that we want to know more about so without further coming okay yeah thanks for having me right so we have we will be discussing how we can so how we can we see how we can use deep learning in finance right so we you know that people in has been successfully and in successfully many other in many other areas right like obviously image recognition and language models right and I would say the fine is also doing a little bit of expunge solution because we're doing quite a lot of time series which was the thing that you will never know with the memory networks and recurrent neural networks with more on language settings right so and we are not right now obviously by trying to see if all these architectures can be can be successfully applied right in finance so this is what and we're going to try to use a scientific method in the sense that there's there's in finance some sort of everything is you know it's very empirical sometimes here's a very noisy is very hard are have a bunch of issues and we need to see if these things are operate well in rem in other contexts right we'll be operating well in finance and the answer is obviously complex and complicated some in some places for example oh the memory nap was too good job in some others maybe bachelor model still continue to do good job right and obvious it is that the older all the all the very exciting applications of deep learning for unstructured data which is obviously something that's going to be very very useful right we last year we found that the education finance Institute in order to help right obviously collaborate work with universities in the idea of doing training and also is doing more research trying to see how we can use AI in finance right and and we have when our more people from from NYU from Raven part from UBS and so on so forth right so we have a diverse set of professionals right trying to help us right helping professionals and a bunch of machine learners the one are also using these models in finance but last week we were in London we were discussing yeah there's a lot of machine learning research well it's true there's a lot of machine learning research you buddy we look at the top journals in finance we see it as only I don't mediated papers right or even less right that try to to research machine learning in finance so there's a lot to do right in that sense right so it's a it is some sort of a young right so I really encourage you right to to continue to research papers also the Journal of machine learning in finance another that are specifically right right because as we all know right so when we look at me which is basically was what can be what are the different fields of machine learning in financial you the machine learning I'm sure you all know well right supervised learning and supervised learning forcible learning we see here that basically we'll try to predict or describe we tried to describe an unsupervised learning we try to be prescriptive we try to decide what is the policy right when we try to go to enforcement learning whenever we look at this some sort of three fields of machine learning we see obviously that that I think the answer needs to be the question is to be the opposite right in the sense that tell me a place in which we can't use right machine learning right if you think about it because basically all we're doing finance right is in many contexts predict or describe right we we try to figure out default probabilities whenever whenever we try to to figure out what are the credit ratings of companies we are obviously right on the supervisor learning space where and supervised learning space is gonna is helping us right figure out whenever we want to have some sort of a better understanding of the features of the data set right so unsupervised learning that it's also huge it's not some huge feeling which is really under the under research in finance when we don't have a lot of unsupervised learning a research right in finance or really also encourage you right to some sort of help us build more tools for unsupervised learning right and then we have all the excitement that we see these days emotional learners so this idea of we have we have a set of states we define a reward function and our job is to some sure what learn by what would be the optimal policy the surveys with statements of things right that could maximize the words here I want to do in terms of enforcement learning you just have maybe five papers right in top children's that during formal learning in finest so that's a lot to do there right and I have to say to that that but reinforcement learning in finance might be probably very close to be a supervisor but to be supervised learning right because it's time serious because the work functions that well know and so on so forth but again reinforcement learning is a very very hot topic these days we discuss it and there's a lot of research right when we see the applications we see that basically as I said so tell me so whenever banks asset managers right look at that you see there's literally no place in finance in which in which supervised or unsupervised or improvement in some way shape or form cannot be used right you see it's basically it's basically so they show me this prediction our training credit losses very ratings to tremendous tables scores and so on and so forth right and in unsupervised is whenever as I said we want to we're gonna have a better sense of the data sets and we just don't ask questions we try to see if the diggers organized somehow right using clustering using PCA right using encoders using decoders right so and then reinforcement learning is can we learn the policy can we learn the best training policy can we learn the best option that the piece of policy right and also a very interesting place in which can use its marketing strategies in the sense that banks have huge star sets in which they have millions of clients that have done millions of things but the knowledge states they know what they need right there obviously some sort of try to set this as a reinforcement right okay so today tonight we're going to discuss specifically one of these right one of these buckets which is deep learning which happens to be also very useful to all the others right so we'll see as you all know that deep learning is basically right in the Angi it can be the ending for impossible learning is the engine for right can be the engine for your regression problems can be the engine for offs unsupervised learning right okay so we can some sort of set all the three some sort of problems right as you all know so many of the best most famous reinforcement learning applications are deeper impossible learning applications because deep neural networks are being used as function approximator for the for the cube for the body functions policy networks and so on okay what is the neural network I'm sure you all know what is a neural network it's basically right no linear function with lots of parameters right so we have here the input you have the outputs we have more than one hidden layer right so I mean in every day in every neighbor what we're gonna do is assume certain nonlinear activation so we're going to be doing nonlinear transformation right like you have a simple function right so we have this we have on the top is some some weights this families an axis right that every neuron in every neighbor Willemijn in a transformation and then we can we can obviously some sort of go on and on right can have as many layers many neurons obviously issues about the Roman versus right the problem we'll see we'll see the second approach on the cones but there's there's obviously big big some sort of warning from statisticians and and engineers that told the told the pioneers that this is not going to work right because you have too many parameters this is not gonna work because it's the not non convex optimization you love you want to get a global minima it's not gonna work right but it is right in the in many contexts right so they so Clinton and in in banja and so on so for they found the right combinations of activation functions number of layers and were of neurons right that make things work even if apparently against or statistically right they should immediately we have in terms of architectures right and this is one of the other issues obviously is that when we talk about deep learning we're talking about this large number of architecture that go from the feed-forward neural network on a multi-layer perceptron right it does it's still very flexible there's a lot of things right we have convolutional neural networks this is what we were is used for image recognition that can also be tricked to use time series right and it has the beauty the beauty of cornets have the the the are especially all of them are beautiful but convolutional neural net is especially useful because there is some sort of feature engine for us right like they did for for images right so cope nets can also do the same feature engineering for your time series right so it can be shown that that code meds can can be some sort of are some sort of doing exponentially weighted moving average things the first layer right and then we have motion the memory Network networks this is this one one of the special what we call deep deep learning with memory that can also be recurrent neural networks and so on and now we have also transformers and so on and so forth right so we have a let's discuss let's discuss just a bit before singing experiments were right what is the pros and the cons right so the process that in general right if they work very well right we'll see some examples right that in fact the models resulted pretty good right or it's slightly better than the linear models right obviously right what what did that's obviously are able to capture right is obviously non-linearity unable to some sort understand especially well the expressive ISTE of of the author of the data set right become they also show the they also showed a lot of efficiency on time series that will see that that that when we do time series well when we do multivariate time series them seem to be really better than others but when we when we do time series right Apple or GP more and so on so for they seem to be very close to the bench models aramis and stuff so they don't seem to be beating right and for classification problems they also need seem to be right very very right they also have set the engine of deep enforcement learning just starting with dipping sauce for learning and finance but but anyway then yes there's also there's also as as we as we so we see the merits of learning we have to we have to say something which is the big energy booster it's it's also very it's always very close right I'm not extra boost so this idea boosting trees right so in times use also but it also in fact almost seems to be really very very close right to what deep learning does right so it so it's a exhibit is a formidable competitor right in many cattle competitions in even in time series right an extra boost is showing a remarkable remarkable performance right and and and and as you know energy boost seems to be one of the best techniques when you try to commit to marry right categorical right numerical does so they exhibit seem to be again doing a lot of teacher engineering for you right okay so you throw right 200 200 factors and you let and you know XC boots to choose right and you realize right well I'll be discussing the regularization issue in a sec and the cons the cons are are are obviously right with what we hear of all the time right you and all over feet right I'll discuss this idea what overfitting interpret it is obviously a big deal right so if you're if you're deep learning more hop happens to be the winner of the competition so if you're doing a rates model and you put 2 to n n and you have some sort of of Austin uhlenbeck models or PCA models and they're very close to your to yogi to your deep learning job is going to be interpreted ET right because we've got many parameters right this is but I'd say the the first for the last which is non stationarity which i think is the real issue right and we discussed last week in alone we were discussing and Gary Katherine said when I we were saying right that one of the main codes we have right certain in finance is non stationarity so it's this idea that that that time series keep changing right the DGP keeps changing and that's really the big deal right and we don't have and we don't have really good models for non stationarity right not only for for promote them capture non stationarity but also to the I need right okay when you read the literature sometimes religion says oh all you have to do to be with non-stationarity is some sort of have to have the next more adoptive so train the way it's right you making but you make them more adaptive but we all know that the ones that we've been using your factor models and all sorts of models we know that that that isn't right if that isn't a good solution okay so obviously requires much more research right and we cost much more but yeah I think this is the real the real deal but more than more than overfitting the spreadability the fact that the DGP might be changing and the fact that the nets and the nets were learned how to how to trade as some p400 stocks and all of a sudden they find themselves in a different environment right okay you you can say oh we're going to create artificial data we are going to use adversarial networks you know that you try to make the Nets more robots that's true but still right still it's a big deal right and it's really one of the biggest problems we have right now enforcement and sorry and deep learning right or a linear a linear regression right they will all be struggling with this idea they learn awaiting an environment environment changes like they might not work well right and many people discussed this idea of cars are not really changing that's right so okay what are the modeling aspects right so we have some sort of right this is from information theory so the idea of Indiana right what we're trying to do is is to compress information we have this minimum description length principal at the mall complexities so amount of inference theory right then they tell us that we're basically in the network network so the model with that we need is the model that some sort of compresses information right to the shorted program possible right example so a monitoring friends theory is assurance program that produces the training data based on commodity complexity okay other quality features would be these that I'm sure you all know so the idea is or why why this deep learning model is so expressive right why did it learning models have right some sort of right extremely large number of parameters right compared to other traditional statistical models right and okay we have things that are listed in a segment right the idea is that the traditional u-shaped function might not be that way right might be a double right see that means okay we have two things that we all know so so Universal proximation theorem is that deep nets are come basically approximate never want anything with arbitrary precision right what is not guaranteed is that these gonna generalize it's not such thing as so the universe approximation theorem doesn't right doesn't guarantee you that it's going to generalize it just gone to you I think we see something this is gonna approximate this thing very well right or are we trading well so we all know and then we also know that these deep nets are also good for stochastic processes that basically are learning right are learning if there's an Austin uhlenbeck in this mean reversion etc delete mexicana learning okay more things and more things that go against right the common sense so to speak was this idea there's a paper but Sun it out that shows these ideals we were thinking that regularization was the way to was the way to generalize and it's not not that way it's not that easy so to speak the idea that simple problems or the path to generalization so there's a there's a paper about that my son it out that shows that that regularization is merely necessary nor sufficient for reducing generalization error right maybe the merits of regularization is that maybe makes the optimization easier but I really invite you to read the paper other papers also make make this point about the intrinsic dimensions of these neural networks the idea that that we cannot measure deep neural networks with the number of parameters right that dimensions might be much lower right in reality okay now I'm just counting the number of parameters right now the number of parameters is not right so there's a fantastic paper from last year by backing it out that makes it very interesting right so push in the direction of at some point right at some point over parameterize neural networks with instead of instead of overfitting right instead of we increasingly general in this remember right when they are over-promised right they might be entering a new regime which is called interpolating regime right ok so welcome I'll show right it is not easy to replicate the results but they show that again deep nets might be entering in a regime in which yes you can write some certified by having a huge number of gravity returned in reaching in which position error we will over right because the model theory it will be physically lowly dimension right okay okay maybe equipment logic models we might be able to discover large the largest function classes that fine interpolating function that might have smaller norms and might be same right okay so read this paper right is the it's a it's a very interesting one right and it's a very surprised right Isis those against right and some people have tried to replicate it it's not easy but this is this could this could some sort of explain right war in the end they're not our fitting right okay because you might be here right the model might be finding simpler things right in with that number of right okay so we also have in terms of palliative aspects right so so other things that are not I will miss this but this idea that that is there's a some sort of not only as a created equal so there's a lot there's a lot more that needs to be researched right obviously in order to understand right but some all right in have been invested in this year the raw parameters in different layers right now say all the layers are creating we are not created equal right and so on so these are right some sort of these the qualitative aspects of of of of people are right for us again I think that for us the main issues are are are we know that the model it's it's gone it's probably gonna find it's gonna be it's gonna be one of the candidates that fits better than data right and we need to right and it's hard to say for me at least at this moment to say that deep Matakana revealing better with no special right I think this is not an unanswered question right questions or okay so we in terms of in terms of time series so in terms of time series right so we wrote a paper that is chapter 13 of the quantitative files big data from Tony guida it's a 13th right in which need some sort of write tested right several models right to see if if deep neural networks can help us write do some sort of kana matrix so to speak to the prediction in conscious right and we used several models we used as one of the candidates that we think it's it's it's more interesting that you'll know shut the memory networks that I won't be discussing in detail right but here basically have this mechanism this memory cell input gate and forget a gate sorry right and they're basically right a good candidate a better candidate the recurrent neural networks is seem to be dealing with with long term memory and cycles right so the idea is that that this can be some sort of birth and obviously as you use like return this this can be some sort of a nonlinear Ahriman right a nonlinear barmen right you know that the benchmark models for us are star obviously re mas right up to regressive integrated moving average models overall the ultimate ebayid so the vector regression moving average models this continues to be obviously the benchmark I haven't I haven't mentioned that but that obviously what we need to do is compare the results of self deflating with with the benchmarks right that are the linear mode ok so in terms of looking at the results and you can see that in the paper right and as I said as I have some sort of introduced in the beginning right we don't see we don't see like for in this particular example in which you do Apple right and you use on to show the memory net was super vector machines and and a little a of people whenever that works right all you can you can see right it's something that you'll see in your machine learning research all models are very close in general right in general all ones are obviously always very close and very weak so to speak right if right if you come from machine learning right you obviously something that has a some sort of an accuracy or a heat ratio to is really disappointing right so but as we all know there's a lot of noise there's a lot of noise in dance ears right so so here right whenever we we see the difference or the different stocks and the different balls we don't see the dog shot the memory that was like being so be right another neural architecture but whenever in this other in this other table we see the results when we do a sensor or long shot the memory networks with 30 stops so instead of just using your like returns of the stock you use 30 stops right and then we have to say that that long shot the memory network seem to be capturing well or seem to find some sort of a very specific on on doing that with multiple stocks instead of just trying to learn right a time series of a particular stock right yeah and this will be confirmed in a second because I'm going to show right they resolve these are the results right doing so we have five giver stops rise and JP Morgan idea and Jenna like chicken apple right they are the the in-sample returns for the moths were trained for 2016 and then we trained and then we sample results are here right and and and this is the absolute error right so the lower the better and we see that that you know that if you use convolutional neural networks if you look at launching the memory net was there there first of all their own bait clothes right so there are very close to this to this standard Standa would be the rms right just anomalies Ahriman right so we should be comparing that right and arm are still it's still doing some sort of a better job right for some of them if this stops very close to the others pay close to deep nets right so you would you would say that deep learning if you do deep learning right you're more is not going to be that is very very very far from the optimal it's always very close to the best models right so if you have to choose blindly right the multi-layer feed-forward might be some sort of always a good choice consistently very close to the best model right but anyway right so on that this is joint work with sonometers Stuber right and we in we do that we go David what prices right here and ice TM seem to be capturing right as you see what it means for very much better than what somebody new stops right so and this is weekly right we're doing that tell you we see we kind of see the same results of Artemis doing pretty well right so that no should that that deep learning is not massively some sort of beating some sort of the bench memo right okay in terms of f√°tima factor models if we look at right so here what it's done is that we defended in 18 S&P 500 stores right and the idea is that you let the deep learning model with feed-forward neural networks choose the best the best stores and and we see here that if we look at the information ratios out-of-sample this is a linear model right and this is the deep nets right the results seem to be seem to be better we we haven't tested right actually who's here but you can read you can also read the XE boost experiment I'm Tony guida school I'm sure FC Busan very close to the results of of information reach out sample right yeah so the difference between so yeah here's a pure time serious exercise in which you stand so low and use the black time series but in the launch of the memory net was the same thing you use it it's the time seriously you use you use the lack returns of these stops and you use the latter in terms of 30 stops you do another and then you do a parma back to some sort of nonlinear vector autoregression and here it's a it's with what and here what it's used is factors right so these they're so quality value momentum and so on and results right you would say you would say that that the nonlinear model might be capturing sense of the nonlinear relationships between right these factors and and and the returns right you get what you get when doing this is a very wise some sort of Allah so Allah so model right and and yet the difference the difference you would argue this is the non-linearity right and the fact that yeah questions yes I really like night question when you move deployed is necessary you all obviously had a lot of parameters to start with and also the the you know the training time the time for which you were creating these models so did you calibrate them along the way or did you just have some fixed a priori no no we basically some sort of use the deep nets and you know in sample I think was 10 years and then it we use the out sample which is three years right yeah see he learned the parameters there then you use it out sample right was the training period also one of the parameters you yeah yeah yeah it's it's it always ease right as we as we measure as I mentioned before there's no right one of the parameters you you you you you should be investigating always is right when are you right training your your neural networks right we don't have to the best of our knowledge we don't have a good theory that has to be right ten years five years and so on right but what we know is that is that is that if we if we use more data right some sort of data from from ten years it's gonna it's not gonna contain the same information that three years ago right okay so you matter what you think you win in terms of using more data you lose in terms of some sort of the data is less representative so to speak yeah you can see here like you know any other steps right this is just an example right of how right this is an itself an example of no special Knightly the sense the pea plant right or the Austrians were very different on this period right okay if you inspect the weight of these LST ends and so on and the results are very different depending on the period this is also proof on non-stationarity the results of the model you see what I mean okay so non-stationary is not a thing that is not it's only the thing that we can investigate from the past right in the sense that here right the model was very different than the one used here right the way it's basically yeah so the average return here is it the predicted returner to stop or is it how much money the Moto actually Mike yeah it's the the the the money that the money that the model make oh but there no cost it's just a pure research into size right so this is based on past data it's not in practice yeah that's right okay yeah we're updating the paper now so what we claim in the paper is not that so when we claim in the paper it says all the DNS diems right might be some sort better than your varmint and your pectoral to arrest moving average just because they able to capture the nonlinear relationships between right variables so they're going to do a better job and they're going to be learning right cycles and and but nothing but but but but again you have to be sure - right you always present but you don't know if if what you learned here is going to be useful here right so so the parameters are also so one of the parameters you need to play is is the window in which networks right learn so say by tempos the training period of 30 2015 that's a model learn all of the data before the detail or it just learn the data within different the DVP only three years oh yes heat around the paper right any more questions but we claim when we claim in the in the in the paper is not that that these deep nets are right or we claim is that that that has to be on your two books right okay and then in addition to that right there's no no such thing as deep learning but needs to replace right benchmarks you can run Aramis and indeed Matt's right in power right okay so this these things are the things that are that are decided in the maulvis Committees of the back right they see we do a piece for race for example we depreciate we do wash the new limit and we do deepness okay did not seem to be always less said 'give next may might not be the others by much but they're gonna be doing always a good job because of their specific specific users but they always be right very close to right so some more risk comedies have decided no we're going to be running right our our benchmark models by also gonna look at what are the people Nets telling us right because in some repair regimes deep nets are telling us different things because deep nets might be might be learning I know things like oil cycles right Oh second third order effects that may be your linear model didn't catch right okay but that you not implementing as a benchmark because of de la quinta prevalence and the bed and the performance is not significantly better well you can run in parallel then the moles whenever there's a divergence right then maybe the risk committee should sit and discuss why things are yeah yeah yeah yeah yeah yeah well but that's common indeed nets right dip nets right you you sometimes have more parameters than than viruses and data points right okay i resource where were most I'm not saying right results were reasonably robust right and the convergence was there right okay yeah trying to compare with is the cross-sectional model and ESPN is actually time series moto and what's the treaty strategy of what STM is the time series model your train yeah yeah so it's to adjust to different sizes here we will use as here here it is used as an ingredient their lack returns of one stock and other stops right and and here in the fact of body using you usually use a bunch of stocks like 500 stocks by conduct is 218 and 200 factors right so you know you know some sort of exogenous right and so features of these stops that might indicate things about future returns right okay you can combine them right and then you know short that's right no I'm not here so we do not show here's the long okay and I'll finish with with one other application which is right which is right language models right so obviously here right well we have and we will stop talking to - Sasha before right so well again we were discussing last week why why we are like want is the only way to go these days right our claim or my personal opinion on one one is the is the way to vote is in the sense that right now basically that basically this there's a lot of information being generated on the markets right right now all right now we have more than ever right we have all the texts we have satellite data you have all the alternative data so some sort of humans right cannot deal with that right so that's why big data right some sort of it is the way to go in the sense that portfolio managers need need all the big data tools all right in order to investigate the market so the ones that you they can't absorb that much information and there's an additional thing we have that sometimes we don't is that machine learning right and and deep learning especially good for for for whenever you have cities nationally dimensional space and obviously one of the one of the main applications of deep learning right is so in language more so not natural language processing right here I'm going to be discussing a few minutes about about that right so in terms of right so there's a lot of things that obviously right that natural product language processing might be doing for you process the money interpretation syntactic in eyes and so on so full right we're going to be discussing here right one idea that especially useful right but for forefathers which is sentiment analysis right but these look these are the kind of things that you can do with language models right so spell-checking table search finance in addition this idea that it's very some sort of right so it's incredibly so so whenever whenever you see the alternative data industry you see that one of the main some sort of successes is all the web scraping so the idea that that there's a lot of information on the web sites by product prices they its location people company names right and portfolio managers are willing to write investigate and know all this information right so even something like scraping the Internet is incredibly useful for portfolio manager portfolio manager might be interested in answering a specific question like like well it is the price policy of Amazon changing right is the Apple is the Apple are the upper price is changing or and so on and so forth right oh how is the labor market evolving right so this is so what scraping is a some sort of not big want but a very efficient waves and sort of search the internet for information changes right that might indicate that things are happening right so yesterday we saw an example someone was saying oh well in fact people that was web scraping did no obviously in a lorry in advance about Thomas Cooper right in the sense that you were seeing a deteriorating numbers right already using alternative data okay and then okay one of the things that we that it's being done in finance right is this idea of sentiment right so the idea of reading a text and figuring out sentiment and because sentiment might be obviously a precursor of momentum the idea of and if we think about it it's yeah you're compressing the information about all the things that have been written about Apple today right in a number of good or bad right might be incredibly useful in this ideal weather manager right that hasn't been able to read all the data but I have a number that indicates right at least if this is good or bad and then it can you can drill down and see if this good or bad is because it's because of supply chain problem or a because it because right I'm a corruption case or problem a problem in production and so on and so forth right okay and we obviously have all the other things machine translation that of systems and so on so forth okay so we do not we we're not gonna discuss okay but just just for the ones that can never seem enemies so it's basically there are two ways basically one way it's more the traditional way which we in which for example we were using words as one vector and then some sort of counting the frequency of these words right and doing the back of words things like that these are the things that still being done in the industry so this one very something and our people sent him a line so there's two ways right one way is this it is very sincere but apparently very too simple which is counting the number of positive words and the number of positive negative words and and may and comparing the frequency of these guys right okay right so this is a traditional you radiate sentiment dictionaries but some words we pack up with representations that obviously right ignore the word order right and that need to deal with and design negation features that at least need to be able to deal with with things like what Apple results went bad right so things like that right and obviously now the deep learning right so deep learning in the sense that that people's design of remodels these are not big balls in which you just turn words and and frequencies and you make ratios right obviously it's some sort not taking into account the context okay and which they used so you don't ask as you don't understand the semantics right so and what how can you do that so you do that first you do that using deep learning and you need to first put the words into factors okay you for example to work to back and then doing deep learning with these factors back to representation of the words okay so here I describe we describe that so first it was this idea of every word right every word is affected zeros are one one foot and coding right and and this is not very efficient because I need 500,000 500,000 back doors right and this is this is not giving me any sense of similarity between words right okay so more of a hold on two completely separate objects while they're very close right so things like that and the way machine learn is some sort of don't deal with that was with work to back which I'm not talking about right and what you back right so it's it's one way to consider to create vectors right which you can compare one word with other words so vectors are our 302 inspectors for example can beat right and then there's a notion of CBT and similarity okay so if we look at now at the most advanced model right it's whispered right that it's a bi-directional encoder representation from transformers right that was invented by by some Google researchers right there has some sort of this complicated architecture bi-directional by the way means that that instead of reading a One Direction you read two directions you get a better sense of right of the position of the words right and and and unsuitable for distributed computing and so on right okay has all sorts empirical advantages compared to so the all mole or was the longshot the memory networks right where it uses the self self attention right instead of the cosna locality bias and there's also more efficiency right okay so and bird is especially efficient right you see that it beats so f1 scores right rather than human performance for some tasks okay and it's open source and you can train it and we did that ourselves for example I think that we can Holly right and with the pre-trained models we just needed 17 minutes with GPU score we want to IMDB data set which is a labeled data set of movies right and reviews and we achieved the order model it's the popular chief and you will see of 97 percent and f1 score of 96 okay so the mall is incredibly complex right when you look at it right but again implementation is not that hard right so so you can imagine you could imagine that that now if you have if you have a financial data set in which you have the headlines right and label data label data means someone that says okay this is good for Apple this is bad for Apple right like Roomba or raven pack right okay if you have the data set you can train right your bird and see if if now in the future maybe you don't obviously you don't need humans reading but bird can be doing that for you right you know that to do that obviously bird should be trained on fire on a finance course right it needs to be in its to learn the financial legal right one of the important thing about about what why bird right in preliminary results with better than other most bird seems to be understanding better than numbers okay so text has numbers right and the language problem the language model needs to be dealing well with numbers numbers are essential part of the financial text right sometimes numbers are IEP s sometimes numbers are what sometimes numbers are a day right 27 September a number of components right so the language model need also to understand if this is a percentage if this is something rather than put it for the for the for the for the context this is just an ordinal number and so on and so forth so numbers are one of the things that language models again needs to understand better and really in the finance context right in others right and we we saw that that word is exceptionally well at understanding the yeah the numbers on the headlights right it's able to represent well the numbers okay and then we don't have a lot of time but right for the ones that have seen deep learning sorry possible learning right so you see that in all the good in all the bad in all the applications basically in order to figure out the body then the body will function of a few function right you need function approximator right and fat and what is the best function approximator available for us right it's obviously right okay so this is some server conclusion right so this is the idea of where can you use the difference the difference I'm sorry the different that means the multi-layer perceptrons the memory that was the caviar enhancer oh the memory net was the cop Nets right let me mention something about the covenants right so we've seen that the idea so we have a lot of visualization and tasks in advance for credit profiles and stuff right and we also have this idea of and supervised learning especially hard right in order to figure out distances so with the cognate might be much more useful than we thought but because the idea of transforming data into images and then we throw the cop necks okay so you transform the data set into an image and then you throw a cognate to see if the cognate is understanding better right is understanding better so if somebody is doing a better unsupervised learning done plastering thing this is a very very interesting idea I've seen recently right and think about those problems right in which is very hard to grasp right but maybe we can just form them and throw the coordinates that seem to be so well at understanding special right and so on so I think this is something and also cornets see they are easy to flip in the time series right so again at the beginning with that we with some syrup set companies and only for for image recognition logical to us right and then it can be useful in tip for time series they also they can be used on this idea of transforming things in primitives and then throwing the top nets okay and see the copies understand better for example credit profiles right you transform the crane profile into an image and then you throw the movements to see right and they're able to produce classes and classify okay and then your own code which is this idea that I haven't mentioned so these are you know you put some outfits are the same right have you just put you just put the narrow Matt was into understanding the primary nonlinear structure right of this data okay so the inputs look at the same inputs and try to create a more compressed non linear representation oh right okay so you can do that intensive flow it's 50 lines of code so what the old multi-layer right can also be used in a variety of different paths they also right you can use them crisis returns and factors they can classify regress issues are right what I said right so we throw these this is easy to implement and then our job is going to is going to be how can we deal with non-stationarity how can use how can we interpret right then and overfitting right the memory networks is basically the same so the main question is not if the RS TMS are suitable for for time series regimes and not yes of course right but the idea is is the environment going to change it's going to be stable enough so that I can use them in the future right I think this is main question not ETL STM is not a good econometric right the commnets right has he used it in all of that so to speak when we were seven auto-encoders that are basically specially useful for for covariance matrices right this idea of doing known some certainly nonlinear pc8 okay and then you can also do n holders and outlet folders as a as a as an NG for no bottom all of them and obviously right well we mentioned about the language moans and a language model right if you thought if you talk about we could have here something that it's not it's a transformers transformers which is the what is used in bird but can also do memory can also do everything but it's a more sophisticated I haven't seen examples where transformers can also hit in forever here because the language folks seem to be very creative right with their architectures right so right within its word right data scientist fonts and so how to implement all these things in tensorflow right okay we think that majority of these models right can be implemented but it's reachable but it's not unreachable and the etapa is bird but we managed to some sort of work and and and and Burt and written it in around three or four days it wasn't more there's a lot of much there's a lot of open source code out there right okay you need to train it in a in a financial petty but it's it's not unreachable right it's not unreachable this is open source so you don't questions yeah so just curious have you heard about like online burnings and I'll batch learning I was there was your Pina about online learning comparative bachelor we should like because uh this my company is a beaut a machine learning framework by using all I have already compared to already set up a batch learning and there's some like this it's a paper saying that as a little achieve a better result compared to the 5:30 it's one of here was what was routine on college yeah I I would be fantastic I don't have a well we I don't have experience on that I come in there's medium a list of 20 models one could choose there's a list of maybe 20 applications that you could use and maybe 20 sources of Vegas if you you know you multiply all these numbers it's a bit overwhelming for you know a student or a researcher and you've given us a really great bird's-eye view of everything that's out there that's how would you describe sort of how you navigate from you know one type of model to a different type of data yeah so so well I think this is easy species I don't know the domain experts come to you say we have an ex-navy problem right and we wanted financial Ernie we have yeah prediction Pro wanna try enforceable learning so I think I think machine learning is my MPI sorry finance very empirical so what it's useful in one context may need not be usefully in some other so I think it's a special case in which domain experts write and machine learners should should be meeting and and try to solve problems right because there's a huge amount of problems in finance right he's shown probably more on or the eyesight but cell psyches is obviously a very fertile ground with RTVE and all these new regulations how to fire these cranberries it's it's a right and you don't see papers there it's just an pepper and option hedging and so on so forth so we haven't right at least there are no papers out there other than but are there any other cutting-edge NLP deep learning models that can verify that the answer is the answer is that the best of my knowledge when we talk with when we talk to let's go back here when we talk to the they're leaders of NLP that might be ever known right who's gusto right I don't know Raven back or was Bloomberg Reuters and so on and so forth they still tell you right that their language walls are they sit comfortably or not but see more here in the sense that to the best of our knowledge right people still not using deep learning right still are still using handcrafted approaches in which you want to be sure that you did you capture JP Morgan let's think about let's step back right what do you need to do in order to understand that an article right the sentiment of an article you need to classify the topic you need to classify the entity you need to classify relevance and so on right so all these first classifications like in which you need to figure out if this thing if this article is related to JP Morgan but people still has a bunch of people by putting together dictionaries right in order to describe JP Morgan Jamie Dimon largest American Bank you see what I mean making sure that you have and all the typos all the typos maybe the Nets haven't seen a typo maybe the nets haven't seen a typo and then and you're not reading something that it was very relevant for GP more than right so people is still doing traditional more handcrafted right in we say control right and they do super vector machines and so on and we seen that that it was it was you can do some short long shot the memory networks and they were pretty well and everybody's looking at birth right I don't know if you have another experience on that or be yeah yeah yeah yeah well yeah obviously I cannot tall obviously right but but yeah III would say that at least on the publish research right the best of our knowledge birth seems to be the state of the art right and and and and and I guess that all the composite to NLP are right now right looking at birth closely right but yeah by science perspective we always thinking about investing stock in your no sure way so cross-sectionally do you have any certain idea or swords that we can share in attacking the LST n into a cross-sectional way or is that a form or direction we invest well i think it's i think i think the more is flexible enough in the sense that you can do to give it like returns you can also give it like factors for example and STM's would be navigating - right but we haven't tested that but you can have some sort of ammonia which you have lack factor that returns everything right any you ask the machine learning ball to make sense of flat returns on factors right but this is very vicious right yes yeah do systematically vary the say you know the you know depth or the of the number of nodes and see how the performance changes as you do that if for instance if you seem like monotonicity then you've achieved a interpolation threshold and you know you know predict so have you done something like that or yeah it's it's in the paper right but yeah I mean now I'm working in the update so we will see that soon would keep achieved a depletion potential right say like yeah well show you that yeah any other however very much I'm sure part for many applications out - because from my experience when I cuz the thing now you're from here or overshadowed by the noises and I can find thank you the amateurs even it's peaceful to find the optimal values it may take a mess I'm efforts yeah yeah so it's yeah so what you're saying sit so yeah some things are an Emmett NP problem so your problem you will need right yeah that's that's what you need human data science is they need to make these heuristic choices based on experience based on and if you look at what they are scientists do many times is that this some sort of have an intuition on the parameters of right all the super macro machines or on the extra boost right what is the length of the trees so then you can develop some sort of an intuition after your work with several datasets right but that help your research but otherwise you cross pollinate so we don't know the answer equals body there's no I can make learning methods hmm yeah yeah I think but yeah yeah again some of these problems are and and and I sit and you look at the lid just just if you look at the literature off of deep nets right just what just four seconds right if we're still trying to understand these right I think you would say we think about that we all we still try to understand this in the sense that there's not there's a few papers of muttering there's a quick paper by by anyone you on mathematics of deep learning if you read them in the paper you see no equations so basically also mathematician with mathematicians are some sort of trying to understand why it is so good but qualitatively right because mathematically is very hard right there's some sort of I understand what's going on right and for the parameters is a little bit the same thing but but anyway you after you've done 10 times actually fools use some sort of develop an intuition ops on the on the on the parameter space right [Applause] [Applause] [Music] not mine I'm very 